```{r}
library(here)
library(readr)
library(dplyr)
library(skimr)
library(summarytools)
library(magrittr)
library(GGally)
library(corrplot)

library(tidymodels)# broom, dials, infer, parsnip, purrr, recipes, rsample, tibble, yardstick

library(workflows)
library(tune)


```




## Data Import

We have one CSV file that contains both our single **outcome variable** and all of our **predictor variables**.

Let's import our data into R now so that we can explore the data further. We will call our data object `pm` for particulate matter.

```{r}
pm <-readr::read_csv(here("docs", "pm25_data.csv"))
```

## Data Exploration and Wrangling

The first step in performing a machine learning analysis is to explore the data to better understand the variables  included in the data, as we may learn about important details about the data that we should keep in mind as we try to predict our outcome variable.

First let's just get a general sense of our data. We can do that using the `glimpse()` function of the `dplyr` package (it is also in the `tibble` package).

We will also use the `%>%` pipe which can be used to define the input for later sequential steps. This will make more sense when we have multiple sequential steps using the same data object. To use the pipe notation we need to install and load dplyr as well.

For example here we will first grab the `pm` data object, then we use the `glimpse()` function on it based on the pipe notation.


```{r}
pm %>%
  dplyr::glimpse()
```

We can see that there are 876 monitors and that we have 50 total variables - one of which is the outcome. In this case our outcome variable is called `value`. 

Notice that some of the variables that we would think of as factors (categorical) are currently of class double as indicated by the `<dbl>` just to the right of the column names/variable names in the `glimpse()` output. For example the monitor ID (id), the Federal Information Processing Standard number for the county where the monitor was located (fips), as well as the zcta

Let's convert these variables into factors. We can do this using the `mutate_at()` function of the `dplyr` package and the `as.factor()` base function. 

In this case we are also using the magrittr assignment pipe or double pipe that looks like this `%<>%` of the `magrittr` package. This allows us use the `pm` data as input but also reassign the output to the same data object name.

```{r}
pm %<>%
  dplyr::mutate_at(vars(id, fips, zcta), as.factor) 

glimpse(pm)
```

Great! Now we can see that these variables are now factors as indicated by `<fct>` after the variable name.

### Packages to get a sense of the data

The `skim()` function of the `skimr` package is also really helpful for getting a general sense of your data.

```{r}
skim(pm)
```

Notice how there is a column called `n_missing` about the number of values that are missing. It looks like our data is very complete and we do not have any missing data. This is also indicated by the `complete_rate` variable, which shows the ratio of completeness, in our case all variables have a value of 1 indicating they are fully complete.

The `n_unqiue` column shows us the number of unique values for each of our columns. We can see that there are 49 states represented in the data, and we know that the data should be of the contiguous states. Let's take a look to see which states are included:

#### {.scrollable }
```{r}
# Scroll through the output!
pm %>% 
  distinct(state) %>%
  print(n = 1e3)
```
####

Looks like "District of Columbia" is being included as a state. We can see that indeed Alaska and Hawaii are not included in the data.

Here is another method of looking at the data using the `dfSummary()` function of the `summarytools`package. We need to copy and paste the output into the rmarkdown.

```{r, eval = FALSE}
dfSummary(pm, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.45,  tmp.img.dir = "tmp")
```


We can see that for many variables there are many low values as the distribution shows two peaks, one near zero and another with a higher value. This is true for the imp variables (measures of development), the nei variables (measures of emission sources) and the road density variables. We can also see that the range of some of the variables is very large, in particular the area and population related variables.

### Evaluate correlation among possible predictors
In prediction analyses, it is also useful to evaluate if any of the variables are correlated.

Intuitively we can expect some of our variables to be correlated.

We expect the development variables (imp) to be correlated with each other, we expect the road density variables to be correlated with each other, we expect the emission variables to be correlated with each other, and we also expect these variables to be correlated with one another and likely correlated with population density. We can get a nice visualization of correlation using the `ggcorr()` function and the `ggpairs()` function of the `GGally` package. to select our variables of interest we can use the `select()` function with the `contains()` function of the `tidyr` package. 

First let's look at the imp/development variables. 
```{r, out.width = "400px"}
select(pm, contains("imp")) %>%
  ggcorr(palette = "RdBu", label = TRUE)

select(pm, contains("imp")) %>%
  ggpairs()
  
```

Indeed, we can see that imp_a1000 and imp_a500 are perfectly correlated, as well as imp_a10000, imp_a15000.


We can see that while some of the road density variables are highly correlated with one another (or anti-correlated), there arr some variables that are less correlated with one another.


We would also expect the population density data might correlate with some of these 

Interesting, so these variables don't appear to be highly correlated, therefore we might need variables from each of the categories to predict our monitor PM~2.5~ pollution values.

We seem to have some pretty extreme population values though, so let's see what happens when we take the log value.

```{r}
pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, log_pri_length_10000, imp_a10000) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
  mutate(log_pop_county = log(county_pop)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, log_pri_length_10000, imp_a10000, log_pop_county) %>%
  ggpairs()
```

Indeed this increased the correlation, but variables from each of these categories may still prove to be useful for prediction.


The `corrplot` package is another option to look at correlation among possible predictors. This is a great option if we have many predictors. 
First we need to create a correlation matrix using the `cor()` function of the `stats` package (which is loaded automatically).

```{r}
library(corrplot)
library(RColorBrewer)
PM_cor <- cor(pm %>% dplyr::select_if(is.numeric))
corrplot::corrplot(PM_cor, tl.cex = 0.5)

corrplot(abs(PM_cor), order = "AOE", tl.cex = 0.5,  cl.lim = c(0, 1))


corrplot(PM_cor, diag = FALSE, order = "FPC",
         tl.pos = "td", tl.cex = 0.5, method = "color", type = "upper", col = brewer.pal(n = 8, name = "PuOr"))

corrplot(PM_cor, diag = FALSE,
         tl.pos = "td", tl.cex = 0.5, method = "color", type = "upper")
```
```{r}
library(ggcorrplot)
ggcorrplot(PM_cor, hc.order = TRUE, type = "lower")

```