---
title: "Open Case Studies: Predicting Annual Air Pollution "
css: style.css
output:
  html_document:
    self_contained: yes
    code_download: yes
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes

---
<style>
#TOC {
  background: url("https://opencasestudies.github.io/img/logo.jpg");
  background-size: contain;
  padding-top: 240px !important;
  background-repeat: no-repeat;
}
</style>

```{r setup, include=FALSE}
library(knitr)
library(here)
knitr::opts_chunk$set(include = TRUE, comment = NA, echo = TRUE,
                      message = FALSE, warning = FALSE, cache = FALSE,
                      fig.align = "center", out.width = '90%')
```


#### {.outline }
```{r, echo = FALSE, out.width = "800 px"}
knitr::include_graphics(here::here("img", "main_plot_maps.png"))
```

####

## {.disclaimer_block}

**Disclaimer**: The purpose of the [Open Case Studies](https://opencasestudies.github.io){target="_blank"} project is **to demonstrate the use of various data science methods, tools, and software in the context of messy, real-world data**. 
A given case study does not cover all aspects of the research process, is not claiming to be the most appropriate way to analyze a given data set, and should not be used in the context of making policy decisions without external consultation from scientific experts. 

## Motivation
A variety of different sources contribute different types of pollutants to what we call air pollution. 
Some sources are natural while others are anthropogenic (human derived):

<p align="center">
<img width="600" src="https://www.nps.gov/subjects/air/images/Sources_Graphic_Huge.jpg?maxwidth=1200&maxheight=1200&autorotate=false">
</p>

##### [[source]](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.nps.gov%2Fsubjects%2Fair%2Fsources.htm&psig=AOvVaw2v7AVxSF8ZSAPEhNudVtbN&ust=1585770966217000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPDN66q_xegCFQAAAAAdAAAAABAD){target="_blank"}

#### Major types of air pollutants

1) **Gaseous** - Carbon Monoxide (CO), Ozone (O~3~), Nitrogen Oxides(NO, NO~2~), Sulpher Dioxide (SO~2~)
2) **Particulate** - small liquids and solids suspended in the air (includes lead- can include certain types of dust)
3) **Dust** - small solids (larger than particulates) that can be suspended in the air for some time but eventually settle
4) **Biological** - pollen, bacteria, viruses, mold spores

See [here](http://www.redlogenv.com/worker-safety/part-1-dust-and-particulate-matter) for more detail on the types of pollutants in the air.


#### Particulate pollution 

Air pollution particulates are generally described by their **size**.

There are 3 major categories:

1) **Large Coarse** Particulate Mater - has diameter of >10 micrometers (10 µm) 

2) **Coarse** Particulate Mater (called **PM~10-2.5~**) - has diameter of between 2.5 µm and 10 µm

3) **Fine** Particulate Mater (called **PM~2.5~**) - has diameter of < 2.5 µm 

**PM~10~** includes any particulate mater <10 µm (both coarse and fine particulate mater)

Here you can see how these sizes compare with a human hair:

```{r, echo = FALSE, out.width= "600 px"}
knitr::include_graphics(here::here("img", "pm2.5_scale_graphic-color_2.jpg"))
```

##### [[source]](https://www.epa.gov/pm-pollution/particulate-matter-pm-basics){target="_blank"}

<!-- <p align="center"> -->
<!--   <img width="500" src="https://www.sensirion.com/images/sensirion-specialist-article-figure-1-cdd70.jpg"> -->
<!-- </p> -->


<u>The following plot and table show the relative sizes of these different pollutants in micrometers (µm):</u>

<p align="center">
  <img width="600" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Airborne-particulate-size-chart.svg/800px-Airborne-particulate-size-chart.svg.png">
</p>

##### [[source]](https://en.wikipedia.org/wiki/Particulates){target="_blank"}


<p align="center">
  <img width="500" src="https://www.frontiersin.org/files/Articles/505570/fpubh-08-00014-HTML/image_m/fpubh-08-00014-t002.jpg">
</p>

##### [[source]](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full){target="_blank"}


<u>This table shows how deeply some of the smaller fine particles can penetrate within the human body:</u>

<p align="center">
  <img width="500" src="https://www.frontiersin.org/files/Articles/505570/fpubh-08-00014-HTML/image_m/fpubh-08-00014-t001.jpg">
</p>

##### [[source]](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full){target="_blank"}


#### Negative impact of particulate exposure on health 

Exposure to air pollution is associated with higher rates of [mortality](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5783186/){target="_blank"} in older adults and is known to be a risk factor for many diseases and conditions including but not limited to:

1) [Asthma](https://www.ncbi.nlm.nih.gov/pubmed/29243937){target="_blank"} - fine particle exposure (**PM~2.5~**) was found to be associated with higher rates of asthma in children
2) [Inflammation in type 1 diabetes](https://www.ncbi.nlm.nih.gov/pubmed/31419765){target="_blank"} - fine particle exposure (**PM~2.5~**) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with Type 1 diabetes
3) [Lung function and emphysema](https://www.ncbi.nlm.nih.gov/pubmed/31408135){target="_blank"} - higher concentrations of ozone (O~3~), nitrogen oxides (NO~x~), black carbon, and fine particle exposure **PM~2.5~** , at study baseline were significantly associated with greater increases in percent emphysema per 10 years 
4) [Low birthweight](https://www.ncbi.nlm.nih.gov/pubmed/31386643){target="_blank"} - fine particle exposure(**PM~2.5~**) was associated with lower birth weight in full-term live births
5) [Viral Infection](https://www.tandfonline.com/doi/full/10.1080/08958370701665434){target="_blank"} - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (**PM~2.5~**)

See this [review article](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full){target="_blank"} for more information about sources of air pollution and the influence of air pollution on health.

#### Sparse monitoring is problematic for Public Health

Historically, epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country. 
However, as can be seen in the following figure, these monitors are relatively sparse in certain regions of the country. 
Furthermore, dramatic differences in pollution rates can be seen even within the same city.

<p align="center">
  <img width="400" src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4137272/bin/1476-069X-13-63-1.jpg">
</p>

##### [[source]](https://ehjournal.biomedcentral.com/articles/10.1186/1476-069X-13-63){target="_blank"}

This lack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations. 


#### Machine learning offers a solution

An [article](https://ehjournal.biomedcentral.com/articles/10.1186/1476-069X-13-63){target="_blank"} published in the *Environmental Health* journal dealt with this issue by using data, including population density, road density, among other features, to model or predict air pollution levels at a more localized scale using machine learning (ML) methods. 

```{r, echo = FALSE, out.width= "800 px"}
knitr::include_graphics(here::here("img", "thepaper.png"))
```

#### {.reference_block}
Yanosky, J. D. et al. Spatio-temporal modeling of particulate air pollution in the conterminous United States using geographic and meteorological predictors. *Environ Health* 13, 63 (2014).

####

The authors of this article state that:

> "Exposure to atmospheric particulate matter (PM) remains an important public health concern,
although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial
resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved
exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or
county-specific ambient concentrations." 


```{r, echo = FALSE, out.width= "700 px", eval = FALSE}
knitr::include_graphics(here::here("img", "deaths.png"))
```

The article above demonstrates that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems. 
We will use similar methods to predict annual air pollution levels spatially within the US.


### Main Question

#### {.main_question_block}
<b><u> Our main question: </u></b>

1) Can we predict annual average air pollution concentrations at the granularity of zip code regional levels using predictors such as data about population density, urbanization, road density, as well as, satellite pollution data and chemical modeling data?

####

### Learning Objectives 

In this case study, we will walk you through importing data from CSV files and performing machine learning methods to predict our outcome variable of interest (in this case annual fine particle air pollution estimates). We will especially focus on using packages and functions from the [`tidyverse`](https://www.tidyverse.org/){target="_blank"}, and more specifically the [`tidymodels`](https://cran.r-project.org/web/packages/tidymodels/tidymodels.pdf){target="_blank"} package/ecosystem primarily developed and maintained by [Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"} and [Davis Vaughan](https://resources.rstudio.com/authors/davis-vaughan){target="_blank"}. This package loads more modeling related packages like `rsample`, `recipes`, `parsnip`, `yardstick`,  and `dials`. We will also briefly cover the `workflows` and `tune` packages. The tidyverse is a library of packages created by RStudio. While some students may be familiar with previous R programming packages, these packages make data science in R especially efficient.


```{r, out.width = "20%", echo = FALSE, fig.align ="center"}
include_graphics("https://tidyverse.tidyverse.org/logo.png")
```

```{r, out.width = "50px", echo = FALSE, fig.align ="center"}
include_graphics("https://pbs.twimg.com/media/DkBFpSsW4AIyyIN.png")
```


We will begin by loading the packages that we will need:

```{r}
library(here)
library(readr)
library(dplyr)
library(skimr)
library(summarytools)
library(magrittr)
library(corrplot)
library(RColorBrewer)
library(ggcorrplot)
library(GGally)
library(tidymodels)
library(workflows)
library(vip)
library(tune)
library(randomForest)
library(ggplot2)
library(stringr)
library(tidyr)
library(lwgeom) # allows 1263
library(sf)
library(maps)
library(rnaturalearth)
library(rgeos)
library(cowplot)
```


 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data
[readr](https://readr.tidyverse.org/){target="_blank"}      | to import CSV files
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to view/arrange/filter/select/compare specific subsets of data 
[skimr](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data
[summarytools](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data in a different style
[magrittr](https://magrittr.tidyverse.org/articles/magrittr.html){target="_blank"}   | to use the `%<>%` pipping operator 
[corrplot](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html){target="_blank"} | to make large correlation plots
[ggcorrplot](http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2){target="_blank"} | also to make large correlation plots
[GGally](https://cran.r-project.org/web/packages/GGally/GGally.pdf){target="_blank"} | to make smaller correlation plots  
[tidymodels](https://www.tidymodels.org){target="_blank"} | to load in a set of packages (broom, dials, infer, parsnip, purrr, recipes, rsample, tibble, yardstick)
[rsample](https://tidymodels.github.io/rsample/articles/Basics.html){target="_blank"}   | to split the data into testing and training sets; to split the training set for cross-validation  
[recipes](https://tidymodels.github.io/recipes/){target="_blank"}   | to pre-process data for modeling in a tidy and reproducible way and to extract pre-processed data (major functions are `recipe()` , `prep()` and various transformation `step_*()` functions, as well as `juice()` - extracts final pre-processed training data and `bake()` - applies recipe steps to testing data). See [here](https://cran.r-project.org/web/packages/recipes/recipes.pdf){target="_blank"}  for more info.
[parsnip](https://tidymodels.github.io/parsnip/){target="_blank"}   | an interface to create models (major functions are `fit()`, `set_engine()`)
[yardstick](https://tidymodels.github.io/yardstick/){target="_blank"}   | to evaluate the performance of models
[broom](https://www.tidyverse.org/blog/2018/07/broom-0-5-0/){target="_blank"} | to get tidy output for our model fit and performance
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}    | to make visualizations with multiple layers
[dials](https://www.tidyverse.org/blog/2019/10/dials-0-0-3/){target="_blank"} | to specify hyper-parameter tuning
[tune](https://tune.tidymodels.org/){target="_blank"} | to perform cross validation, tune hyper-parameters, and get performance metrics
[workflows](https://www.rdocumentation.org/packages/workflows/versions/0.1.1){target="_blank"}| to create modeling workflow to streamline the modeling process
[vip](https://cran.r-project.org/web/packages/vip/vip.pdf){target="_blank"} | to create variable importance plots
[randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf){target="_blank"} | to perform the random forest analysis
[stringr](https://stringr.tidyverse.org/articles/stringr.html){target="_blank"}    | to manipulate the text the map data
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to separate data within a column into multiple columns
[rnaturalearth](https://cran.r-project.org/web/packages/rnaturalearth/README.html){target="_blank"} | to get the geometry data for the earth to plot the US
[maps](https://cran.r-project.org/web/packages/maps/maps.pdf){target="_blank"} | to get map database data about counties to draw them on our US map
[sf](https://r-spatial.github.io/sf/){target="_blank"} | to convert the map data into a data frame
[lwgeom](https://cran.r-project.org/web/packages/lwgeom/lwgeom.pdf){target="_blank"} | to use the `sf` function to convert map geographical data
[rgeos](https://cran.r-project.org/web/packages/rgeos/rgeos.pdf){target="_blank"} | to use geometry data
[cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html){target="_blank"} | to allow plots to be combined
___


The first time we use a function, we will use the `::` to indicate which package we are using. 
Unless we have overlapping function names, this is not necessary, but we will include it here to be informative about where the functions we will use come from.


### Context

The [State of Global Air](https://www.stateofglobalair.org/){target="_blank"} is a report released every year to communicate the impact of air pollution on public health. 

The [State of Global Air 2019 report](https://www.stateofglobalair.org/sites/default/files/soga_2019_report.pdf){target="_blank"}
which uses data from 2017 stated that:

> Air pollution is the **fifth** leading risk factor for mortality worldwide. It is responsible for more
deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity.
Each year, **more** people die from air pollution–related disease than from road **traffic injuries** or **malaria**.

<p align="center">
<img width="600" src="https://www.healtheffects.org/sites/default/files/SoGA-Figures-01.jpg">
</p>

##### [[source]](https://www.stateofglobalair.org/sites/default/files/soga_2019_report.pdf){target="_blank"}

The report also stated that:

> In 2017, air pollution is estimated to have contributed to close to 5 million
deaths globally — nearly **1 in every 10 deaths**.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","2017deaths.png"))
```

##### [[source]](https://www.stateofglobalair.org/sites/default/files/soga_2019_fact_sheet.pdf){target="_blank"}

The [State of Global Air 2018 report](https://www.stateofglobalair.org/sites/default/files/soga-2018-report.pdf){target="_blank"} using data from 2016 which separated different types of air pollution, found that **particulate pollution was particularly associated with mortality**.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","2017mortality.png"))
```

The 2019 report shows that the highest levels of fine particulate pollution occurs in Africa and Asia and that:

> More than **90%** of people worldwide live in areas **exceeding** the World Health Organization (WHO) **Guideline** for healthy air. More than half live in areas that do not even meet WHO's least-stringent air quality target.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","PMworld.png"))
```

##### [[source]](https://www.stateofglobalair.org/sites/default/files/soga_2019_fact_sheet.pdf){target="_blank"}

Looking at the US specifically, air pollution levels are generally improving, with declining national air pollutant concentration averages as shown from the 2019 [*Our Nation's Air*](https://gispub.epa.gov/air/trendsreport/2019/#home){target="_blank"} report from the US Environmental Protection Agency (EPA): 

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "US.png"))
```

##### [[source]](https://gispub.epa.gov/air/trendsreport/2019/documentation/AirTrends_Flyer.pdf){target="_blank"}

However, air pollution **continues to contribute to health risk for Americans**, in particular in **regions with higher than national average rates** of pollution that actually at time exceed the WHO's recommended level. 
Thus, it is important to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.

You can see that current air quality conditions at this [website](https://aqicn.org/city/usa/){target="_blank"} and you will notice variation across different cities.
For example, here are the conditions in Topeka Kansas at the time this case study was created:

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "Kansas.png"))
```

It reports particulate values using what is called the [Air Quality Index](https://www.airnow.gov/index.cfm?action=aqibasics.aqi){target="_blank"} (AQI).
This [calculator](https://airnow.gov/index.cfm?action=airnow.calculator){target="_blank"} indicates that 114 AQI is equivalent to 40.7 ug/m^3^ and is considered unhealthy for sensitive individuals.
Thus, some areas exceed the WHO annual exposure guideline (10 ug/m^3^) and this may adversely affect the health of people living in these locations.

Adverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines. 
Also, it appears that the composition of the particulate mater and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. 
For example, see this [article](https://www.nejm.org/doi/full/10.1056/NEJMoa1702747){target="_blank"} for more details.

The monitor data that we will use in this case study come from a system of monitors in which roughly 90% are located within cities. 
Hence, there is an **equity issue** in terms of capturing the air pollution levels of more rural areas. 
To get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be useful to estimate air pollution levels in **areas with little to no monitoring**. 
Specifically, these methods can be used to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:

<p align="center">
  <img width="600" src="https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/SAWOEGBXMVGQ7AS5PZ6UUOX6FY.png">
</p>

##### [[source]](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.washingtonpost.com%2Fbusiness%2F2019%2F10%2F23%2Fair-pollution-is-getting-worse-data-show-more-people-are-dying%2F&psig=AOvVaw3v-ZDTBPnLP2MYtKf3Undj&ust=1585784479068000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPCyn9fxxegCFQAAAAAdAAAAABAd){target="_blank"}

This is what we aim to achieve in this case study.

### Limitations

There are some important considerations regarding the data analysis in this case study to keep in mind: 

1. The data do not include information about the composition of particulate mater. Different types of particulates may be more benign or deleterious for health outcomes.

2. Outdoor pollution levels are not necessarily an indication of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. Researchers are now developing personal monitoring systems to track air pollution levels on the personal level.

3. Our analysis will use annual mean estimates of pollution levels, but these can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data, however we are interested in long term exposures, as these appear to be the most influential for health outcomes, so we chose to use annual level data. 


## What are the data? {#whatarethedata}

When using machine learning for prediction, there are two main types of data of interest:

1. An **continuous** outcome variable that we want to predict 
2. A set of feature(s) (or predictor variables) that we use to predict the outcome variable

The **outcome variable** is what are trying to **predict**. 
To build (or train) our model, we use both the outcome and features.
The goal is to identify informative features that can explain a large amount of variation in our outcome variable. 
Using this model, we can then predict the outcome from new observations with the same features where have not observed the outcome. 

As a simple example, imagine that we have data about the sales and characteristics of cars from last year and we want to predict which cars might sell well this year. 
We do not have the sales data yet for this year, but we do know the characteristics of our cars for this year. 
We can build a model of the characteristics that explained sales last year to estimate what cars might sell well this year. 
In this case, our outcome variable is the sales of cars, while the different characteristics of the cars make up our features.

### Start with a question

This is the most commonly missed step when developing a machine learning algorithm. 
Machine learning can very easily be turned into an engineering problem. 
Just dump the outcome and the features into a black box algorithm and viola! 
But this kind of thinking can lead to major problems. In general good machine learning questions:

1. Have a plausible explanation for why the features predict the outcome. 
2. Consider potential variation in both the features and the outcome over time
3. Are consistently re-evaluated on criteria 1 and 2 over time. 

In this case study, we want to **predict** air pollution levels. 
To build this machine learning algorithm, our **outcome variable** is fine particulate matter (PM~2.5~) captured from air pollution monitors in the contiguous US from 2008. 
Our **features** (or predictor variables) include data about population density, road density, urbanization levels, and NASA satellite data. 
Using this model, we predict air pollution in new areas where there are no monitors. 

### Our outcome variable

The monitor data that we will be using comes from **[gravimetric monitors](https://publiclab.org/wiki/filter-pm){target="_blank"}** (see picture below) operated by the US [Enivornmental Protection Agency (EPA)](https://www.epa.gov/){target="_blank"}.

```{r, echo = FALSE, out.width="100px"}
knitr::include_graphics(here::here("img","monitor.png"))
```

These monitors use a filtration system to specifically capture fine particulate matter. 

```{r, echo = FALSE, out.width="150px"}
knitr::include_graphics(here::here("img","filter.png"))
```

##### [[source]](https://publiclab.org/wiki/filter-pm){target="_blank"}

The weight of this particulate matter is manually measured daily or weekly. 
For the EPA standard operating procedure for PM gravimetric analysis in 2008, we refer the reader to [here](https://www3.epa.gov/ttnamti1/files/ambient/pm25/spec/RTIGravMassSOPFINAL.pdf){target="_blank"}.

<details><summary>For more on Gravimetric analysis, you can expand here </summary>

Gravimetric analysis is also used for [emission testing](https://www.mt.com/us/en/home/applications/Laboratory_weighing/emissions-testing-particulate-matter.html){target="_blank"}. 
The same idea applies: a fresh filter is applied and the desired amount of time passes, then the filter is removed and weighed. 

There are [other monitoring systems](https://www.sensirion.com/en/about-us/newsroom/sensirion-specialist-articles/particulate-matter-sensing-for-air-quality-measurements/){target="_blank"} that can provide hourly measurements, but we will not be using data from these monitors in our analysis. 
Gravimetric analysis is considered to be among the most accurate methods for measuring particulate matter.

</details>

In our data set, the `value` column indicates the PM~2.5~ monitor average for 2008 in mass of fine particles/volume of air for 876 gravimetric monitors. 
The units are micrograms of fine particulate mater (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m^3^).
Recall the WHO exposure guideline is < 10 ug/m^3^ on average annually for PM~2.5~.

### Our features (predictor variables) 

There are 48 features with values for each of the 876 monitors (observations). 
The data comes from the US [Enivornmental Protection Agency (EPA)](https://www.epa.gov/){target="_blank"}, the [National Aeronautics and Space Administration (NASA)](https://www.nasa.gov/){target="_blank"}, the US [Census](https://www.census.gov/about/what/census-at-a-glance.html){target="_blank"}, and the [National Center for Health Statistics (NCHS)](https://www.cdc.gov/nchs/about/index.htm){target="_blank"}.

<details><summary> Click here to see a table about the set of features </summary>

Variable   | Details                                                                        
---------- |-------------
**id**  | Monitor number  <br> -- the county number is indicated before the decimal <br> -- the monitor number is indicated after the decimal <br>  **Example**: 1073.0023  is Jefferson county (1073) and .0023 one of 8 monitors 
**fips** | Federal information processing standard number for the county where the monitor is located <br> -- 5 digit id code for counties (zero is often the first value and sometimes is not shown) <br> -- the first 2 numbers indicate the state <br> -- the last three numbers indicate the county <br>  **Example**: Alabama's state code is 01 because it is first alphabetically <br> (note: Alaska and Hawaii are not included because they are not part of the contiguous US)  
**Lat** | Latitude of the monitor in degrees  
**Lon** | Longitude of the monitor in degrees  
**state** | State where the monitor is located
**county** | County where the monitor is located
**city** | City where the monitor is located
**CMAQ**  | Estimated values of air pollution from a computational model called [**Community Multiscale Air Quality (CMAQ)**](https://www.epa.gov/cmaq){target="_blank"} <br> --  A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution <br> -- ***Does not use any of the PM~2.5~ gravimetric monitoring data.*** (There is a version that does use the gravimetric monitoring data, but not this one!) <br> -- Data from the EPA
**zcta** | [Zip Code Tabulation Area](https://www2.census.gov/geo/pdfs/education/brochures/ZCTAs.pdf){target="_blank"} where the monitor is located <br> -- Postal Zip codes are converted into "generalized areal representations" that are non-overlapping  <br> -- Data from the 2010 Census  
**zcta_area** | Land area of the zip code area in meters squared  <br> -- Data from the 2010 Census  
**zcta_pop** | Population in the zip code area  <br> -- Data from the 2010 Census  
**imp_a500** | Impervious surface measure <br> -- Within a circle with a radius of 500 meters around the monitor <br> -- Impervious surface are roads, concrete, parking lots, buildings <br> -- This is a measure of development 
**imp_a1000** | Impervious surface measure <br> --  Within a circle with a radius of 1000 meters around the monitor
**imp_a5000** | Impervious surface measure <br> --  Within a circle with a radius of 5000 meters around the monitor  
**imp_a10000** | Impervious surface measure <br> --  Within a circle with a radius of 10000 meters around the monitor   
**imp_a15000** | Impervious surface measure <br> --  Within a circle with a radius of 15000 meters around the monitor  
**county_area** | Land area of the county of the monitor in meters squared  
**county_pop** | Population of the county of the monitor  
**Log_dist_to_prisec** | Log (Natural log) distance to a primary or secondary road from the monitor <br> -- Highway or major road  
**log_pri_length_5000** | Count of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_10000** | Count of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_15000** | Count of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_25000** | Count of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log) <br> -- Highways only  
**log_prisec_length_500** | Count of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_1000** | Count of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_5000** | Count of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_10000** | Count of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_15000** | Count of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_25000** | Count of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads      
**log_nei_2008_pm25_sum_10000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)    
**log_nei_2008_pm25_sum_15000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)     
**log_nei_2008_pm25_sum_25000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)     
**log_nei_2008_pm10_sum_10000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)      
**log_nei_2008_pm10_sum_15000**| Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)      
**log_nei_2008_pm10_sum_25000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)      
**popdens_county** | Population density (number of people per kilometer squared area of the county)
**popdens_zcta** | Population density (number of people per kilometer squared area of zcta)
**nohs** | Percentage of people in zcta area where the monitor is that **do not have a high school degree** <br> -- Data from the Census
**somehs** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was **some high school education** <br> -- Data from the Census
**hs** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing a **high school degree** <br> -- Data from the Census  
**somecollege** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing **some college education** <br> -- Data from the Census 
**associate** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing an **associate degree** <br> -- Data from the Census 
**bachelor** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **bachelor's degree** <br> -- Data from the Census 
**grad** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **graduate degree** <br> -- Data from the Census 
**pov** | Percentage of people in zcta area where the monitor is that lived in [**poverty**](https://aspe.hhs.gov/2008-hhs-poverty-guidelines) in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines <br> -- Data from the Census  
**hs_orless** |  Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **high school degree or less** (sum of nohs, somehs, and hs)  
**urc2013** | [2013 Urban-rural classification](https://www.cdc.gov/nchs/data/series/sr_02/sr02_166.pdf){target="_blank"} of the county where the monitor is located <br> -- 6 category variable - 1 is totally urban 6 is completely rural <br>  -- Data from the National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm){target="_blank"}     
**urc2006** | [2006 Urban-rural classification](https://www.cdc.gov/nchs/data/series/sr_02/sr02_154.pdf){target="_blank"} of the county where the monitor is located <br> -- 6 category variable - 1 is totally urban 6 is completely rural <br> -- Data from the [National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm){target="_blank"}     
**aod** | Aerosol Optical Depth measurement from a NASA satellite <br> -- based on the diffraction of a laser <br> -- used as a proxy of particulate pollution <br> -- unit-less - higher value indicates more pollution <br> -- Data from NASA  

</details>

Many of these features have to do with the circular area around the monitor called the "buffer". These are illustrated in the following figure:

```{r, echo = FALSE, out.width = "800px",}
knitr::include_graphics(here::here("img", "regression.png"))
```

##### [[source]](https://www.ncbi.nlm.nih.gov/pubmed/15292906){target="_blank"}



## Data Import

We have one CSV file that contains both our single **outcome variable** and all of our **features** (or predictor variables).

Next, we import our data into R now so that we can explore the data further. 
We will call our data object `pm` for particulate matter. 
We import the data using the `read_csv()` function from the `readr` package. 

```{r}
pm <- readr::read_csv(here("docs", "pm25_data.csv"))
```

## Data Exploration and Wrangling

The first step in performing any data analysis is to explore the data. 
For example, we might want to better understand the variables included in the data, as we may learn about important details about the data that we should keep in mind as we try to predict our outcome variable.

First, let's just get a general sense of our data. 
We can do that using the `glimpse()` function of the `dplyr` package (it is also in the `tibble` package).

We will also use the `%>%` pipe, which can be used to define the input for later sequential steps. 
This will make more sense when we have multiple sequential steps using the same data object. 
To use the pipe notation we need to install and load `dplyr` as well.

For example, here we start with `pm` data object and "pipe" the object into as input into the `glimpse()` function. 
The output is is an overview of what is in the `pm` object such as the number of rows and columns, all the column names, the data types for each column and the first view values in each column. 
The output below is scrollable so you can see everything from the `glimpse()` function. 

#### {.scrollable }

```{r}
# Scroll through the output!
pm %>%
  dplyr::glimpse()
```

####

We can see that there are 876 monitors (rows) and that we have 50 total variables (columns) - one of which is the outcome variable. In this case, the outcome variable is called `value`. 

AVOCADO: Should we link to a code book here? Like could we use this as a teaching opportunity to demonstrate the purpose of a code book so a student knows how we know the PM values are `value`? 

Notice that some of the variables that we would think of as factors (or categorical data) are currently of class double as indicated by the `<dbl>` just to the right of the column names/variable names in the `glimpse()` output. 
For example, the monitor ID (`id`), the Federal Information Processing Standard number for the county where the monitor was located (`fips`), as well as the zip code tabulation area (`zcta`). 

AVOCADO: should we explain what we mean by a "double" data type? I think factor and character are more understandable? But maybe we should explain those too? 

Let's convert these variables into factors. 
We can do this using the `across()` function of the `dplyr` package and the `as.factor()` base function. 
The `across()` function has two main arguments: (i) the columns you want to operate on and (ii) the function or list of functions to apply to each column. 

In this case, we are also using the `magrittr` assignment pipe or double pipe that looks like this `%<>%` of the `magrittr` package. 
This allows us use the `pm` data as input, but also reassigns the output to the same data object name.

#### {.scrollable }

```{r}
# Scroll through the output!
pm %<>%
  mutate(across(c(id, fips, zcta), as.factor)) 

glimpse(pm)
```

####

Great! Now we can see that these variables are now factors as indicated by `<fct>` after the variable name.

### Packages to get a sense of the data

The `skim()` function of the `skimr` package is also really helpful for getting a general sense of your data.
By design, it provies summary statistics about variables in the data set. 
designed to provide summary statistics about variables. 


#### {.scrollable }

```{r}
# Scroll through the output!
skim(pm)
```

####

Notice how there is a column called `n_missing` about the number of values that are missing. 
This is also indicated by the `complete_rate` variable (or missing/number of observations). 
In our data set, it looks like our data do not contain any missing data. 

Also notice how the function provides separate tables of summary statistics for each data type: character, factor and numeric. 

Next, the `n_unqiue` column shows us the number of unique values for each of our columns. 
We can see that there are 49 states represented in the data, and we know that the data should be of the contiguous states. (AVOCADO: how do we know it's only the contiguous states?)
Let's take a look to see which states are included:

AVOCADO: I'm not sure I understand why the `print(n = 1e3)` is needed below? It looks the same if I remove the code? I was going to just deleted, but left an avocado as a sanity check in case I'm missing something? 

#### {.scrollable }
```{r}
# Scroll through the output!
pm %>% 
  distinct(state) %>%
  print(n = 1e3)
```
####

Ah, it looks like "District of Columbia" is being included as a state. 
We can see that indeed Alaska and Hawaii are not included in the data.

AVOCADO: do you think we need two summary functions or is 1 enough? 

Here is another method of looking at the data using the `dfSummary()` function of the `summarytools`package. We need to copy and paste the output into the rmarkdown.

```{r, eval = FALSE}
dfSummary(pm, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.45,  tmp.img.dir = "tmp")
```

<details><summary> Click here to see the dfSummary table </summary>


**Dimensions:** 876 x 50  
**Duplicates:** 0  

+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| No | Variable                     | Stats / Values                           | Freqs (% of Valid)  | Graph               | Valid  | Missing |
+====+==============================+==========================================+=====================+=====================+========+=========+
| 1  | id\                          | 1\. 1003.001\                            | 1 ( 0.1%)\          | ![](tmp/ds0101.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1027.0001\                           | 1 ( 0.1%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1033.1002\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1049.1003\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 5\. 1055.001\                            | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1069.0003\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1073.0023\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 8\. 1073.1005\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 1073.1009\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 10\. 1073.101\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 866 others ]                           | 866 (98.9%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 2  | value\                       | Mean (sd) : 10.8 (2.6)\                  | 875 distinct values | ![](tmp/ds0102.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 3 < 11.2 < 23.2\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 3.1 (0.2)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 3  | fips\                        | 1\. 1003\                                | 1 ( 0.1%)\          | ![](tmp/ds0103.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1027\                                | 1 ( 0.1%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1033\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1049\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 5\. 1055\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1069\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1073\                                | 8 ( 0.9%)\          |                     |        |         |
|    |                              | 8\. 1089\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 1097\                                | 2 ( 0.2%)\          |                     |        |         |
|    |                              | 10\. 1101\                               | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 559 others ]                           | 858 (98.0%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 4  | lat\                         | Mean (sd) : 38.5 (4.6)\                  | 876 distinct values | ![](tmp/ds0104.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 25.5 < 39.3 < 48.4\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 6.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 5  | lon\                         | Mean (sd) : -91.7 (15)\                  | 876 distinct values | ![](tmp/ds0105.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | -124.2 < -87.5 < -68\                    |                     |                     |        |         |
|    |                              | IQR (CV) : 18.5 (-0.2)                   |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 6  | state\                       | 1\. California\                          | 85 ( 9.7%)\         | ![](tmp/ds0106.png) | 876\   | 0\      |
|    | [character]                  | 2\. Ohio\                                | 44 ( 5.0%)\         |                     | (100%) | (0%)    |
|    |                              | 3\. Illinois\                            | 38 ( 4.3%)\         |                     |        |         |
|    |                              | 4\. Indiana\                             | 36 ( 4.1%)\         |                     |        |         |
|    |                              | 5\. North Carolina\                      | 35 ( 4.0%)\         |                     |        |         |
|    |                              | 6\. Pennsylvania\                        | 32 ( 3.7%)\         |                     |        |         |
|    |                              | 7\. Michigan\                            | 30 ( 3.4%)\         |                     |        |         |
|    |                              | 8\. Florida\                             | 29 ( 3.3%)\         |                     |        |         |
|    |                              | 9\. Georgia\                             | 28 ( 3.2%)\         |                     |        |         |
|    |                              | 10\. Texas\                              | 27 ( 3.1%)\         |                     |        |         |
|    |                              | [ 39 others ]                            | 492 (56.2%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 7  | county\                      | 1\. Jefferson\                           | 18 ( 2.1%)\         | ![](tmp/ds0107.png) | 876\   | 0\      |
|    | [character]                  | 2\. Cook\                                | 12 ( 1.4%)\         |                     | (100%) | (0%)    |
|    |                              | 3\. Hamilton\                            | 11 ( 1.3%)\         |                     |        |         |
|    |                              | 4\. Lake\                                | 11 ( 1.3%)\         |                     |        |         |
|    |                              | 5\. Los Angeles\                         | 10 ( 1.1%)\         |                     |        |         |
|    |                              | 6\. Wayne\                               | 10 ( 1.1%)\         |                     |        |         |
|    |                              | 7\. Washington\                          | 9 ( 1.0%)\          |                     |        |         |
|    |                              | 8\. Cuyahoga\                            | 7 ( 0.8%)\          |                     |        |         |
|    |                              | 9\. Jackson\                             | 7 ( 0.8%)\          |                     |        |         |
|    |                              | 10\. Madison\                            | 7 ( 0.8%)\          |                     |        |         |
|    |                              | [ 461 others ]                           | 774 (88.4%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 8  | city\                        | 1\. Not in a city\                       | 103 (11.8%)\        | ![](tmp/ds0108.png) | 876\   | 0\      |
|    | [character]                  | 2\. New York\                            | 9 ( 1.0%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. Cleveland\                           | 6 ( 0.7%)\          |                     |        |         |
|    |                              | 4\. Baltimore\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 5\. Chicago\                             | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 6\. Detroit\                             | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 7\. Milwaukee\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 8\. New Haven\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 9\. Philadelphia\                        | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 10\. Springfield\                        | 5 ( 0.6%)\          |                     |        |         |
|    |                              | [ 597 others ]                           | 723 (82.5%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 9  | CMAQ\                     | Mean (sd) : 8.4 (3)\                     | 601 distinct values | ![](tmp/ds0109.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 1.6 < 8.6 < 23.1\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 3.7 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 10 | zcta\                        | 1\. 1022\                                | 1 ( 0.1%)\          | ![](tmp/ds0110.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1103\                                | 2 ( 0.2%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1201\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1608\                                | 2 ( 0.2%)\          |                     |        |         |
|    |                              | 5\. 1832\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1840\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1863\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 8\. 1904\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 2113\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 10\. 2119\                               | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 832 others ]                           | 864 (98.6%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 11 | zcta_area\                   | Mean (sd) : 183173481.9 (542598878.5)\   | 842 distinct values | ![](tmp/ds0111.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 15459 < 37653560.5 < 8164820625\         |                     |                     |        |         |
|    |                              | IQR (CV) : 145836906.5 (3)               |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 12 | zcta_pop\                    | Mean (sd) : 24227.6 (17772.2)\           | 837 distinct values | ![](tmp/ds0112.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 22014 < 95397\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 25207.8 (0.7)                 |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 13 | imp_a500\                    | Mean (sd) : 24.7 (19.3)\                 | 816 distinct values | ![](tmp/ds0113.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 25.1 < 69.6\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 36.5 (0.8)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 14 | imp_a1000\                   | Mean (sd) : 24.3 (18)\                   | 860 distinct values | ![](tmp/ds0114.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 24.5 < 67.5\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 33.3 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 15 | imp_a5000\                   | Mean (sd) : 19.9 (14.7)\                 | 870 distinct values | ![](tmp/ds0115.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 19.1 < 74.6\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 23.3 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 16 | imp_a10000\                  | Mean (sd) : 15.8 (13.8)\                 | 870 distinct values | ![](tmp/ds0116.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 12.4 < 72.1\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 19.6 (0.9)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 17 | imp_a15000\                  | Mean (sd) : 13.4 (13.1)\                 | 870 distinct values | ![](tmp/ds0117.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 9.7 < 71.1\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 17.3 (1)                      |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 18 | county_area\                 | Mean (sd) : 3768701992.1 (6212829553.6)\ | 564 distinct values | ![](tmp/ds0118.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 33703512 < 1690826566.5 < 51947229509\   |                     |                     |        |         |
|    |                              | IQR (CV) : 1761655911.5 (1.6)            |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 19 | county_pop\                  | Mean (sd) : 687298.4 (1293488.7)\        | 564 distinct values | ![](tmp/ds0119.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 783 < 280730.5 < 9818605\                |                     |                     |        |         |
|    |                              | IQR (CV) : 642211 (1.9)                  |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 20 | log_dist_to_prisec\          | Mean (sd) : 6.2 (1.4)\                   | 870 distinct values | ![](tmp/ds0120.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | -1.5 < 6.4 < 10.5\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1.7 (0.2)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 21 | log_pri_length_5000\         | Mean (sd) : 9.8 (1.1)\                   | 586 distinct values | ![](tmp/ds0121.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 8.5 < 10.1 < 12\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 2.2 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 22 | log_pri_length_10000\        | Mean (sd) : 10.9 (1.1)\                  | 687 distinct values | ![](tmp/ds0122.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.2 < 11.2 < 13\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 2 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 23 | log_pri_length_15000\        | Mean (sd) : 11.5 (1.1)\                  | 726 distinct values | ![](tmp/ds0123.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.6 < 11.7 < 13.6\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1.5 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 24 | log_pri_length_25000\        | Mean (sd) : 12.2 (1.1)\                  | 787 distinct values | ![](tmp/ds0124.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 10.1 < 12.5 < 14.4\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 1.4 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 25 | log_prisec_length_500\       | Mean (sd) : 7 (1)\                       | 382 distinct values | ![](tmp/ds0125.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 6.2 < 6.2 < 9.4\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 1.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 26 | log_prisec_length_1000\      | Mean (sd) : 8.6 (0.8)\                   | 591 distinct values | ![](tmp/ds0126.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 7.6 < 8.7 < 10.5\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 1.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 27 | log_prisec_length_5000\      | Mean (sd) : 11.3 (0.8)\                  | 852 distinct values | ![](tmp/ds0127.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 8.5 < 11.4 < 12.8\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 0.9 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 28 | log_prisec_length_10000\     | Mean (sd) : 12.4 (0.7)\                  | 867 distinct values | ![](tmp/ds0128.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.2 < 12.5 < 13.8\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 29 | log_prisec_length_15000\     | Mean (sd) : 13 (0.7)\                    | 869 distinct values | ![](tmp/ds0129.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.6 < 13.1 < 14.4\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 30 | log_prisec_length_25000\     | Mean (sd) : 13.8 (0.7)\                  | 870 distinct values | ![](tmp/ds0130.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 10.1 < 13.9 < 15.2\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 31 | log_nei_2008_pm25_sum_10000\ | Mean (sd) : 4 (2.4)\                     | 828 distinct values | ![](tmp/ds0131.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 4.3 < 9.1\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 3.5 (0.6)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 32 | log_nei_2008_pm25_sum_15000\ | Mean (sd) : 4.7 (2.2)\                   | 855 distinct values | ![](tmp/ds0132.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5 < 9.4\                             |                     |                     |        |         |
|    |                              | IQR (CV) : 2.9 (0.5)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 33 | log_nei_2008_pm25_sum_25000\ | Mean (sd) : 5.7 (2.1)\                   | 860 distinct values | ![](tmp/ds0133.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.9 < 9.7\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.6 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 34 | log_nei_2008_pm10_sum_10000\ | Mean (sd) : 4.3 (2.3)\                   | 829 distinct values | ![](tmp/ds0134.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 4.6 < 9.3\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 3.4 (0.5)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 35 | log_nei_2008_pm10_sum_15000\ | Mean (sd) : 5.1 (2.2)\                   | 855 distinct values | ![](tmp/ds0135.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.4 < 9.7\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.8 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 36 | log_nei_2008_pm10_sum_25000\ | Mean (sd) : 6.1 (2)\                     | 860 distinct values | ![](tmp/ds0136.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 6.4 < 9.9\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.4 (0.3)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 37 | popdens_county\              | Mean (sd) : 551.8 (1711.5)\              | 564 distinct values | ![](tmp/ds0137.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.3 < 156.7 < 26821.9\                   |                     |                     |        |         |
|    |                              | IQR (CV) : 470 (3.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 38 | popdens_zcta\                | Mean (sd) : 1279.7 (2757.5)\             | 840 distinct values | ![](tmp/ds0138.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 610.3 < 30418.8\                     |                     |                     |        |         |
|    |                              | IQR (CV) : 1281.4 (2.2)                  |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 39 | nohs\                        | Mean (sd) : 7 (7.2)\                     | 215 distinct values | ![](tmp/ds0139.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.1 < 100\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 6.1 (1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 40 | somehs\                      | Mean (sd) : 10.2 (6.2)\                  | 230 distinct values | ![](tmp/ds0140.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 9.4 < 72.2\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 8 (0.6)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 41 | hs\                          | Mean (sd) : 30.3 (11.4)\                 | 347 distinct values | ![](tmp/ds0141.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 30.8 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 12.3 (0.4)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 42 | somecollege\                 | Mean (sd) : 21.6 (8.6)\                  | 240 distinct values | ![](tmp/ds0142.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 21.3 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 7.2 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 43 | associate\                   | Mean (sd) : 7.1 (4)\                     | 157 distinct values | ![](tmp/ds0143.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 7.1 < 71.4\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 3.9 (0.6)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 44 | bachelor\                    | Mean (sd) : 14.9 (9.7)\                  | 301 distinct values | ![](tmp/ds0144.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 12.9 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 10.4 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 45 | grad\                        | Mean (sd) : 8.9 (8.6)\                   | 245 distinct values | ![](tmp/ds0145.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 6.7 < 100\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 7.1 (1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 46 | pov\                         | Mean (sd) : 15 (11.3)\                   | 345 distinct values | ![](tmp/ds0146.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 12.1 < 65.9\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 14.7 (0.8)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 47 | hs_orless\                   | Mean (sd) : 47.5 (16.8)\                 | 464 distinct values | ![](tmp/ds0147.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 48.7 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 21.2 (0.4)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 48 | urc2013\                     | Mean (sd) : 2.9 (1.5)\                   | 1 : 203 (23.2%)\    | ![](tmp/ds0148.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        | 2 : 163 (18.6%)\    |                     | (100%) | (0%)    |
|    |                              | 1 < 3 < 6\                               | 3 : 228 (26.0%)\    |                     |        |         |
|    |                              | IQR (CV) : 2 (0.5)                       | 4 : 123 (14.0%)\    |                     |        |         |
|    |                              |                                          | 5 : 101 (11.5%)\    |                     |        |         |
|    |                              |                                          | 6 :  58 ( 6.6%)     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 49 | urc2006\                     | Mean (sd) : 3 (1.5)\                     | 1 : 195 (22.3%)\    | ![](tmp/ds0149.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        | 2 : 162 (18.5%)\    |                     | (100%) | (0%)    |
|    |                              | 1 < 3 < 6\                               | 3 : 221 (25.2%)\    |                     |        |         |
|    |                              | IQR (CV) : 2 (0.5)                       | 4 : 127 (14.5%)\    |                     |        |         |
|    |                              |                                          | 5 : 115 (13.1%)\    |                     |        |         |
|    |                              |                                          | 6 :  56 ( 6.4%)     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 50 | aod\                         | Mean (sd) : 43.7 (19.6)\                 | 581 distinct values | ![](tmp/ds0150.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 5 < 40.2 < 143\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 18 (0.4)                      |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+

</details>

We can see that for many variables there are many low values as the distribution shows two peaks, one near zero and another with a higher value. 
This is true for the `imp` variables (measures of development), the `nei` variables (measures of emission sources) and the road density variables. 
We can also see that the range of some of the variables is very large, in particular the area and population related variables.

### Evaluate correlation among possible predictors

In prediction analyses, it is also useful to evaluate if any of the variables are correlated.

AVOCADO: maybe we should add an explanation of why we would care about correlated variables in a model? 

Intuitively we can expect some of our variables to be correlated.

Let's first take a look at all of our numeric variabels with the`corrplot` package:
The `corrplot` package is another option to look at correlation among possible predictors, and particularly useful if we have many predictors. 
First, we calculate the Pearson correlation coefficients between all features pairwise using the `cor()` function of the `stats` package (which is loaded automatically). Then we use the `corrplot::corrplot()` function. 

```{r}
PM_cor <- cor(pm %>% dplyr::select_if(is.numeric))
corrplot::corrplot(PM_cor, tl.cex = 0.5)
```
The `tl.cex = 0.5` argument controls the size of the text label. 

We can also plot the absolute value of the Pearson correlation coefficients using the `abs()` function from base R and change the order of the columns.  
```{r}
corrplot(abs(PM_cor), order = "AOE", tl.cex = 0.5, cl.lim = c(0, 1))
```
AVOCADO: I read the help file, but I'm still not sure what "AOE" means? Either, I think if we change a default parameter argument, then we should explain it here. 

The `cl.lim = c(0, 1)` argument limits the color label to be between 0 and 1. 

Next, we can change the visualization method (`method = "color"`) with colored 
boxes, only show the upper half of the plot (`type = "upper"`) without the diagnoal 
(`diag = FALSE), change the order of the columns (`order = "FPC"`), and change
the color palatte using the `RColorBrewer` package. 

```{r}
corrplot(PM_cor, method = "color",  type = "upper", 
         diag = FALSE, order = "FPC", tl.cex = 0.5,  
         col = RColorBrewer::brewer.pal(n = 8, name = "PuOr"))
```

Also notice that none of the predictors are highly correlated with our outcome variable (`value`).

The `ggcorrplot()` function from the `ggcorrplot` package is also very useful.
```{r}
ggcorrplot(PM_cor, hc.order = TRUE, type = "lower", tl.cex = 5)
```

AVOCADO: So I think if we introduce more than one package to do a similar task, I think we have to explain what's the adv or disadv of both. Otherwise, it feels like we are just showing the student to just show them but without clear guidance on when to use which function? 

We can see that the development variables (`imp`) variables are correlated with each other as we might expect. 
We also see that the road density variables seem to be correlated with each other, and the emission variables seem to be correlated with each other. 
We can take a closer look  using the `ggcorr()` function and the `ggpairs()` function of the `GGally` package. 
To select our variables of interest we can use the `select()` function with the `contains()` function of the `tidyr` package. 

First let's look at the `imp`/development variables. 
We can change the default color palette (`palette = "RdBu"`) and add on 
correlation coefficients to the plot (`label = TRUE`).

```{r, out.width = "400px"}
select(pm, contains("imp")) %>%
  ggcorr(palette = "RdBu", label = TRUE)

select(pm, contains("imp")) %>%
  ggpairs()
```



Indeed, we can see that `imp_a1000` and `imp_a500` are highly correlated, as well as `imp_a10000`, `imp_a15000`.

Next, let's take a look at the road density data:

```{r, fig.weight=12}
select(pm, contains("pri")) %>%
  ggcorr(palette = "RdBu", hjust = .85, size = 3,
       layout.exp=2, label = TRUE)
```

We can see that many of the road density variables are highly correlated with one another, while others are less so.

Finally let's look at the emission variables.

```{r}
select(pm, contains("nei")) %>%
  ggcorr(palette = "RdBu", hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

select(pm, contains("nei")) %>%
  ggpairs()
```

We would also expect the population density data might correlate with some of these variables. 
Let's take a look.

```{r}
pm %>%
select(log_nei_2008_pm25_sum_10000, popdens_county, 
       log_pri_length_10000, imp_a10000) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

pm %>%
select(log_nei_2008_pm25_sum_10000, popdens_county, 
       log_pri_length_10000, imp_a10000, county_pop) %>%
  ggpairs()
```


Interesting, so these variables don't appear to be highly correlated, therefore we might need variables from each of the categories to predict our monitor PM~2.5~ pollution values.

Because some variables in our data have extreme values, it might be good to take a log transformation. This can affect our estimates of correlation. 
```{r}
pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, 
       log_pri_length_10000, imp_a10000) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
  mutate(log_pop_county = log(county_pop)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, 
       log_pri_length_10000, imp_a10000, log_pop_county) %>%
  ggpairs()
```

Indeed this increased the correlation, but variables from each of these categories may still prove to be useful for prediction.


Now that we have a sense of what our data are, we can get started with building a machine learning model to predict air pollution. 

## What is machine learning? 

You may have learned about the central dogma of statistics that you sample from a population

![](img/cdi1.png)

Then you try to guess what will happen in the population from the sample. 

![](img/cdi2.png)

For prediction we have a similar sampling problem

![](img/cdp1.png)

But now we are trying to build a rule that can be used to predict a single observation's value of some characteristic using the others. 

![](img/cdp2.png)

Let's make this more concrete.

If you recall from the [What are the data?](#whatarethedata) section above, when we are using machine learning for prediction, our data consists of: 

1. An **continuous** outcome variable that we want to predict 
2. A set of feature(s) (or predictor variables) that we use to predict the outcome variable

We will use $Y$ to denote the outcome variable and $X = (X_1, \dots, X_p)$ to denote $p$ different features (or predictor variables). 
Because our outcome variable is **continuous** (as opposed to categorical), we are interested in a particular type of machine learning algorithm. 

Our goal is to build a machine learning algorithm uses the features $X$ as input and predicts a outcome variable (or air pollution levels) in the situation where we do not know the outcome variable. 

The way we do this is to use data where we have both the features $(X_1=x_1, \dots X_p=x_p)$ and the actual outcome $Y$ data to _train_ a machine learning algorithm to predict the outcome, which we call $\hat{Y}$. 
When we say train a machine learning algorithm we mean estimate a function $f$ that uses the predictor variables $X$ as input or $\hat{Y} = f(X)$. 

### ML as an optimization problem

If we are doing a good job, then our predicted outcome $\hat{Y}$ should closely match our actual outcome $Y$ that we observed. 

In this way, we can think of machine learning (ML) as an optimization problem that tries to minimize the distance between $\hat{Y} = f(X)$ and $Y$. 

$$d(Y - f(X))$$
The choice of distance metric $d(\cdot)$ can be the mean of the absolute or squared difference or something more complicated. 
Much of the fields of statistics and computer science are focused on defining $f$ and $d$.

### The parts of an ML problem

To set up a machine learning (ML) problem, we need a few components.
To solve a (standard) machine learning problem you need: 

1. A data set to train from. 
2. An algorithm or set of algorithms you can use to try values of $f$
3. A distance metric $d$ for measuring how close $Y$ is to $\hat{Y}$
4. A definition of what a "good" distance is

While each of these components is a _technical_ problem, there has been a ton of work addressing those technical details. The most pressing open issue in machine learning is realizing that though these are _technical_ steps they are not _objective_ steps. In other words, how you choose the data, algorithm, metric, and definition of "good" says what you value and can dramatically change the results. A couple of cases where this was a big deal are: 

1. [Machine learning for recidivism](https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing) - people built ML models to predict who would re-commit a crime. But these predictions were based on historically biased data which led to biased predictions about who would commit new crimes. 
2. [Deciding how self driving cars should act](https://www.nature.com/articles/d41586-018-07135-0) - self driving cars will have to make decisions about how to drive, who they might injure, and how to avoid accidents. Depending on our choices for $f$ and $d$ these might lead to wildly different kinds of self driving cars. Try out the [moralmachine](http://moralmachine.mit.edu/) to see how this looks in practice. 

Now that we know a bit more about machine learning, let's build a model to predict air pollution levels using the `tidymodels` framework. 

## Machine learning with `tidymodels`


The goal is to build a machine learning algorithm uses the features as input and predicts a outcome variable (or air pollution levels) in the situation where we do not know the outcome variable. 
The way we do this is to use data where we have both the input and output data to _train_ a machine learning algorithm. 
To train a machine learning algorithm, we will use the `tidymodels` package ecosystem. 

#### The tidymodels ecosystem

To perform our analysis we will be using the `tidymodels` suite of packages. 
You may be familiar with the older packages `caret` or `mlr` which are also for machine learning and modeling but are not a part of the `tidyverse`. 
[Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"} describes `tidymodels` like this:

> "Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: pre-processing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret.
The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do."

There are many R packages in the `tidymodels` ecosystem, which assist with various steps in the process of building a machine learning algorithm:

AVOCADO: Is it possible to get a higher resolution figure here?

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","simpletidymodels.png"))
```

This is a schematic of how these packages work together to build a machine learning algorithm:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","MachineLearning.png"))
```

#### Benefits of tidymodels  

The two major benefits of `tidymodels` are: 

1. Standardized workflow/format/notation across different types of machine learning algorithms  

Different notations are required for different algorithms as the algorithms have been developed by different people. This would require the painstaking process of reformatting the data to be compatible with each algorithm if multiple algorithms were tested.

2. Can easily modify pre-processing, algorithm choice, and hyper-parameter tuning making optimization easy  

Modifying a piece of the overall process is now easier than before because many of the steps are specified using the `tidymodels` packages in a convenient manner. Thus the entire process can be rerun after a simple change to pre-processing without much difficulty.



### Splitting the data

The first step after data exploration in machine learning analysis is to [split the data](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7){target="_blank"} into **training** and **testing** data sets. 

The training data set will be used to build and tune our model. 
This is the data that the model "learns" on. 
The testing data set will be used to evaluate the performance of our model in a more generalizable way. What do we mean by "generalizable"?

Remember that our main goal is to use our model to be able to predict air pollution levels in areas where there are no gravimetric monitors. 
Therefore, if our model is really good at predicting air pollution with the data that we use to build it, it might not do the best job for the areas where there are few to no monitors. 
This would cause us to have really good prediction accuracy and we might assume that we were going to do a good job estimating air pollution any time we use our model, but in fact this would likely not be the case. 
This situation is what we call **[overfitting](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6){target="_blank"} **.

Overfitting happens when we end up modeling not only the major relationships in our data but also the noise within our data. 

```{r, echo=FALSE}
knitr::include_graphics("https://miro.medium.com/max/1110/1*tBErXYVvTw2jSUYK7thU2A.png")
```

##### [[source]](https://miro.medium.com/max/1110/1*tBErXYVvTw2jSUYK7thU2A.png){target="_blank"}

If we get good prediction with our testing set, then we know that our model can be applied to other data and will likely perform well. We will discuss this more later.

We will not touch the testing set until we have completed optimizing our model with the training set. 
This will allow us to have a less biased evaluation of how well our model can do with other data besides the data used in the training set to build the model. 
Ideally, you would also want a completely independent data set to further test the performance of your model.

To split the data into training and testing, we will use the `initial_split()` function in the `rsample` package to specify how we want to split our data.

AVOCADO: should we use "sample" or "observation"? Also, should we use "predictor" or "feature" in the figure? Could the figure be made to be more horizontal where the yellow and blue boxes are on top of each other? 

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","split.png"))
```

```{r}
set.seed(1234)
pm_split <- rsample::initial_split(data = pm, prop = 2/3)
pm_split
```

A couple of notes from the code above: 

- Typically, data are split into 3/4 of the observations for training and 1/4 for testing. This is the default proportion and does not need to be specified. However, you can change the proportion using the `prop` argument, which we will do that here for illustrative purposes.
- Since the split is performed randomly, it is a good idea to use the `set.seed()` function in base R to ensure that if your rerun your code that your split will be the same next time.
- We can see the number of monitors in our training, testing, and original data by typing in the name of our split object. The result will look like this:
<training data sample number, testing data sample number, original sample number> 

Now, you can also specify a variable to stratify by with the `strata` argument. 
This is useful if you have imbalanced categorical variables and you would like to intentionally make sure that there are similar number of samples of the rarer categories in both the testing and training sets. 
Otherwise the split is performed randomly. 

AVOCADO: Is this a quote from someone/some place? If so should we add a citation / URL? 

> The strata argument causes the random sampling to be conducted within the stratification variable. The can help ensure that the number of data points in the training data is equivalent to the proportions in the original data set.

In the case with our data set, perhaps we would like our training set to have similar proportions of monitors from each of the states as in the initial data. 
This might be useful if we want our model to be generalizable across all of the states.

We can see that indeed there are different proportions of monitors in each state by using the `count()` function of the `dplyr` package. 

AVOCADO: why `print(n = 1e3)`? it's a same question as above? 

#### {.scrollable }
```{r}
# Scroll through the output!
count(pm, state) %>%
  print(n = 1e3)
```
####

If our data set were large enough it might be nice then to stratify by state using the `strata = "state"` argument in `initial_split()`, but our data is unfortunately not large enough. 

Importantly the `initial_split` function only determines what rows of our `pm` data frame should be assigned for training or testing, it does not actually split the data. 

To extract the testing and training data we can use the `training()` and `testing()` functions also of the `rsample` package.

#### {.scrollable }
```{r}
train_pm <-rsample::training(pm_split)
test_pm <-rsample::testing(pm_split)
 
# Scroll through the output!
count(train_pm, state)
count(test_pm, state)
```
####



### Pre-processing the data

After splitting the data, the next step is to process the training and testing data so that the data are are compatible and optimized to be used with the model. 
This involves assigning variables to specific roles within the model and pre-processing like scaling variables and removing redundant variables. 
This process can also be called feature engineering.

To do this in `tidymodels`, we will create a what's called a "recipe" using the `recipes` package, which is a standardized format for a sequence of steps for processing the data.
This can be very useful because it makes testing out different pre-processing steps or different algorithms with the same pre-processing very easy and reproducible.
Creating a recipe specifies **how a data frame of predictors should be created** - it specifies what  variables to be used  and the pre-processing steps, but it **does not execute these steps** or create the data frame of predictors.

#### Step 1: Specify the variables with `recipe()` function

The first thing to do to create a recipe is to specify which variables we will be using as our outcome and predictors using the `recipe()` function. 
In terms of the metaphor of baking, we can think of this as listing our ingredients. 
Translating this to the `recipes` package, we use the `recipe()` function to assign roles to all the variables. 

Let's try the simplest recipe with no pre-processing steps: simply list the outcome and predictor variables.

We can do so in two ways:  

1) Using formula notation  
2) Assigning roles to each variable  

Let's look at the first way using formula notation, which looks like this:  

outcome(s) ~ predictor(s)  

If in the case of multiple predictors or a multivariate situation with two outcomes, use a plus sign  

outcome1 + outcome2 ~ predictor1 + predictor2  

If we want to include all predictors we can use a period like so:  

outcome_variable_name ~ .  

Now with our data, we will start by making a recipe for our training data.
If you recall, the continuous outcome variable is `value` (the average annual gravimetric monitor PM~2.5~ concentration in ug/m^3^). 
Our features (or predictor variables) are all the other variables except the monitor ID, which is an `id` variable.

The reason not to include the `id` variable is because this variable includes the county number and a number designating which particular monitor the values came from of the monitors there are in that county. 
Since this number is arbitrary and the county information is also given in the data, and the fact that each monitor only has one value in the `value` variable, nothing is gained by including this variable and it may instead introduce noise. 
However, it is useful to keep this data to take a look at what is happening later. 
We will show you what to do in this case in just a bit.

In the simplest case, we might use all predictors like this:

```{r}
simple_rec <- train_pm %>%
  recipes::recipe(value ~ .)

simple_rec
```

We see a recipe has been created with 1 outcome variable and 49 predictor variables (or features). 
Also, notice how we named the output of `recipe()`. 
The naming convention for recipe objects is `*_rec` or `rec`. 

Now, let's get back to the `id` variable. 
Instead of including it as a predictor variable, we could also use the `update_role()` function of the `recipes` package.

```{r}
simple_rec <- train_pm %>%
  recipes::recipe(value ~ .) %>%
  recipes::update_role(id, new_role = "id variable")

simple_rec
```

<details><summary> Click here learn more about the working with `id` variables </summary>

This option works well with the newer `workflows` package, however `id` variables are often dropped from analyses that do not use this newer package as they can make the process difficult with using the `parsnip` package alone due to the fact that new levels (or possible values) may be introduced with the testing data.

</details>

We could also specify the outcome and predictors in the same way as the id variable. 
Please see [here](https://tidymodels.github.io/recipes/reference/recipe.html){target="_blank"} for examples of other roles for variables. 
The role can be actually be any value. 

The order is important here, as we first make all variables predictors and then override this role for the outcome and `id` variable. 
We will use the `everything()` function of the `dplyr` package to start with all of the variables in `train_pm`.

```{r}
simple_rec <-recipe(train_pm) %>%
    update_role(everything(), new_role = "predictor")%>%
    update_role(value, new_role = "outcome")%>%
    update_role(id, new_role = "id variable")

simple_rec
```

If we want to take a look at our formula from our recipe, we can do use the `formula()` function of the `stats` package.

```{r}
formula(simple_rec)
```

We can also view our recipe in more detail using the base `summary()` function.

```{r}
summary(simple_rec)
```

To summarize this step, we use the `recipe()` function to assign roles to all the variables: 

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","Starting_a_recipe_recipes1.png"))
```


#### Step 2: Specify the pre-processing steps with `step*()` functions

Next, we use the `step*()` functions from the `recipe` package to specify pre-processing steps. 

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","Making_a_recipe_recipes2.png"))
```

**This [link](https://tidymodels.github.io/recipes/reference/index.html){target="_blank"} and this [link](https://cran.r-project.org/web/packages/recipes/recipes.pdf){target="_blank"} show the many options for recipe step functions.**

<u>There are step functions for a variety of purposes:</u>

1. [**Imputation**](https://en.wikipedia.org/wiki/Imputation_(statistics)){target="_blank"} -- filling in missing values based on the existing data 
2. [**Transformation**](https://en.wikipedia.org/wiki/Data_transformation_(statistics)){target="_blank"} -- changing all values of a variable in the same way, typically to make it more normal or easier to interpret
3. [**Discretization**](https://en.wikipedia.org/wiki/Discretization_of_continuous_features){target="_blank"} -- converting continuous values into discrete or nominal values - binning for example to reduce the number of possible levels (However this is generally not advisable!)
4. [**Encoding / Creating Dummy Variables**](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)){target="_blank"} -- creating a numeric code for categorical variables
([**More on Dummy Variables and one hot encoding**](https://medium.com/p/b5840be3c41a/responses/show){target="_blank"})
5. [**Data type conversions**](https://cran.r-project.org/web/packages/hablar/vignettes/convert.html){target="_blank"}  -- which means changing from integer to factor or numeric to date etc.
6. [**Interaction**](https://statisticsbyjim.com/regression/interaction-effects/){target="_blank"}  term addition to the model -- which means that we would be modeling for predictors that would influence the capacity of each other to predict the outcome
7. [**Normalization**](https://en.wikipedia.org/wiki/Normalization_(statistics)){target="_blank"} -- centering and scaling the data to a similar range of values
8. [**Dimensionality Reduction/ Signal Extraction**](https://en.wikipedia.org/wiki/Dimensionality_reduction){target="_blank"} -- reducing the space of features or predictors to a smaller set of variables that capture the variation or signal in the original variables (ex. Principal Component Analysis and Independent Component Analysis)
9. **Filtering** -- filtering options for removing variables (ex. remove variables that are highly correlated to others or remove variables with very little variance and therefore likely little predictive capacity)
10. [**Row operations**](https://tartarus.org/gareth/maths/Linear_Algebra/row_operations.pdf){target="_blank"} -- performing functions on the values within the rows  (ex. rearranging, filtering, imputing)
11. **Checking functions** -- Sanity checks to look for missing values, to look at the variable classes etc.

All of the step functions look like `step_*()` with the `*` replaced with a name, except for the check functions which look like `check_*()`.

There are several ways to select what variables to apply steps to:  

1. Using `tidyselect` methods: `contains()`, `matches()`, `starts_with()`, `ends_with()`, `everything()`, `num_range()`  
2. Using the type: `all_nominal()`, `all_numeric()` , `has_type()` 
3. Using the role: `all_predictors()`, `all_outcomes()`, `has_role()`
4. Using the name - use the actual name of the variable/variables of interest  

Let's try adding some steps to our recipe.

We might consider log transforming our population and area variables (that aren't densities) - let's take a look at the range of these variables.
```{r, eval = FALSE}
pm %>%
  select(matches("_pop|_area")) %>%
  map(range)
```

AVOCADO: should we explain `map()`? 
AVOCADO: unclear why this is commented out? The next sentence suggests it was executed but I don't think the code works? 

We can see that the range for each of these variables is quite large, we can log transform this data using the `step_log()` function of the `recipes` package.

We would also want to potentially [one hot encode](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/){target="_blank"} some of our categorical variables so that they can be used with certain algorithms. 
We can do this with the `step_dummy()` function and the `one_hot = TRUE` argument. 
One hot encoding means that we do not simply encode our categorical variables numerically, as our numeric assignments can be interpreted by algorithms as having a particular rank or order. 
Instead, binary variables made of 1s and 0s are used to arbitrarily assign a numeric value that has no apparent order.

```{r}
simple_rec %>%
  step_dummy(state, county, city, zcta, one_hot = TRUE)
```

Our `fips` variable includes a numeric code for state and county - and therefore is essentially a proxy for county.
Since we already have county, we will just use it and keep the `fips` ID as another ID variable.

We can remove the `fips` variable from the predictors using `update_role()` to make sure that the role is no longer `"predictor"`. 
We can make the role anything we want actually, so we will keep it something identifiable.

```{r}
simple_rec %>%
  update_role("fips", new_role = "county id")
```

We might also want to remove variables that appear to be redundant and are highly correlated with others, as we know from our exploratory data analysis that many of our variables are correlated with one another. 
We can do this using the `step_corr()` function.

```{r}
simple_rec %>%
  step_corr(all_predictors(), - CMAQ, - aod)
```

Notice, we don't want to remove some of our variables, like the `CMAQ` and `aod` variables. 

It is also a good idea to remove variables with near-zero variance, which can be done with the `step_nzv()` function. 
Variables have low variance if all the values are very similar, the values are very sparse, or if they are highly imbalanced. 

```{r}
simple_rec %>%
  step_nzv(all_predictors(), - CMAQ, - aod)
```

<details><summary> Click here to learn about examples where you might have near-zero variance variables</summary>

1) **Similar Values** - If the population density was nearly the same for every zcta that contained a monitor, then knowing the population density near our monitor would contribute little to our model in assisting us to predict monitor air pollution values. 
2) **Sparse Data** - If all of the monitors were in locations where the populations did not attend graduate school, then these values would mostly be zero, again this would do very little to help us distinguish our air pollution monitors.When many of the values are zero this is also called sparse data.  
3) **Imbalanced Data** If nearly all of the monitors were located in one particular state, and all the others only had one monitor each, then the real predictive value would simply be in knowing if a monitor is located in that particular state or not. In this case we don't want to remove our variable, we just want to simplify it.

See this [blog post](https://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/){target="_blank"} about why removing near-zero variance variables isn't always a good idea if we think that a variable might be especially informative.

</details>

Let's put all this together now. 

**Remember: it is important to add the steps to the recipe in an order that makes sense just like with a cooking recipe.**

First, we are going to create numeric values for our categorical variables, then we will look at correlation and near-zero variance. 
We do not want to remove the `CMAQ` and `aod` variables, so we can make sure they are kept in the model by excluding them from those steps. 
If we specifically wanted to remove a predictor we could use `step_rm()`.

```{r}
simple_rec %<>%
  update_role("fips", new_role = "county id") %>%
  step_dummy(state, county, city, zcta, one_hot = TRUE) %>%
  step_corr(all_predictors(), - CMAQ, - aod)%>%
  step_nzv(all_predictors(), - CMAQ, - aod)
  
simple_rec
```



### Running the pre-processing

#### Step 1: Update the recipe with training data using `prep()`

The next major function of the `recipes` package is `prep()`.
This function updates the recipe object based on the training data. 
It estimates parameters (estimating the required quantities and statistics required by the steps for the variables) for pre-processing and updates the model terms, as some of the predictors may be removed, this allows the recipe to be ready to use on other data sets. 
It **does not necessarily actually execute the pre-processing itself**, however we will specify in argument for it to do this so that we can take a look at the pre-processed data.

AVOCADO: I'm not sure I get what "updates the model terms" in the paragraph above means. I didn't think a model had been specified yet? 

There are some important arguments to know about:

1. `training` - you must supply a training data set to estimate parameters for pre-processing operations (recipe steps) - this may already be included in your recipe - as is the case for us
2. `fresh` - if `fresh=TRUE`, - will retrain and estimate parameters for any previous steps that were already prepped if you add more steps to the recipe
3. `verbose` - if `verbose=TRUE`, shows the progress as the steps are evaluated and the size of the pre-processed training set
4. `retain` - if `retain=TRUE`, then the pre-processed training set will be saved within the recipe (as template). This is good if you are likely to add more steps and do not want to rerun the `prep()` on the previous steps. However this can make the recipe size large. This is necessary if you want to actually look at the pre-processed data.

Let's try out the `prep()` function: 

```{r}
prepped_rec <- prep(simple_rec, verbose = TRUE, retain = TRUE )
names(prepped_rec)
```

There are also lots of useful things to checkout in the output of `prep()`.
You can see:

1. the `steps` that were run  
2. the variable info (`var_info`)  
3. the model (`term_info`)
4. the new `levels` of the variables 
5. the original levels of the variables (`orig_lvls`)
6. info about the training data set size and completeness (`tr_info`)

**Note**: You may see the `prep.recipe()` function in material that you read about the `recipes` package. This is referring to the `prep()` function of the `recipes` package.


#### Step 2: Extracting pre-processed training data using `juice()`

Since we retained our pre-processed training data (i.e. `prep(retain=TRUE)`), we can take a look at it like by using the `juice()` function of the `recipes` package like this:

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","training_preprocessing_recipes3.png"))
```

Let's juice! 


#### {.scrollable }
```{r}
# Scroll through the output!
juiced_train <- juice(prepped_rec)
glimpse(juiced_train)
```
####


For easy comparison sake - here is our original data:

#### {.scrollable }

```{r}
# Scroll through the output!
glimpse(pm)
```
####

Notice how we only have 36 variables now instead of 50! 
Two of these are our ID variables (`fips` and the actual monitor ID (`id`)) and one is our outcome (`value`). 
Thus we only have 33 predictors now. 
We can also see that variables that we no longer have any categorical variables. 
Variables like `state` are gone and only `state_California` remains as it was the only state identity to have nonzero variance.
We can also see that there were more monitors listed as `"Not in a city"` than any city. 

We can see that California had the largest number of monitors compared to the other states.

#### {.scrollable }

```{r}
#Scroll through the output!
pm %>% count(state)  %>%
  print(n = 1e3)
```

####

#### {.scrollable }

```{r}
#Scroll through the output!
pm %>% count(city) %>%
  print(n = 1e3)
```

####

**Note**: Recall that you must specify `retain = TRUE` argument of the `prep()` function to use `juice()`.

#### Step 3: Extracting pre-processed testing data using `bake()`

According to the `tidymodels` documentation:

> `bake()` takes a trained recipe and applies the operations to a data set to create a design matrix.
 For example: it applies the centering to new data sets using these means used to create the recipe

Therefore, if you wanted to look at the pre-processed testing data you would use the `bake()` function of the `recipes` package.
(You generally want to leave your testing data alone, but it is good to look for issues like the introduction of NA values).

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","testing_preprocessing_recipes4.png"))
```

Let's bake! 

#### {.scrollable }
```{r,}
# Scroll through the output!
baked_test_pm <- recipes::bake(prepped_rec, new_data = test_pm)
glimpse(baked_test_pm)
```
####


Notice that our `city_Not.in.a.city` variable seems to be NA values. 
Why might that be?

Ah! Perhaps it is because some of our levels were not previously seen in the training set!

Let's take a look using the [set operations](https://www.probabilitycourse.com/chapter1/1_2_2_set_operations.php){target="_blank"} of the `dplyr` package. 
We can take a look at cities that were different between the test and training set.

```{r}
traincities <- train_pm %>% distinct(city)
testcities <- test_pm %>% distinct(city)

#get the number of cities that were different
dim(dplyr::setdiff(traincities, testcities))

#get the number of cities that overlapped
dim(dplyr::intersect(traincities, testcities))
```

Indeed, there are lots of different cities in our test data that are not in our training data!

AVOCADO: yeah, I would remove the paragraph below. 

Maybe remove this?: Thus we need to update our original recipe to include a very important step function called `step_novel()` this helps in cases like this were there are new factors in our testing set that were not in our training set. It is a good idea to include this in most of your recipes where you have a categorical variables with many distinct values. This step needs to come before we create dummy variables. However, we are also creating a dummy variable from this, which still results in a problem. 

Next, let go back to our `pm` data set and modify the `city` variable to be values of `in a city` or `not in a city` using the `case_when()` function of `dplyr`.
This function allows you to vectorize multiple `if_else()` statements.

```{r}
pm %>%
  mutate(city = case_when(city == "Not in a city" ~ "Not in a city",
                          city != "Not in a city" ~ "In a city"))
```

Alternatively you could create a [custom step function](https://recipes.tidymodels.org/articles/Custom_Steps.html){target="_blank"} to do this and add the step function to your recipe, but that is beyond the scope of this case study. 

We will need to repeat all the steps (splitting the data, pre-processing, etc) as the levels of our variables have now changed. 

While we are doing this, we might also have this issue for `state` and `county`. 
So let's also do a similar thing for `state`. 
The `county` variables appears to get dropped due to either correlation or near zero variance. 
It is likely due to near zero variance because this is the more granular of these geographic categorical variables and likely sparse.

```{r}
pm %<>%
  mutate(city = case_when(city == "Not in a city" ~ "Not in a city",
                          city != "Not in a city" ~ "In a city"))

set.seed(1234) # same seed as before
pm_split <-rsample::initial_split(data = pm, prop = 2/3)
pm_split
 train_pm <-rsample::training(pm_split)
 test_pm <-rsample::testing(pm_split)

  
novel_rec <-recipe(train_pm) %>%
    update_role(everything(), new_role = "predictor") %>%
    update_role(value, new_role = "outcome") %>%
    update_role(id, new_role = "id variable") %>%
    update_role("fips", new_role = "county id") %>%
    step_dummy(state, county, city, zcta, one_hot = TRUE) %>%
    step_corr(all_numeric()) %>%
    step_nzv(all_numeric()) 
```

Now let's retrain our training data and try baking our test data:

```{r}
prepped_rec <- prep(novel_rec, verbose = TRUE, retain = TRUE)
```


#### {.scrollable }
```{r}
# Scroll through the output!
juiced_train <- juice(prepped_rec)

glimpse(juiced_train)
```

####

Notice, it looks like we gained the `log_prisec_length_25000` back with this recipe using the data with our changes to `state` and `city`.

#### {.scrollable }

```{r}
# Scroll through the output!
baked_test_pm <- recipes::bake(prepped_rec, new_data = test_pm)

glimpse(baked_test_pm)
```

####

Great now we no longer have `NA` values! :)

**Note**: if you use the skip option for some of the pre-processing steps, be careful. 
`juice()` will show all of the results ignoring `skip = TRUE`. 
`bake()` will not necessarily conduct these steps on the new data.


### Specifying the model

So far we have used the packages `rsample` to split the data and `recipes` to assign variable types, and to specify and prep our pre-processing (as well as to optionally extract the pre-processed data).

We will now use the `parsnip` package (which is similar to the previous `caret` package - and hence why it is named after the vegetable) to specify our model.

There are four things we need to define about our model:  

1. The **type** of model (using specific functions in parsnip like `rand_forest()`, `logistic_reg()` etc.)  
2. The package or **engine** that we will use to implement the type of model selected (using the `set_engine()` function) 
3. The **mode** of learning - classification or regression (using the `set_mode()` function) 
4. Any **arguments** necessary for the model/package selected (using the `set_args()`function -  for example the `mtry =` argument for random forest which is the number of variables to be used as options for splitting at each tree node)

Let's walk through these steps one by one. 
For our case, we are going to start our analysis with a linear regression but we will demonstrate how we can try different models.

The first step is to define what type of model we would like to use. 
See [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html){target="_blank"} for modeling [options] in `parsnip`.

AVOCADO: looks like the "[options]" above is missing a link?  

```{r}
PM_model <- parsnip::linear_reg() # PM for particulate matter
PM_model
```

OK. So far, all we have defined is that we want to use a linear regression...  
Let's tell `parsnip` more about what we want.

We would like to use the ordinary least squares method to fit our linear regression. 
So we will tell `parsnip` that we want to use the `lm` package to implement our linear regression (there are many options actually such as [`rstan`](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html){target="_blank"}  [`glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html){target="_blank"}, [`keras`](https://keras.rstudio.com/){target="_blank"}, and [`sparklyr`](https://therinspark.com/starting.html#starting-sparklyr-hello-world){target="_blank"}). 
We will do so by using the `set_engine()` function of the `parsnip` package.

```{r}
lm_PM_model <- 
  PM_model  %>%
  parsnip::set_engine("lm")

lm_PM_model
```

In some cases some packages can do either classification or prediction, so it is a good idea to specify which mode you intend to perform. 
Here, we aim to predict the air pollution. 
You can do this with the `set_mode()` function of the `parsnip` package, by using either `set_mode("classification")` or `set_mode("regression")`.

```{r}
lm_PM_model <- 
  PM_model  %>%
  parsnip::set_engine("lm") %>%
  set_mode("regression")

lm_PM_model
```

### Fitting the model

At the moment, there are actually a few ways to fit our model. 

1. We can use the `parsnip` package and then assess our fit using the `yardstick` package. 
2. We can also the `parsnip` package with a newer package called `workflows` (still under development) that allows us to keep track of both our pre-processing steps and our model specification. It also allows us to implement fancier optimizations in an automated way and it is currently being developed to also handle post-processing operations. 

AVOCADO: I feel like we should do a bit more explaining or maybe draw a figure of these two approaches? 

Here we will talk about the second way to fit out model using the `workflows` and `parsnip` package. 

We begin by creating a workflow using the `workflow()` function in the `workflows` package. 
Next, we use `add_recipe()` (our pre-processing specifications) and we add our model with the `add_model()` function -- both functions from the `workflows` package.

**Note**: We do not need to actually `prep()` our recipe before using workflows!

If you recall `novel_rec` is the recipe we previously created with the `recipes` package and `lm_PM_model` is where we specified our model with the `parsnip` package.
Here, we combine everything together into a `workflow()`. 

```{r}
PM_wflow <-workflows::workflow() %>%
           workflows::add_recipe(novel_rec) %>%
           workflows::add_model(lm_PM_model)
PM_wflow
```

Ah, nice. 
Notice how it tells us about both our pre-processing steps and our model specifications.

Next, we "prepare the recipe" (or estimate the parameters) and fit the model to our training data all at once. 
Printing the output, we can see the coefficients of the model.

```{r}
PM_wflow_fit <- parsnip::fit(PM_wflow, data = train_pm)
PM_wflow_fit
```


<details><summary> Click here learn how to do this without the `workflows` package </summary>

AVOCADO: this section feels like it should either have more explanation or just deleted. To me it feels confusing because it feels like we are showing a compeltely alternative path without an explanation why you might want to do? or when you would want to do this? 

We could have also done this without the `workflows` package. 
Notice below that we use the processed training data (`juiced_train`) as opposed to the raw training data that we used with the `workflow()` we created with `workflows` above.

In this case, we actually need to write your model again! 
Recall that `id` and `fips` are ID variables and that `values` is our outcome of interest (the air pollution measure at each monitor). 

```{r}
juiced_train_ready <- juiced_train %>% 
  select(-id, -fips)

PM_fit <- lm_PM_model %>% 
  parsnip::fit(value ~., data = juiced_train_ready)
```

</details>

### Assessing the model fit

After we fit our model, we can use the `broom` package allows for an easy/tidy way to look at the output from the fitted model.   

The three functions from `broom` that we want to use are: 

1. `tidy()` - returns a tidy data frame with coefficients from the model (one row per coefficient)
2. `glance()` - summarizes the model fit
3. `augment()` - gives a 150 row observation level summary of the data and fit 

AVOCADO: not sure I understand `augment()`. maybe more text here? 

These `broom` functions currently only work with `parsnip` objects, not raw `workflows` objects. 
Here, we will use the `PM_fit` using `parsnip`. 

```{r}
broom::tidy(PM_fit) %>% 
  arrange(p.value)
broom::glance(PM_fit[["fit"]])
broom::augment(PM_fit[["fit"]])  
```
The `augment()` function gives us the fitted values, standard error for each and more!

However, we can use the `broom` functions (e.g. `tidy()`) with `workflows`, but we first need to use the `pull_workflow_fit()` function.

```{r}
wflowoutput <- PM_wflow_fit %>% 
  pull_workflow_fit() %>% 
  broom::tidy() 
```

We can see that the output is identical using `workflows` to fit the model or using the output from `parsnip`

```{r}
identical(tidy(PM_fit), wflowoutput)
```

We have fit our model on our training data, which means we have created a model to predict values of air pollution based on the predictors that we have included. Yay!

One last thing before we leave this section. 
We often are interested in getting a sense of which variables are the most important in our model. 
We can explore the variable importance using the `vip()` function of the `vip` package. 
This function create a bar plot of  variable importance scores for each predictor variable (or feature) in a model. 
The bar plot is ordered by importance (highest to smallest). 

Let's take a look at the top 10 contributing variables:

```{r}
PM_wflow_fit %>% 
  pull_workflow_fit() %>% 
  vip(num_features = 10)
```



### Model performance

In this next section, our goal is to assess the overall model performance. 
The way we do this is to compare the similarity between the predicted estimates of the outcome variable produced by the model and the true outcome variable values. 




This is done typically as an iterative process with the training data along side modification of the model until the performance using the training data is satisfactory. At this point, the final model performance is assessed using the testing data. This then gives an estimate about how well the model will predict or classify the outcome variable of interest with new independent data. Ideally one would also perform an evaluation with independent data to provide a sense of how generalizable the model is to other data sources.

Let's take a look at how well our model fit our training data:


```{r}
##using the parsnip version
#can grab the fitted values simply like this:
parsnip_fitted_values<-fitted(PM_fit[["fit"]])

head(parsnip_fitted_values)

#can also get the values using the augment function
parsnip_fitted_values <- augment(PM_fit[["fit"]], data = juiced_train) %>% 
select(value, .fitted:.std.resid)

head(parsnip_fitted_values)

##using the workflows version
wf_fit <-PM_wflow_fit %>% 
  pull_workflow_fit()

#can grab the fitted values simply like this:
wf_fitted_values<-fitted(wf_fit[["fit"]])

head(wf_fitted_values)

#can also get the values using the augment function
wf_fitted_values <-augment(wf_fit[["fit"]], data = juiced_train) %>% 
select(value, .fitted:.std.resid)

head(wf_fitted_values)

#gives us the same fitted values
identical(parsnip_fitted_values, wf_fitted_values)

## Let's make a plot of fitted and real values

ggplot(wf_fitted_values, aes(x = .fitted, y = value)) + geom_point()
```

OK, so our fitted range appears to be smaller than the real values. We could probably do a bit better.


Let's take a look at how well our model seems to be preforming more formally:

When assessing the performance of a model, the metrics we use depend on if we are preforming a classification or prediction also known as regression analysis. In our case we are performing a regression or prediction analysis and the metrics often used are:
1) mean absolute error (mae)  
2) R squared error (rsq) 
This is also known as the coefficient of determination which is the squared correlation between truth and estimate  
3) root mean squared error (rmse)   


We can use the `yardstick` package to quickly calculate estimates for all of these values using the `metrics()` function. Alternatively if you only wanted one metric you could use the `mae()`, `rsq()`, or `rmse()` functions respectively. This is helpful to examine with our fitted training set values to see how well our model is performing and if we need to make adjustments. 

```{r}

yardstick::metrics(wf_fitted_values, truth = value, estimate = .fitted)
yardstick::mae(wf_fitted_values, truth = value, estimate = .fitted)

```


### Cross validation sample splitting

We will use the `rsample` package again in order to further implement what are called [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} techniques. This is also called **resampling** or **repartioning**.  

Note: we are not actually getting new samples from the underlying distribution so the term resampling is a bit of a misnomer.

[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} splits our training data into multiple training data sets to allow for a deeper assessment of the accuracy of the model.

Here is a visualization of the concept for cross validation/resampling/repartitioning from [Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"}:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","resampling.png"))
```

Technically creating our testing and training set out of our original training data is sometimes considered a form of [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} called the holdout method. As we just learned this can give us a better sense of the accuracy of our data in a more generalizable way. 

However, we can do a better job of optimizing our model for accuracy if we also perform another type of [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} on the newly defined training set that we just created. There are many [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} methods and most can be easily implemented using `rsamples` package. We will use a very popular method called either [k-fold or v-fold cross validation](https://machinelearningmastery.com/k-fold-cross-validation/){target="_blank"}. 

This method involves essentially preforming the hold out method iteratively with the training data. 

First the training set is divided into k or v equally sized smaller pieces. 

Then the model is trained on the model on k-1 or v-1 subsets of the data iteratively (removing a different v or k until all possible k-1 or v-1 sets have been evaluated) to get a sense of the performance of the model. This is really useful for fine tuning specific aspects of the model in a process called model tuning.


Here is a visualization of how the folds are created:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","vfold.png"))
```


Note: People typically ignore spatial dependence with cross validation of air pollution monitoring data in the air pollution field, so we will do the same.  However, it might make sense to leave out blocks of monitors rather than  random individual monitors to help account for some spatial dependence.

The [`vfold_cv()`](https://tidymodels.github.io/rsample/reference/vfold_cv.html){target="_blank"} function of the `rsample` package can be used to parse the training data into folds for k-fold/v-fold cross validation.

The `v` argument specifies the number of folds to create.
The `repeats` argument specifies if any samples should be repeated across folds - default is `FALSE`
The `strata` argument specifies a variable to stratify samples across folds (just like in `initial_split()`).

Again because these are created at random, we need to use the base `set.seed()` function in order to obtain the same results each time we knit this document. Generally speaking using 10 folds is good practice, but this depends on the variablity within your data. We are going to use 4 for the sake of expediency. 

```{r}
set.seed(1234)

vfold_pm <-rsample::vfold_cv(data = train_pm, v = 4)

vfold_pm


vfold_pm$splits$`1`
vfold_pm$splits$`2`
```

Once the folds are created they can be used to evaluate performance by fitting the model to each of the resamples that we created:


```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","cross_validation.png"))
```


We can fit the model to our cross validation folds using the `fit_resamples()` function of the `tune` package, by specifying our workflow object and the cross validation fold object we just created. See [here](https://tidymodels.github.io/tune/reference/fit_resamples.html){target="_blank"}  for more information.

```{r}
control <- control_resamples(save_pred = TRUE)
resample_fit <-tune::fit_resamples(PM_wflow, vfold_pm)

```

We can now take a look at various metrics of performance based on the fit of our cross validation "resamples". To do this we will use the `show_best()` function of the `tune` package.

```{r}
tune::show_best(resample_fit, metric = "rmse", "rsq")
```


### Tuning

Now let's try some tuning.

Let's take a closer look at how the air pollution monitor values vary with the location latitude and longitude.

```{r}

train_pm %>% 
  dplyr::select(value, lon, lat) %>% 
  tidyr::pivot_longer(cols = c(lon, lat), 
                      names_to = "predictor", values_to = "loc_value") %>% 
  ggplot(aes(x = loc_value, value)) + 
  geom_point(alpha = .2) + 
  geom_smooth(se = FALSE) + 
  #scale_y_log10() +
  facet_wrap(~ predictor, scales = "free_x")
```

We can see that there does not appear to be a single linear relationship for either of these predictors. Thus we might want to think about using  [splines](https://www.math.uh.edu/~jingqiu/math4364/spline.pdf){target="_blank"}  or this(https://towardsdatascience.com/numerical-interpolation-natural-cubic-spline-52c1157b98ac){target="_blank"}  or just this(https://tidymodels.github.io/tune/articles/getting_started.html){target="_blank"}  or this(https://www.psych.mcgill.ca/misc/fda/ex-basis-b1.html){target="_blank"}  to model the relationship in our training data more closely. For example for the latitude plot (left) if we had 2 lines and one break-point called a knot around 40, with the first line having a positive slope and the second with a negative slope this would fit the data more similarly to the blue line shown in the figure.

We can tune for the number of knots by using a step function in the `recipes` package called `step_ns()` where ns stands for natural splines. In order to tune for the number of knots or degrees of freedom, we can set the `deg_free` argument to `tune()`. This is helpful, becuase we aren't exactly sure how closely we should be following the relationship with the value and our longitude and latitude data in our training data to achieve good accuracy yet keep our model generalizable for other data. 

This is when our cross validation methods become really handy. We can test out different values for the `deg_free` argument and see how our model performance varies across our training folds to try to find the optimal value.

We will update our recipe to add these steps. It is a good idea to do this for individual predictors because you can name each with the `tune` argument so that you can keep track of it later. We can see what we intend to tune with the `parameters()` function of the `dials` package. 

See [here](this(https://tidymodels.github.io/tune/articles/getting_started.html)){target="_blank"}  for more information about implementing this in tidymodels.

```{r}

novel_rec %<>%
  step_ns(lon, deg_free = tune("lon df")) %>%
  step_ns(lat,  deg_free = tune("lat df"))
# novel_rec %<>%
#   step_ns(lat,  deg_free = tune())
# novel_rec

pm_param <-dials::parameters(novel_rec)
pm_param
```

Generally you could use the `grid_*()` functions of the `dials` package to create the different combinations of degrees of freedom to test for both variables to optimize the model. In our case we can visibly see that if we add more than say 4 or 5 degrees of freedom we will likely over-fit the data. So instead of using these functions we will create our own grid using the base `seq()` and `expand.grid()` functions.

```{r}
#an example of what you could do:
#spline_grid <-dials::grid_regular(pm_param, levels = 3)
df_vals <- seq(1, 5, by = 2)
spline_grid <- expand.grid(`lon df` = df_vals, `lat df` = df_vals)
spline_grid
```


Now we will tune this hyper-parameter (degrees of freedom) for both the `lat` and `lon` variables using our cross validation folds. To do this we will use the `tune_grid()` function of the `tune` package.

```{r}

df_tuning <-lm_PM_model %>% 
  tune::tune_grid(novel_rec, resamples =vfold_pm, 
                  grid = spline_grid)

#df_tuning <-PM_wflow %>% tune::tune_grid(resamples =vfold_pm, 
 #                                        grid = spline_grid, 
  
#                                       param_set =pm_param)

df_tuning
```



```{r}

df_tuning %>%
  collect_metrics()

show_best(df_tuning, metric = "rmse", n =1)
```






## Data Analysis

### Linear Regression Model with PCA

We can create another workflow to see how model performance compares using a different model. In this case we are going to perform something called [Principal Component Analysis or PCA](https://medium.com/@savastamirko/pca-a-linear-transformation-f8aacd4eb007){target="_blank"}. 

So what is PCA?

PCA is a widely used dimensionality reduction method (a form of unsupervised machine learning). It creates new variables that capture the most variation within the data, yet reduce the data down to just a number of principal components. It does so by transforming the data using [orthogonal](https://en.wikipedia.org/wiki/Orthogonality#:~:text=In%20mathematics%2C%20orthogonality%20is%20the,u%2C%20v)%20%3D%200.){target="_blank"} linear transformation. In other words, it creates new variables that are [linear combinations](https://www.mathbootcamps.com/linear-combinations-vectors/){target="_blank"} of the variables within the data. Importantly these new variables are orthogonal, meaning that the new variables have zero covariance. In simpler terms, we are expressing unique types of variation within the data as new variables.

Check out this [video](https://youtu.be/_UVHneBUBW0){target="_blank"} for more information.

```{r}
lm_PCA_rec <-recipe(train_pm) %>%
    update_role(everything(), new_role = "predictor")%>%
    update_role(value, new_role = "outcome")%>%
    update_role(id, new_role = "id variable") %>%
    update_role("fips", new_role = "county id") %>%
    step_dummy(state, county, city, zcta) %>%
    step_pca(all_predictors()) 
```

Let's take a look to see what the `step_pca` function does to our predictors. To do so recall that we need to use the `prep` and `juice`  functions of the `recipes` package  on our recipe.

```{r}
prepped_rec <- prep(lm_PCA_rec, verbose = TRUE, retain = TRUE )
juiced_train<- juice(prepped_rec)
glimpse(juiced_train)
```

We still want to use the `lm` package for our regression so we can use the same model object as before:
```{r}
lm_PM_model
```



```{r}
pca_wflow <-workflows::workflow() %>%
            workflows::add_recipe(lm_PCA_rec) %>%
            workflows::add_model(lm_PM_model)


pca_wflow
```





Remember that using `workflows` we don't actually need to prep our recipe, we can just fit our model directly. 

Fit the cross validation samples:

```{r}
resample_pca_fit <-tune::fit_resamples(pca_wflow, vfold_pm)
```


Look at the performance:
```{r}
collect_metrics(resample_pca_fit)
```

And we can compare this with our previous performance:

```{r}
collect_metrics(resample_fit)
```

So we can see that our performance isn't quite as good - especially if we look at the `rmse` value.  


### Random Forest


Now for one last recipe, we are going to predict using a decision tree method called [random forest](https://en.wikipedia.org/wiki/Random_forest){target="_blank"}.

A [decision tree](https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb){target="_blank"} is a tool to partition data or anything really, based on a series of sequential (often binary) decisions, where the decisions are chosen  based on their ability to optimally split the data.

Here you can see a simple example:

```{r}
knitr::include_graphics("https://miro.medium.com/max/1000/1*LMoJmXCsQlciGTEyoSN39g.jpeg")

```

##### [[source]](https://towardsdatascience.com/understanding-random-forest-58381e0602d2){target="_blank"}

In the case of [random forest](https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9){target="_blank"} , multiple decision trees are created - hence the name forest, and each tree is built using a random subset of the training data (with replacement) - hence the full name random forest. This random aspect helps to keep the algorithm from overfitting the data.

The mean of the predictions from each of the trees is used in the final output.

```{r}
knitr::include_graphics("https://miro.medium.com/max/1400/0*f_qQPFpdofWGLQqc.png")
```


In our case, the random forest algorithm that we are working with does not work well when there are categorical variables with more than 53 levels, so we will need to remove the `zcta` variable.

```{r}

RF_rec <-recipe(train_pm) %>%
    update_role(everything(), new_role = "predictor")%>%
    update_role(value, new_role = "outcome")%>%
    update_role(id, new_role = "id variable") %>%
    update_role("fips", new_role = "county id") %>%
    step_string2factor("state", "county", "city") %>%
    step_rm("county") %>%
    step_rm("id") %>%
    step_rm("fips")%>%
    step_rm("zcta") %>%
    step_corr(all_numeric())%>%
    step_nzv(all_numeric())
```

The `rand_forest()` function of the `parsnip` package has three important arguments that act as an interface for the different possible engines to perform a random forest analysis:

1) mtry
The number of predictor or explanatory variables that will be randomly sampled as options at each split when creating the tree models. The default number for regression analyses is the number of predictors divided by 3. 

2) min_n - The minimum number of data points in a node that are required for the node to be split further.

3) trees - the number of trees in the ensemble
10 and 3

```{r}

PMtree_model <- parsnip::rand_forest(mtry = 10, min_n = 3, mode = "regression")
PMtree_model

RF_PM_model <- 
  PMtree_model  %>%
  parsnip::set_engine("randomForest") %>%
  set_mode("regression")

RF_PM_model


RF_wflow <-workflows::workflow() %>%
            workflows::add_recipe(RF_rec) %>%
            workflows::add_model(RF_PM_model)
RF_wflow
```

Fitting the data with just parsnip and with the workflow:
```{r}

##just parsnip
RF_fit <- RF_PM_model %>% 
parsnip::fit(value ~., data =juiced_train_ready)

## with workflow
RF_wflow_fit <- parsnip::fit(RF_wflow, data = train_pm)
RF_wflow_fit
```


Let's take a look at the top 10 contributing variables:

```{r}
RF_wflow_fit%>% 
  pull_workflow_fit() %>% 
  vip(num_features = 10)
```

Interesting, in the previous model the CMAQ values were also important, however the variable about if the monitor was located in California or not was also very predictive. 

Now let's take a look at model performance by fitting the data using cross validation:
```{r}
resample_RF_fit <-tune::fit_resamples(RF_wflow, vfold_pm)
collect_metrics(resample_RF_fit)
```

Now let's compare the performance of this model with the others:

```{r}
# our initial linear regression model:
collect_metrics(resample_fit)
# our initial linear regression model with lat/lon degrees of freedom tuning:
show_best(df_tuning, metric = "rmse", n =1)
# our PCA model:
collect_metrics(resample_pca_fit)

```
OK, so our first model had a mean rmse value of 2.18.
The model with the lat/long degrees of freedom  tuning had a mean rmse value of 2.02, thus showing some improvement.
The PCA model had a mean rmse value of 2.76.

It looks like the random forest model had the lowest rmse value of 1.79.


If we tuned our random forest model based on the number of trees or the value for `mtry` (which is "The number of predictors that will be randomly sampled at each split when creating the tree models"), we might get a model with even better performance.


However, our cross validated mean rmse value of 1.79 is quite good because our range of true outcome values is much larger:`r range(test_pm$value)`.

### Final Model Performance Evaluation


Now that we have decided that we have reasonable performance with the training data, we could stop here and use the `yardstick` package (and `tune` if using `workflows` to fit our model) to evaluate performance with our testing data. 

So now we will use our random forest model to predict values for the monitors in the testing data.

Using `parsnip` we would need to use the baked data testing data. With the `workflows` package, we could use the raw testing data.

Importantly, ID variables are not dealt with as nicely as with the `workflows` package so we would need to remove them. We did this above when created the processed training data for this model, the `juiced_train` data as well.

```{r}

 baked_test_pm_ready <-baked_test_pm %>%select( -"id", -"fips")
 values_pred_parsnip <-predict(RF_fit, baked_test_pm_ready)
 values_pred_parsnip
 
# using the workflows version
values_pred_wfs <- 
  predict(RF_wflow_fit, test_pm) %>% 
  bind_cols(test_pm %>% select(value, fips, county, id)) 
values_pred_wfs

#model performance with test set
yardstick::metrics(values_pred_wfs, truth = value, estimate = .pred)

```
Awesome! We can see that our rmse of 1.49 is quite similar with our testing data. We achieved quite good performance, which suggests that we would could predict other locations with more sparse monitoring based on our predictors with reasonable accuracy.

We could also use the `last_fit()` function of the `tune` package to look at performance if we chose to create a workflow using the `workflows` package.

```{r}
overallfit <-tune::last_fit(RF_wflow, pm_split)
 # or
overallfit <-RF_wflow %>%
  tune::last_fit(pm_split)

overallfit$.metrics[[1]]
```

We could check out test performance using the `collect_metrics()` function of the `tune` package.


```{r}
test_performance <- overallfit  %>%tune::collect_metrics()
test_performance
```

Here you can see the predictions for the test set (the 292 rows with predictions out of the 876 original monitor values) also using the `tune` package with the `collect_predictions()` function.

#### {.scrollable }
```{r}
test_predictions <- overallfit  %>%tune::collect_predictions()
test_predictions %>%
  print(n = 1e3)
```

####


## Data Visualization

Our main question for this case study was:  

1) Can we predict annual average air pollution concentrations at the granularity of zip code regional levels using predictors such as data about population density, urbanization, road density, as well as, satellite pollution data and chemical modeling data?

We have indeed created a model that can predict fine particulate matter air pollution levels based on our predictor variables.

Now let's make a plot of our predicted values and the true values.

First, let's start by making a plot of our monitors:

We will use the following packages to create a map of the US:
1)`sf`
2)`maps`
2)`rnaturalearth`
3)`rgeos`

According to this [link on wikipedia](https://en.wikipedia.org/wiki/List_of_extreme_points_of_the_United_States#Westernmost){target="_blank"} , these are the latitude and longitude bounds of the continental US.

top = 49.3457868 # north lat
left = -124.7844079 # west long
right = -66.9513812 # east long
bottom =  24.7433195 # south lat

We will start with getting an outline of the US with the `ne_countries()` function of the `rnaturalearth` package.
```{r}
library("rgeos")

world <- rnaturalearth::ne_countries(scale = "medium", returnclass = "sf")

ggplot(data = world) +
    geom_sf() +
    geom_point(data = pm, aes(x = lon, y = lat), size = 2, 
        shape = 23, fill = "darkred") +
    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), expand = FALSE)
```

Now let's add county lines.

County graphical data is available from the `maps` package. The `sf` package which is short for simple features creates a data frame about this graphical data so that we can work with it.

```{r}

counties <- sf::st_as_sf(maps::map("county", plot = FALSE, fill = TRUE))

monitors <-ggplot(data = world) +
    geom_sf(data = counties, fill = NA, color = gray(.5))+
    geom_point(data = pm, aes(x = lon, y = lat), size = 2, 
        shape = 23, fill = "darkred") +
    coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), expand = FALSE)+
    ggtitle("Monitor Locations")+
    theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

monitors
```

Now let's add a fill at the county level for the true monitor values of air pollution.

First, we need to get the data for county to look the same for both our data and the county map data.

```{r}

head(counties)

pm <-readr::read_csv(here("docs", "pm25_data.csv"))
counties %<>% tidyr::separate(ID, into = c("state", "county"), sep = ",")

counties[["county"]] <-stringr::str_to_title(counties[["county"]])

map_data <-inner_join(counties, pm, by = "county")


truth <-ggplot(data = world) +
    geom_sf() +
    geom_sf(data = map_data, aes(fill = value)) +
    #scale_fill_viridis_c(trans = "sqrt", alpha = .4) +
        coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), expand = FALSE)+
  scale_fill_gradientn(colours=topo.colors(7),na.value = "transparent",
                           breaks=c(0,10,20),labels=c(0,10,20),
                           limits=c(0,23.5), name = "PM ug/m3")+
  ggtitle("True PM 2.5 levels")+
    theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

truth

```


Now let's do the same with our predicted values.

Let's grab both the testing and training fitted values so that we have as much data as possible. 
In this case, the output structure for the training data fit is slightly different using `randomForest`. The fitted values are called `predicted` and the `broom` functions like `tidy()` and `augment()` will not work. So we will manually grab the fitted training data values.

```{r}

#test data
values_pred_wfs

#training data
training_RF_estimates <-RF_fit[["fit"]][["predicted"]]
training_RF_estimates <-tibble(".pred" =training_RF_estimates,
value= train_pm$value, fips = train_pm$fips,  county = train_pm$county, id =train_pm$id)

all_pred <-bind_rows(values_pred_wfs, training_RF_estimates )


```


```{r}
map_data <-inner_join(counties, all_pred, by = "county")

pred <-ggplot(data = world) +
    geom_sf() +
    geom_sf(data = map_data, aes(fill = .pred)) +
    # scale_fill_viridis_c(trans = "sqrt", alpha = .4) +
        coord_sf(xlim = c(-125, -66), ylim = c(24.5, 50), expand = FALSE)+
  scale_fill_gradientn(colours=topo.colors(7),na.value = "transparent",
                           breaks=c(0,10,20),labels=c(0,10,20),
                           limits=c(0,23.5), name = "PM ug/m3")+
  ggtitle("Predicted PM 2.5 levels")+
    theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank(),
        axis.title.y=element_blank(),
        axis.text.y=element_blank(),
        axis.ticks.y=element_blank())

pred

```



```{r}
cowplot::plot_grid(monitors, truth, pred)
```


```{r}
cowplot::plot_grid(truth, pred,nrow = 2 )

```


```{r, echo = FALSE, message=FALSE}

png(here::here("img", "main_plot_maps.png"), height = 1500, width = 2000, res = 300)
cowplot::plot_grid(truth, pred,nrow = 2 )

dev.off()

```

## Summary

Let's review everything:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","ecosystem.png"))
```


We have explored gravimetric monitoring data of fine particulate matter air pollution. We have utilized the tidymodels ecosystem of packages to predict monitor values using a variety of predictors, also known as explanatory variables, including satellite data, road density data, and population density, among others. Our model could now be extended to be used to predict pollution levels in areas poor  monitoring, to help identify regions where populations maybe especially at risk for the health effects of air pollution.  

We learned that there are two major types of what is called supervised machine learning: prediction and classification. We learned that prediction is used when the outcome variable is numeric and classification is performed when the outcome variable is categorical.  

We performed the major steps of machine learning that we introduced in the beginning of the data analysis:  

1) Data exploration  

We used a packages like `skimr`, `summarytools`, `corrplot`, `ggcorrplot`, and `GGally` to better understand our data. These packages gave can tell us how many missing values each variable has (if any), the class of each variable, the distribution of values for each variable, the sparsity of each variable, and the level of correlation between variables.  

2) Data splitting 

We used the `rsample` package to first perform an initial split of our data into two pieces: a training set and a testing set. The training set was used to optimize the model, while the testing set was used only to evaluate the performance of our final model. We also used the `rsample` package to create cross validation subsets of our training data. This allowed us to better assess the performance of our tested models using our training data.  

3) Variable assignment and pre-processing   

We used the `recipes` package to assign variable roles (such as outcome, predictor, and id variable). We also used this package to create a recipe for pre-processing our training and testing data. This involved steps such as: ` step_dummy` to create dummy numeric encodings of our categorical variables, `step_corr` to remove highly correlated variables, `step_nzv` to remove near zero variance variables that would contribute little to our model and potentially add noise.  We learned that once our recipe was created and prepped using `prep()`we could extract the pre-processed training data using `juice()` or our pre-processed testing data using `bake()`. We also learned that if we used the newer workflows package that we did not need to the `prep()`, `juice()`, or `bake()` functions, but that it is still useful to know how to do so if we want to look at our data and how the recipe is influencing it more deeply.  

4) Model specification, fitting, tuning and performance evaluation using the training data  

We learned that the model needs to first be fit to the training data. We learned that in both classification and prediction, the model is fit to the training data and the explanatory variables are used to estimate numeric values (in the case of prediction) or categorical values (in the case of classification) of the outcome variable of interest. We learned that we specify the model and its specifications using the `parnsip` package and that we also use this package to fit the model using the `fit()` function. We learned that we if just use `parsnip` to fit the model, then we need to use the pre-processed training data (output from `juice()`). We learned that we can use the raw training data if we use the `workflows` package to create a workflow that pre-processes our data for us.   

We learned that if the model fits well than the estimated values will be very similar to the true outcome variable values in our training data. We learned that we can assess model performance using the `yardstick` package and the `metrics()` function. We also learned that we can use subsets of our training data (which we created with the `rsample` package) to perform cross validation to get a better estimate about the performance of our model using our training data, as we want our results to be generalizable and to perform well with other data, not just our training data. We used the `fit_resamples()` function of the tune package to fit our model on our different training data subsets and the `collect_metrics()` function (also of the `tune` package) to evaluate model performance using these subsets.  We also learned that we can potentially improve model performance by tuning aspects about the model called [hyper-parameters](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models){target="_blank"} to determine the best option for model performance. We learned that we can do this using the `tune` and `dials` packages and evaluating the performance of our model with the different hyper-parameter options and our training data subsets that we used for cross validation. After we tested several different methods to model our data, we compared them to choose the best performing model as our final model.  

5) Overall model performance evaluation  

Once we chose our final model, we evaluated the final model performance using the testing data. This gives us a better estimate about how well the model will predict or classify the outcome variable of interest with new independent data. Ideally one would also perform an evaluation with independent data to provide a sense of how generalizable the model is to other data sources. 

We first fit our model to our testing data using either just parsnip and the pre-processed testing data (using the `bake()` recipes function), or our raw testing data if we used a workflow. We used the same performance evaluation functions (`yardstick::metrics()`  and `tune::collect_metrics()`(when using cross validation)). We also learned how we can use the `last_fit()` function of the `tune` package if we used a workflow to get the test data performance using the initial data and the testing/training split information.

Analyses like the one in our case study are important for defining which groups could benefit the most from interventions, education, and policy changes when attempting to mitigate public health challenges. You can see in this [article](https://www.nejm.org/doi/full/10.1056/NEJMoa1702747){target="_blank"} that many additional considerations would be involved to adequately understand the data enough to recommend policy changes.

### Suggested Homework

Students can predict air pollution monitor values using a different algorithm and provide an explanation for how that algorithm works and why it may be a good choice for modeling this data.

### Helpful Links

1) A review of [tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/){target="_blank"}  
2) A [course on tidymodels](https://juliasilge.com/blog/tidymodels-ml-course/){target="_blank"} by Julia Silge  
3) [More examples, explanations, and info about tidymodels development](https://www.tidymodels.org/learn/){target="_blank"} from the developers  
4) A guide for [pre-processing with recipes](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/){target="_blank"}  
5) A [guide](https://briatte.github.io/ggcorr/){target="_blank"} for using GGally to create correlation plots  
6) A [guide](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/){target="_blank"} for using parsnip to try different algorithms or engines  
7) A [list of recipe functions](https://tidymodels.github.io/recipes/reference/index.html){target="_blank"}  
8) A great blog post about [cross validation](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6){target="_blank"}  
9) A discussion about [evaluating model performance](https://medium.com/@limavallantin/metrics-to-measure-machine-learning-model-performance-e8c963665476){target="_blank"} for a deeper explanation about how to evaluate model performance  
10) [RStudio cheatsheets](https://rstudio.com/resources/cheatsheets/){target="_blank"}
11) An [explanation](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d){target="_blank"} of supervised vs unsupervised machine learning and bias-variance trade-off.
12) A thorough [explanation](https://royalsocietypublishing.org/doi/10.1098/rsta.2015.0202#:~:text=Principal%20component%20analysis%20(PCA)%20is,variables%20that%20successively%20maximize%20variance.){target="_blank"} of principal component analysis.
13) If you have access, this is a great [discussion](https://www.tandfonline.com/doi/abs/10.1080/00031305.1984.10483183){target="_blank"}  about the difference between independence, orthogonality, and lack of correlation.
14) Great [video explanation](https://youtu.be/_UVHneBUBW0){target="_blank"} of PCA.  

<u>Terms and concepts covered:</u>  

[Tidyverse](https://www.tidyverse.org/){target="_blank"}  
[Imputation](https://en.wikipedia.org/wiki/Imputation_(statistics)){target="_blank"}  
[Transformation](https://en.wikipedia.org/wiki/Data_transformation_(statistics)){target="_blank"}  
[Discretization](https://en.wikipedia.org/wiki/Discretization_of_continuous_features){target="_blank"}  
[Dummy Variables](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)){target="_blank"}  
[One Hot Encoding](https://machinelearningmastery.com/why-one-hot-encode-data-in-machine-learning/){target="_blank"}  
[Data Type Conversions](https://cran.r-project.org/web/packages/hablar/vignettes/convert.html){target="_blank"}  
[Interaction](https://statisticsbyjim.com/regression/interaction-effects/){target="_blank"}  
[Normalization](https://en.wikipedia.org/wiki/Normalization_(statistics)){target="_blank"}  
[Dimensionality Reduction/Signal Extraction](https://en.wikipedia.org/wiki/Dimensionality_reduction){target="_blank"}  
[Row Operations](https://tartarus.org/gareth/maths/Linear_Algebra/row_operations.pdf){target="_blank"}  
[Near Zero Varaince](https://www.r-bloggers.com/near-zero-variance-predictors-should-we-remove-them/){target="_blank"}  
[Parameters and Hyper-parameters](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models){target="_blank"}   
[Supervised and Unspervised Learning](https://towardsdatascience.com/supervised-vs-unsupervised-learning-14f68e32ea8d){target="_blank"}  
[Principal Component Analysis](https://medium.com/@savastamirko/pca-a-linear-transformation-f8aacd4eb007){target="_blank"}  
[Orthogonal](https://en.wikipedia.org/wiki/Orthogonality#:~:text=In%20mathematics%2C%20orthogonality%20is%20the,u%2C%20v)%20%3D%200.){target="_blank"}  
[Linear Combinations](https://www.mathbootcamps.com/linear-combinations-vectors/){target="_blank"}  
[Decision Tree](https://medium.com/greyatom/decision-trees-a-simple-way-to-visualize-a-decision-dc506a403aeb){target="_blank"}  
[Random Forest](https://towardsdatascience.com/decision-tree-ensembles-bagging-and-boosting-266a8ba60fd9){target="_blank"}  


<u>Packages used in this case study: </u>

 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data
[readr](https://readr.tidyverse.org/){target="_blank"}      | to import the CSV file data
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to view/arrange/filter/select/compare specific subsets of the data 
[skimr](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data
[summarytools](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data in a different style
[magrittr](https://magrittr.tidyverse.org/articles/magrittr.html){target="_blank"}   | to use the `%<>%` pipping operator 
[corrplot](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html){target="_blank"} | to make large correlation plots
[ggcorrplot](http://www.sthda.com/english/wiki/ggcorrplot-visualization-of-a-correlation-matrix-using-ggplot2){target="_blank"}| also to make large correlation plots
[GGally](https://cran.r-project.org/web/packages/GGally/GGally.pdf){target="_blank"} | to make smaller correlation plots  
[rsample](https://tidymodels.github.io/rsample/articles/Basics.html){target="_blank"}   | to split the data into testing and training sets and to split the training set for cross-validation  
[recipes](https://tidymodels.github.io/recipes/){target="_blank"}   | to pre-process data for modeling in a tidy and reproducible way and to extract pre-processed data (major functions are `recipe()` , `prep()` and various transformation `step_*()` functions, as well as `juice()` - extracts final pre-processed training data and `bake()` - applies recipe steps to testing data). See [here](https://cran.r-project.org/web/packages/recipes/recipes.pdf){target="_blank"}  for more info.
[parsnip](https://tidymodels.github.io/parsnip/){target="_blank"}   | an interface to create models (major functions are  `fit()`, `set_engine()`)
[yardstick](https://tidymodels.github.io/yardstick/){target="_blank"}   | to evaluate the performance of models
[broom](https://www.tidyverse.org/blog/2018/07/broom-0-5-0/){target="_blank"} | to get tidy output for our model fit and performance
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}    | to make visualizations with multiple layers
[dials](https://www.tidyverse.org/blog/2019/10/dials-0-0-3/){target="_blank"} | to specify hyper-parameter tuning
[tune](https://tune.tidymodels.org/){target="_blank"} | to perform cross validation, tune hyper-parameters, and get performance metrics
[workflows](https://www.rdocumentation.org/packages/workflows/versions/0.1.1){target="_blank"} | to create modeling workflow to streamline the modeling process
[vip](https://cran.r-project.org/web/packages/vip/vip.pdf){target="_blank"} | to create variable importance plots
[randomForest](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf){target="_blank"} | to perform the random forest analysis
[stringr](https://stringr.tidyverse.org/articles/stringr.html){target="_blank"}    | to manipulate the text the map data
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to separate data within a column into multiple columns
[rnaturalearth](https://cran.r-project.org/web/packages/rnaturalearth/README.html){target="_blank"} | to get the geometry data for the earth to plot the US
[maps](https://cran.r-project.org/web/packages/maps/maps.pdf){target="_blank"} | to get map database data about counties to draw them on our US map
[sf](https://r-spatial.github.io/sf/){target="_blank"}  | to convert the map data into a data frame
[lwgeom](https://cran.r-project.org/web/packages/lwgeom/lwgeom.pdf){target="_blank"} | to use the `sf` function to convert the map geographical data
[rgeos](https://cran.r-project.org/web/packages/rgeos/rgeos.pdf){target="_blank"} | to use geometry data
[cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html){target="_blank"} | to allow plots to be combined

## Session info
***

```{r}
sessionInfo()
```


