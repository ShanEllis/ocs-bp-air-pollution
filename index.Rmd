---
title: "Open Case Studies : Predicting Annual Air Pollution "
css: style.css
output:
  html_document:
    self_contained: yes
    code_download: yes
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes

---
<style>
#TOC {
  background: url("https://opencasestudies.github.io/img/logo.jpg");
  background-size: contain;
  padding-top: 240px !important;
  background-repeat: no-repeat;
}
</style>




```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, comment = NA, echo = TRUE,
                      message = FALSE, warning = FALSE, cache = FALSE,
                      fig.align = "center", out.width = '90%')
library(here)
library(knitr)
```


#### {.outline }
```{r, echo = FALSE, out.width = "800 px"}
knitr::include_graphics(here::here("img", "mainplot.png"))
```

####

## {.disclaimer_block}

**Disclaimer**: The purpose of the [Open Case Studies](https://opencasestudies.github.io){target="_blank"} project is **to demonstrate the use of various data science methods, tools, and software in the context of messy, real-world data**. A given case study does not cover all aspects of the research process, is not claiming to be the most appropriate way to analyze a given data set, and should not be used in the context of making policy decisions without external consultation from scientific experts. 

## Motivation
A variety of different sources contribute different types of pollutants to what we call air pollution. 
Some sources are natural while others are anthropogenic (human derived):

<p align="center">
  <img width="600" src="https://www.nps.gov/subjects/air/images/Sources_Graphic_Huge.jpg?maxwidth=1200&maxheight=1200&autorotate=false">
</p>

##### [[source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.nps.gov%2Fsubjects%2Fair%2Fsources.htm&psig=AOvVaw2v7AVxSF8ZSAPEhNudVtbN&ust=1585770966217000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPDN66q_xegCFQAAAAAdAAAAABAD)]

#### Major types of air pollutants

1) **Gaseous** - Carbon Monoxide (CO), Ozone (O~3~), Nitrogen Oxides(NO, NO~2~), Sulpher Dioxide (SO~2~)
2) **Particulate** - small liquids and solids suspended in the air (includes lead- can include certain types of dust)
3) **Dust** - small solids (larger than particulates) that can be suspended in the air for some time but eventually settle
4) **Biological** - pollen, bacteria, viruses, mold spores

See [here])http://www.redlogenv.com/worker-safety/part-1-dust-and-particulate-matter) for more detail on the types of pollutants in the air.


#### Particulate pollution 

Air pollution particulates are generally described by their **size**.

There are 3 major categories:

1) **Large Coarse** Particulate Mater - has diameter of >10 micrometers (10 µm) 

2) **Coarse** Particulate Mater (called **PM~10-2.5~**) - has diameter of between 2.5 µm and 10 µm

3) **Fine** Particulate Mater (called **PM~2.5~**) - has diameter of < 2.5 µm 

**PM~10~** includes any particulate mater <10 µm (both coarse and fine particulate mater)

Here you can see how these sizes compare with a human hair:

```{r, echo = FALSE, out.width= "600 px"}
knitr::include_graphics(here::here("img", "pm2.5_scale_graphic-color_2.jpg"))
```

##### [[source](https://www.epa.gov/pm-pollution/particulate-matter-pm-basics)]

<!-- <p align="center"> -->
<!--   <img width="500" src="https://www.sensirion.com/images/sensirion-specialist-article-figure-1-cdd70.jpg"> -->
<!-- </p> -->


<u>The following plot and table show the relative sizes of these different pollutants in micrometers(µm):</u>

<p align="center">
  <img width="600" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Airborne-particulate-size-chart.svg/800px-Airborne-particulate-size-chart.svg.png">
</p>

##### [[source](https://en.wikipedia.org/wiki/Particulates)]


<p align="center">
  <img width="500" src="https://www.frontiersin.org/files/Articles/505570/fpubh-08-00014-HTML/image_m/fpubh-08-00014-t002.jpg">
</p>

##### [[source](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full)]


<u>This table shows how deeply some of the smaller fine particles can penetrate within the human body:</u>

<p align="center">
  <img width="500" src="https://www.frontiersin.org/files/Articles/505570/fpubh-08-00014-HTML/image_m/fpubh-08-00014-t001.jpg">
</p>

##### [[source](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full)]


#### Negative Impact of Particulate Exposure on Health 

Exposure to air pollution is associated with higher rates of [mortality](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5783186/){target="_blank"} in older adults and is known to be a risk factor for many diseases and conditions including but not limited to:

1) [Asthma](https://www.ncbi.nlm.nih.gov/pubmed/29243937){target="_blank"} - fine particle exposure (**PM~2.5~**) was found to be associated with higher rates of asthma in children
2) [Inflammation in type 1 diabetes](https://www.ncbi.nlm.nih.gov/pubmed/31419765){target="_blank"} - fine particle exposure (**PM~2.5~**) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with type 1 diabetes
3) [Lung function and emphysema](https://www.ncbi.nlm.nih.gov/pubmed/31408135){target="_blank"} - higher concentrations of ozone (O~3~), nitrogen oxides (NO~x~), black carbon, and fine particle exposure **PM~2.5~** , at study baseline were significantly associated with greater increases in percent emphysema per 10 years 
4) [Low birthweight](https://www.ncbi.nlm.nih.gov/pubmed/31386643){target="_blank"} - fine particle exposure(**PM~2.5~**) was associated with lower birth weight in full-term live births
5) [Viral Infection](https://www.tandfonline.com/doi/full/10.1080/08958370701665434){target="_blank"} - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (**PM~2.5~**)

See this [review article](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full){target="_blank"} for more information about sources of air pollution and the influence of air pollution on health.

#### Sparse Monitoring is Problematic for Public Health

Historically epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country. However as can be seen in the following figure, these monitors remain to be relatively sparse in certain regions of the country. Furthermore, dramatic differences in pollution rates can be seen even within the same city.

<p align="center">
  <img width="400" src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4137272/bin/1476-069X-13-63-1.jpg">
</p>

##### [[source](https://ehjournal.biomedcentral.com/articles/10.1186/1476-069X-13-63)]

This lack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations. 


#### Machine Learning Offers a Solution

An [article](https://ehjournal.biomedcentral.com/articles/10.1186/1476-069X-13-63){target="_blank"} published in the *Environmental Health* journal dealt with this issue by using data about population density, road density, among other features to model or predict air pollution levels at a more localized scale using machine learning methods. 

```{r, echo = FALSE, out.width= "800 px"}
knitr::include_graphics(here::here("img", "thepaper.png"))
```

#### {.reference_block}
Yanosky, J. D. et al. Spatio-temporal modeling of particulate air pollution in the conterminous United States using geographic and meteorological predictors. *Environ Health* 13, 63 (2014).

####

The authors of this article state that:

> "Exposure to atmospheric particulate matter (PM) remains an important public health concern,
although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial
resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved
exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or
county-specific ambient concentrations." 


```{r, echo = FALSE, out.width= "700 px", eval = FALSE}
knitr::include_graphics(here::here("img", "deaths.png"))
```

The article above explains that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems. We will use similar methods to predict annual air pollution levels spatially within the US.


### Main Questions

#### {.main_question_block}
<b><u> Our main question: </u></b>

1) Can we predict annual average air pollution concentrations at the granularity of zip code regional levels using predictors such as data about population density, urbanization, road density, as well as, satellite pollution data and chemical modeling data?

####

### Learning Objectives 

In this case study, we will walk you through importing data from CSV files and performing machine learning methods to predict our outcome variable of interest (in this case annual fine particle air pollution estimates). We will especially focus on using packages and functions from the [`Tidyverse`](https://www.tidyverse.org/){target="_blank"}, and more specifically the [`tidymodels`](https://cran.r-project.org/web/packages/tidymodels/tidymodels.pdf){target="_blank"} package/ecosystem primarily developed and maintained by [Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"} and [Davis Vaughan](https://resources.rstudio.com/authors/davis-vaughan){target="_blank"}. This package loads more modeling related packages like `rsample`, `recipes`, `parsnip`, `yardstick`,  and `dials`. We will also briefly cover the `workflows` and `tune` packages. The tidyverse is a library of packages created by RStudio. While some students may be familiar with previous R programming packages, these packages make data science in R especially efficient.


```{r, out.width = "20%", echo = FALSE, fig.align ="center"}
include_graphics("https://tidyverse.tidyverse.org/logo.png")
```

```{r, out.width = "100px", echo = FALSE, fig.align ="center"}
include_graphics("https://pbs.twimg.com/media/DkBFpSsW4AIyyIN.png")
```


We will begin by loading the packages that we will need:

```{r}
library(here)
library(readr)
library(dplyr)
library(skimr)
library(summarytools)
library(magrittr)
library(GGally)

library(tidymodels)# broom, dials, infer, parsnip, purrr, recipes, rsample, tibble, yardstick

library(workflows)
library(tune)

# library(stringr)
# library(purrr)
# library(tibble)
# library(tidyr)
# library(ggplot2)
# library(ggpubr)
# library(forcats)
# library(lmerTest)
# library(car)
# library(ggiraph)
# library(ggforce)
# library(viridis)
# library(cowplot)
```


 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data
[readr](https://readr.tidyverse.org/){target="_blank"}      | to import the CSV file data
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to view/arrange/filter/select/compare specific subsets of the data 
[skimr](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data
[summarytools](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data in a different style
[magrittr](https://magrittr.tidyverse.org/articles/magrittr.html){target="_blank"}   | to use the `%<>%` pipping operator 
[GGally](https://cran.r-project.org/web/packages/GGally/GGally.pdf){target="_blank"} | to create correlation plots  
[rsample](https://tidymodels.github.io/rsample/articles/Basics.html){target="_blank"}   | to split the data into testing and training sets and to split the training set for cross-validation  
[recipes](https://tidymodels.github.io/recipes/){target="_blank"}   | to pre-process data for modeling in a tidy and reproducible way and to extract pre-processed data (major functions are `recipe()` , `prep()` and various transformation `step_*()` functions, as well as `juice()` - extracts final preprocessed training data and `bake()` - applies recipe steps to testing data). See [here](https://cran.r-project.org/web/packages/recipes/recipes.pdf){target="_blank"}  for more info.
[parsnip](https://tidymodels.github.io/parsnip/){target="_blank"}   | an interface to create models (major functions are  `fit()`, `set_engine()`)
[yardstick](https://tidymodels.github.io/yardstick/){target="_blank"}   | to evaluate the performance of models
 


[stringr](https://stringr.tidyverse.org/articles/stringr.html){target="_blank"}    | to manipulate the text within the PDF of the data

[purrr](https://purrr.tidyverse.org/){target="_blank"}      | to perform functions on all columns of a tibble
[tibble](https://tibble.tidyverse.org/){target="_blank"}     | to create data objects that we can manipulate with dplyr/stringr/tidyr/purrr
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to separate data within a column into multiple columns
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}    | to make visualizations with multiple layers
[ggpubr](https://cran.r-project.org/web/packages/ggpubr/index.html){target="_blank"}    | to easily add regression line equations to plots
[forcats](https://forcats.tidyverse.org/){target="_blank"}    | to change details about factors (categorical variables)
[lmerTest](https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf)| to perform linear mixed model testing
[car](https://cran.r-project.org/web/packages/car/car.pdf)| to perform Levene's Test of Homogeneity of Variances
[ggiraph](https://cran.r-project.org/web/packages/ggiraph/index.html)| to make plots interactive
[ggforce](https://cran.r-project.org/web/packages/ggforce/ggforce.pdf)| to modify facets in plots
[viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)| to plot in color palette that is easily interpreted by colorblind individuals
[cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html){target="_blank"} | to allow plots to be combined
___



The first time we use a function, we will use the `::` to indicate which package we are using. Unless we have overlapping function names, this is not necessary, but we will include it here to be informative about where the functions we will use come from.


### Context

The [State of Global Air](https://www.stateofglobalair.org/){target="_blank"} is a report released every year to communicate the impact of air pollution on public health. 

The [State of Global Air 2019 report](https://www.stateofglobalair.org/sites/default/files/soga_2019_report.pdf){target="_blank"}
which uses data from 2017 stated that:

> Air pollution is the **fifth** leading risk factor for mortality worldwide. It is responsible for more
deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity.
Each year, **more** people die from air pollution–related disease than from road **traffic injuries** or **malaria**.

<p align="center">
  <img width="600" src="https://www.healtheffects.org/sites/default/files/SoGA-Figures-01.jpg">
</p>

The report also stated that:

>In 2017, air pollution is estimated to have contributed to close to 5 million
deaths globally — nearly **1 in every 10 deaths**.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","2017deaths.png"))
```
##### [[source]](https://www.stateofglobalair.org/sites/default/files/soga_2019_fact_sheet.pdf)

The [State of Global Air 2018 report](https://www.stateofglobalair.org/sites/default/files/soga-2018-report.pdf){target="_blank"}  using data from 2016 which separated different types of air pollution, found that **particulate pollution was particularly associated with mortality**.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","2017mortality.png"))
```

The 2019 report shows that the highest levels of fine particulate pollution occurs in Africa and Asia and that:

> More than **90%** of people worldwide live in areas **exceeding** the World Health Organization (WHO) **Guideline** for healthy air. More than half live in areas that do not even meet WHO’s least-stringent air quality target.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","PMworld.png"))
```

Looking at the US specifically, air pollution levels are generally improving. The US Environmental Protection Agency (EPA) also releases a report about air pollution levels called [*Our Nation's Air*](https://gispub.epa.gov/air/trendsreport/2019/#home){target="_blank"}.

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "US.png"))
```

##### [[source]](https://gispub.epa.gov/air/trendsreport/2019/documentation/AirTrends_Flyer.pdf)

However, air pollution **continues to contribute to health risk for Americans**, in particular in **regions with higher than national average rates** of pollution that actually at time exceed the world health organization's recommended level. Thus it is necessary to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.


You can see that current air quality conditions at this [website](https://aqicn.org/city/usa/){target="_blank"} and you will notice variation across different cities.

Here were the conditions in Topeka Kansas when this was written:

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "Kansas.png"))
```

It reports particulate values using what is called the [Air Quality Index](https://www.airnow.gov/index.cfm?action=aqibasics.aqi){target="_blank"} scale (AQI), this [calculator](https://airnow.gov/index.cfm?action=airnow.calculator){target="_blank"} indicates that 114 AQI is equivalent to 40.7 ug/m^3^ and is considered unhealthy for sensitive individuals. Thus some areas very much exceed the World Health Organization (WHO)  annual exposure guideline (10 ug/m^3^) at certain times and this may adversely affect the health of people living in these locations.

Furthermore, adverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines. Secondly, it appears that the composition of the particulate mater and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. See this [article](https://www.nejm.org/doi/full/10.1056/NEJMoa1702747){target="_blank"} for more details.

The monitor data that we will use in this case study comes from a system of monitors in which roughly 90% are located within cities. Thus there is an **equity issue** in terms of capturing the air pollution levels of more rural areas. Therefore, to get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be very useful to estimate air pollution levels in **areas with little to no monitoring**.

Indeed, machine learning methods are in fact used to be able to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:

<p align="center">
  <img width="600" src="https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/SAWOEGBXMVGQ7AS5PZ6UUOX6FY.png">
</p>


##### [[source]](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.washingtonpost.com%2Fbusiness%2F2019%2F10%2F23%2Fair-pollution-is-getting-worse-data-show-more-people-are-dying%2F&psig=AOvVaw3v-ZDTBPnLP2MYtKf3Undj&ust=1585784479068000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPCyn9fxxegCFQAAAAAdAAAAABAd)

This is what we aim to achieve in this case study.

### Limitations

There are some important considerations regarding this data analysis to keep in mind: 

1) The data in our analysis does not include information about the composition of particulate mater. Different types of particulates may be more benign or deleterious for health outcomes.

2) Outdoor pollution levels are not necessarily an indication of of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. People are now developing personal monitoring systems to track air pollution levels on the personal level.

Our analysis will use annual mean estimates, however pollution levels can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data, however we are interested in long term exposures, as these appear to be the most influential for health outcomes, so we chose to use annual level data. 


## What are the data?

In Machine Learning for prediction, there are two main types of variables:

1) Outcome variable
2) Predictor variables

The **outcome variable** is what are trying to **predict**. In building our model we actually have the outcome variable data, but we want to see how well our predictor variables can explain the variation in our outcome data. This gives us a sense of how well we can use the predictor variable data to predict our outcome variable levels when we in fact do not have data about the outcome.

As a simpler example, imagine that we have data about the sales and characteristics of cars from last year and we want to predict which cars might sell well this year. We do not have the sales data yet for this year, but we do know the characteristics of our cars for this year. We can use a model of the characteristics that explained sales last year to estimate what cars might sell well this year. In this case, our outcome variable is the sale performance of the cars, while the different characteristics of the cars make up our predictor or explanatory variables.

In this case study, we will evaluate air pollution monitor data of fine particulate mater (PM~2.5~) in the contiguous US from 2008, as well as data about population density, road density, urbanization levels, and NASA satellite data to develop models to predict localized air pollution levels. 

The monitor data will be our **outcome variable**.  We want to determine if we can **predict** air pollution levels based on other types of data, like road density and population density to see if we can use these data to predict air pollution in areas where there are no monitors. 


### Our outcome variable

The monitor data that we will be using comes from **[gravimetric monitors](https://publiclab.org/wiki/filter-pm){target="_blank"}** operated by the US [Enivornmental Protection Agency (EPA)](https://www.epa.gov/){target="_blank"}. These monitors use a filtration system to specifically capture fine particulate matter. The weight of this matter is manually measured daily or weekly. See [here](https://www3.epa.gov/ttnamti1/files/ambient/pm25/spec/RTIGravMassSOPFINAL.pdf){target="_blank"} for the EPA standard operating procedure for PM gravimetric analysis in 2008.



```{r, echo = FALSE, out.width="150px"}
knitr::include_graphics(here::here("img","filter.png"))
```

##### [[source](https://publiclab.org/wiki/filter-pm)]

Here is an image of what the gravimetric monitors look like:

```{r, echo = FALSE, out.width="100px"}
knitr::include_graphics(here::here("img","monitor.png"))
```


Gravimetric analysis is also used for [emission testing](https://www.mt.com/us/en/home/applications/Laboratory_weighing/emissions-testing-particulate-matter.html){target="_blank"}. The same idea applies: a fresh filter is applied and the desired amount of time passes, then the filter is removed and weighed. 

There are [other monitoring systems](https://www.sensirion.com/en/about-us/newsroom/sensirion-specialist-articles/particulate-matter-sensing-for-air-quality-measurements/){target="_blank"} that can provide hourly measurements, but we will not be using data from these monitors in our analysis. Gravimetric analysis is considered to be among the most accurate methods.

In our csv, the **value** column indicates the PM~2.5~ monitor average for 2008 in mass of fine particles/volume of air for 876 gravimetric monitors. The units are micro grams of fine particulate mater (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m^3^).  Recall the the WHO exposure  guideline is < 10 ug/m^3^ on average annually for PM~2.5~.

### Our predictor variables

There are 48 predictor variables with values for each of the 876 monitors included in our outcome variable. The data comes from the US [Enivornmental Protection Agency (EPA)](https://www.epa.gov/){target="_blank"}, the [National Aeronautics and Space Administration (NASA)](https://www.nasa.gov/){target="_blank"}, the US [Census](https://www.census.gov/about/what/census-at-a-glance.html){target="_blank"}, and the [National Center for Health Statistics (NCHS)](https://www.cdc.gov/nchs/about/index.htm){target="_blank"}.

Variable   | Details                                                                        
---------- |-------------
**id**  | Monitor number  <br> -- the county number is indicated before the decimal <br> -- the monitor number is indicated after the decimal <br>  **Example**: 1073.0023  is Jefferson county (1073) and .0023 one of 8 monitors 
**fips** | Federal information processing standard number for the county where the monitor is located <br> -- 5 digit id code for counties (zero is often the first value and sometimes is not shown) <br> -- the first 2 numbers indicate the state <br> -- the last three numbers indicate the county <br>  **Example**: Alabama's state code is 01 because it is first alphabetically <br> (note: Alaska and Hawaii are not included because they are not part of the contiguous US)  
**Lat** | Latitude of the monitor in degrees  
**Lon** | Longitude of the monitor in degrees  
**state** | State where the monitor is located
**county** | County where the monitor is located
**city** | City where the monitor is located
**CMAQ**  | Estimated values of air pollution from a computational model called [**Community Multiscale Air Quality (CMAQ)**](https://www.epa.gov/cmaq){target="_blank"} <br> --  A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution <br> -- ***Does not use any of the PM~2.5~ gravimetric monitoring data.*** (There is a version that does use the gravimetric monitoring data, but not this one!) <br> -- Data from the EPA
**zcta** | [Zip Code Tabulation Area](https://www2.census.gov/geo/pdfs/education/brochures/ZCTAs.pdf){target="_blank"} where the monitor is located <br> -- Postal Zip codes are converted into "generalized areal representations" that are non-overlapping  <br> -- Data from the 2010 Census  
**zcta_area** | Land area of the zip code area in meters squared  <br> -- Data from the 2010 Census  
**zcta_pop** | Population in the zip code area  <br> -- Data from the 2010 Census  
**imp_a500** | Impervious surface measure <br> -- Within a circle with a radius of 500 meters around the monitor <br> -- Impervious surface are roads, concrete, parking lots, buildings <br> -- This is a measure of development 
**imp_a1000** | Impervious surface measure <br> --  Within a circle with a radius of 1000 meters around the monitor
**imp_a5000** | Impervious surface measure <br> --  Within a circle with a radius of 5000 meters around the monitor  
**imp_a10000** | Impervious surface measure <br> --  Within a circle with a radius of 10000 meters around the monitor   
**imp_a15000** | Impervious surface measure <br> --  Within a circle with a radius of 15000 meters around the monitor  
**county_area** | Land area of the county of the monitor in meters squared  
**county_pop** | Population of the county of the monitor  
**Log_dist_to_prisec** | Log (Natural log) distance to a primary or secondary road from the monitor <br> -- Highway or major road  
**log_pri_length_5000** | Count of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_10000** | Count of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_15000** | Count of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_25000** | Count of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log) <br> -- Highways only  
**log_prisec_length_500** | Count of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_1000** | Count of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_5000** | Count of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_10000** | Count of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_15000** | Count of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_25000** | Count of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads      
**log_nei_2008_pm25_sum_10000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)    
**log_nei_2008_pm25_sum_15000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)     
**log_nei_2008_pm25_sum_25000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)     
**log_nei_2008_pm10_sum_10000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)      
**log_nei_2008_pm10_sum_15000**| Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)      
**log_nei_2008_pm10_sum_25000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)      
**popdens_county** | Population density (number of people per kilometer squared area of the county)
**popdens_zcta** | Population density (number of people per kilometer squared area of zcta)
**nohs** | Percentage of people in zcta area where the monitor is that **do not have a high school degree** <br> -- Data from the Census
**somehs** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was **some high school education** <br> -- Data from the Census
**hs** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing a **high school degree** <br> -- Data from the Census  
**somecollege** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing **some college education** <br> -- Data from the Census 
**associate** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing an **associate degree** <br> -- Data from the Census 
**bachelor** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **bachelor's degree** <br> -- Data from the Census 
**grad** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **graduate degree** <br> -- Data from the Census 
**pov** | Percentage of people in zcta area where the monitor is that lived in [**poverty**](https://aspe.hhs.gov/2008-hhs-poverty-guidelines) in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines <br> -- Data from the Census  
**hs_orless** |  Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **high school degree or less** (sum of nohs, somehs, and hs)  
**urc2013** | [2013 Urban-rural classification](https://www.cdc.gov/nchs/data/series/sr_02/sr02_166.pdf){target="_blank"} of the county where the monitor is located <br> -- 6 category variable - 1 is totally urban 6 is completely rural <br>  -- Data from the National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm){target="_blank"}     
**urc2006** | [2006 Urban-rural classification](https://www.cdc.gov/nchs/data/series/sr_02/sr02_154.pdf){target="_blank"} of the county where the monitor is located <br> -- 6 category variable - 1 is totally urban 6 is completely rural <br> -- Data from the [National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm){target="_blank"}     
**aod** | Aerosol Optical Depth measurement from a NASA satellite <br> -- based on the defraction of a laser <br> -- used as a proxy of particulate pollution <br> -- unit-less - higher value indicates more pollution <br> -- Data from NASA  

Many of these predictor variables have to do with the circular area around the monitor called the "buffer". These are illustrated in the following figure:

```{r, echo = FALSE, out.width = "800px",}
knitr::include_graphics(here::here("img", "regression.png"))
```

##### [[source](https://www.ncbi.nlm.nih.gov/pubmed/15292906)]



## Data Import

We have one CSV file that contains both our single **outcome variable** and all of our **predictor variables**.

Let's import our data into R now so that we can explore the data further. We will call our data object `pm` for particulate matter.

```{r}
pm <-readr::read_csv(here("docs", "pm25_data.csv"))
```

## Data Exploration and Wrangling

The first step in performing a machine learning analysis is to explore the data to better understand the variables  included in the data, as we may learn about important details about the data that we should keep in mind as we try to predict our outcome variable.

First let's just get a general sense of our data. We can do that using the `glimpse()` function of the `dplyr` package (it is also in the `tibble` package).

We will also use the `%>%` pipe which can be used to define the input for later sequential steps. This will make more sense when we have multiple sequential steps using the same data object. To use the pipe notation we need to install and load dplyr as well.

For example here we will first grab the `pm` data object, then we use the `glimpse()` function on it based on the pipe notation.


```{r}
pm %>%
  dplyr::glimpse()
```

We can see that there are 876 monitors and that we have 50 total variables - one of which is the outcome. In this case our outcome variable is called `value`. 

Notice that some of the variables that we would think of as factors (categorical) are currently of class double as indicated by the `<dbl>` just to the right of the column names/variable names in the `glimpse()` output. For example the monitor ID (id), the Federal Information Processing Standard number for the county where the monitor was located (fips), as well as the zcta

Let's convert these variables into factors. We can do this using the `mutate_at()` function of the `dplyr` package and the `as.factor()` base function. 

In this case we are also using the magrittr assignment pipe or double pipe that looks like this `%<>%` of the `magrittr` package. This allows us use the `pm` data as input but also reassign the output to the same data object name.

```{r}
pm %<>%
  dplyr::mutate_at(vars(id, fips, zcta), as.factor) 

glimpse(pm)
```

Great! Now we can see that these variables are now factors as indicated by `<fct>` after the variable name.

The `skim()` function of the `skimr` package is also really helpful for getting a general sense of your data.

```{r}
skim(pm)
```

Notice how there is a column called `n_missing` about the number of values that are missing. It looks like our data is very complete and we do not have any missing data. This is also indicated by the `complete_rate` variable, which shows the ratio of completeness, in our case all variables have a value of 1 indicating they are fully complete.

The `n_unqiue` column shows us the number of unique values for each of our columns. We can see that there are 49 states represented in the data, and we know that the data should be of the contiguous states. Let's take a look to see which states are included:

#### {.scrollable }
```{r}
# Scroll through the output!
pm %>% 
  distinct(state) %>%
  print(n = 1e3)
```
####

Looks like "District of Columbia" is being included as a state. We can see that indeed Alaska and Hawaii are not included in the data.

Here is another method of looking at the data using the `dfSummary()` function of the `summarytools`package. We need to copy and paste the output into the rmarkdown.

```{r, eval = FALSE}
dfSummary(pm, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.45,  tmp.img.dir = "tmp")
```

**Dimensions:** 876 x 50  
**Duplicates:** 0  

+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| No | Variable                     | Stats / Values                           | Freqs (% of Valid)  | Graph               | Valid  | Missing |
+====+==============================+==========================================+=====================+=====================+========+=========+
| 1  | id\                          | 1\. 1003.001\                            | 1 ( 0.1%)\          | ![](tmp/ds0101.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1027.0001\                           | 1 ( 0.1%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1033.1002\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1049.1003\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 5\. 1055.001\                            | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1069.0003\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1073.0023\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 8\. 1073.1005\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 1073.1009\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 10\. 1073.101\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 866 others ]                           | 866 (98.9%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 2  | value\                       | Mean (sd) : 10.8 (2.6)\                  | 875 distinct values | ![](tmp/ds0102.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 3 < 11.2 < 23.2\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 3.1 (0.2)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 3  | fips\                        | 1\. 1003\                                | 1 ( 0.1%)\          | ![](tmp/ds0103.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1027\                                | 1 ( 0.1%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1033\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1049\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 5\. 1055\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1069\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1073\                                | 8 ( 0.9%)\          |                     |        |         |
|    |                              | 8\. 1089\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 1097\                                | 2 ( 0.2%)\          |                     |        |         |
|    |                              | 10\. 1101\                               | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 559 others ]                           | 858 (98.0%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 4  | lat\                         | Mean (sd) : 38.5 (4.6)\                  | 876 distinct values | ![](tmp/ds0104.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 25.5 < 39.3 < 48.4\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 6.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 5  | lon\                         | Mean (sd) : -91.7 (15)\                  | 876 distinct values | ![](tmp/ds0105.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | -124.2 < -87.5 < -68\                    |                     |                     |        |         |
|    |                              | IQR (CV) : 18.5 (-0.2)                   |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 6  | state\                       | 1\. California\                          | 85 ( 9.7%)\         | ![](tmp/ds0106.png) | 876\   | 0\      |
|    | [character]                  | 2\. Ohio\                                | 44 ( 5.0%)\         |                     | (100%) | (0%)    |
|    |                              | 3\. Illinois\                            | 38 ( 4.3%)\         |                     |        |         |
|    |                              | 4\. Indiana\                             | 36 ( 4.1%)\         |                     |        |         |
|    |                              | 5\. North Carolina\                      | 35 ( 4.0%)\         |                     |        |         |
|    |                              | 6\. Pennsylvania\                        | 32 ( 3.7%)\         |                     |        |         |
|    |                              | 7\. Michigan\                            | 30 ( 3.4%)\         |                     |        |         |
|    |                              | 8\. Florida\                             | 29 ( 3.3%)\         |                     |        |         |
|    |                              | 9\. Georgia\                             | 28 ( 3.2%)\         |                     |        |         |
|    |                              | 10\. Texas\                              | 27 ( 3.1%)\         |                     |        |         |
|    |                              | [ 39 others ]                            | 492 (56.2%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 7  | county\                      | 1\. Jefferson\                           | 18 ( 2.1%)\         | ![](tmp/ds0107.png) | 876\   | 0\      |
|    | [character]                  | 2\. Cook\                                | 12 ( 1.4%)\         |                     | (100%) | (0%)    |
|    |                              | 3\. Hamilton\                            | 11 ( 1.3%)\         |                     |        |         |
|    |                              | 4\. Lake\                                | 11 ( 1.3%)\         |                     |        |         |
|    |                              | 5\. Los Angeles\                         | 10 ( 1.1%)\         |                     |        |         |
|    |                              | 6\. Wayne\                               | 10 ( 1.1%)\         |                     |        |         |
|    |                              | 7\. Washington\                          | 9 ( 1.0%)\          |                     |        |         |
|    |                              | 8\. Cuyahoga\                            | 7 ( 0.8%)\          |                     |        |         |
|    |                              | 9\. Jackson\                             | 7 ( 0.8%)\          |                     |        |         |
|    |                              | 10\. Madison\                            | 7 ( 0.8%)\          |                     |        |         |
|    |                              | [ 461 others ]                           | 774 (88.4%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 8  | city\                        | 1\. Not in a city\                       | 103 (11.8%)\        | ![](tmp/ds0108.png) | 876\   | 0\      |
|    | [character]                  | 2\. New York\                            | 9 ( 1.0%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. Cleveland\                           | 6 ( 0.7%)\          |                     |        |         |
|    |                              | 4\. Baltimore\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 5\. Chicago\                             | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 6\. Detroit\                             | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 7\. Milwaukee\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 8\. New Haven\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 9\. Philadelphia\                        | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 10\. Springfield\                        | 5 ( 0.6%)\          |                     |        |         |
|    |                              | [ 597 others ]                           | 723 (82.5%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 9  | CMAQ\                     | Mean (sd) : 8.4 (3)\                     | 601 distinct values | ![](tmp/ds0109.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 1.6 < 8.6 < 23.1\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 3.7 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 10 | zcta\                        | 1\. 1022\                                | 1 ( 0.1%)\          | ![](tmp/ds0110.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1103\                                | 2 ( 0.2%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1201\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1608\                                | 2 ( 0.2%)\          |                     |        |         |
|    |                              | 5\. 1832\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1840\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1863\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 8\. 1904\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 2113\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 10\. 2119\                               | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 832 others ]                           | 864 (98.6%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 11 | zcta_area\                   | Mean (sd) : 183173481.9 (542598878.5)\   | 842 distinct values | ![](tmp/ds0111.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 15459 < 37653560.5 < 8164820625\         |                     |                     |        |         |
|    |                              | IQR (CV) : 145836906.5 (3)               |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 12 | zcta_pop\                    | Mean (sd) : 24227.6 (17772.2)\           | 837 distinct values | ![](tmp/ds0112.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 22014 < 95397\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 25207.8 (0.7)                 |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 13 | imp_a500\                    | Mean (sd) : 24.7 (19.3)\                 | 816 distinct values | ![](tmp/ds0113.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 25.1 < 69.6\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 36.5 (0.8)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 14 | imp_a1000\                   | Mean (sd) : 24.3 (18)\                   | 860 distinct values | ![](tmp/ds0114.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 24.5 < 67.5\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 33.3 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 15 | imp_a5000\                   | Mean (sd) : 19.9 (14.7)\                 | 870 distinct values | ![](tmp/ds0115.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 19.1 < 74.6\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 23.3 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 16 | imp_a10000\                  | Mean (sd) : 15.8 (13.8)\                 | 870 distinct values | ![](tmp/ds0116.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 12.4 < 72.1\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 19.6 (0.9)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 17 | imp_a15000\                  | Mean (sd) : 13.4 (13.1)\                 | 870 distinct values | ![](tmp/ds0117.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 9.7 < 71.1\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 17.3 (1)                      |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 18 | county_area\                 | Mean (sd) : 3768701992.1 (6212829553.6)\ | 564 distinct values | ![](tmp/ds0118.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 33703512 < 1690826566.5 < 51947229509\   |                     |                     |        |         |
|    |                              | IQR (CV) : 1761655911.5 (1.6)            |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 19 | county_pop\                  | Mean (sd) : 687298.4 (1293488.7)\        | 564 distinct values | ![](tmp/ds0119.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 783 < 280730.5 < 9818605\                |                     |                     |        |         |
|    |                              | IQR (CV) : 642211 (1.9)                  |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 20 | log_dist_to_prisec\          | Mean (sd) : 6.2 (1.4)\                   | 870 distinct values | ![](tmp/ds0120.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | -1.5 < 6.4 < 10.5\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1.7 (0.2)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 21 | log_pri_length_5000\         | Mean (sd) : 9.8 (1.1)\                   | 586 distinct values | ![](tmp/ds0121.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 8.5 < 10.1 < 12\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 2.2 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 22 | log_pri_length_10000\        | Mean (sd) : 10.9 (1.1)\                  | 687 distinct values | ![](tmp/ds0122.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.2 < 11.2 < 13\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 2 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 23 | log_pri_length_15000\        | Mean (sd) : 11.5 (1.1)\                  | 726 distinct values | ![](tmp/ds0123.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.6 < 11.7 < 13.6\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1.5 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 24 | log_pri_length_25000\        | Mean (sd) : 12.2 (1.1)\                  | 787 distinct values | ![](tmp/ds0124.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 10.1 < 12.5 < 14.4\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 1.4 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 25 | log_prisec_length_500\       | Mean (sd) : 7 (1)\                       | 382 distinct values | ![](tmp/ds0125.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 6.2 < 6.2 < 9.4\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 1.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 26 | log_prisec_length_1000\      | Mean (sd) : 8.6 (0.8)\                   | 591 distinct values | ![](tmp/ds0126.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 7.6 < 8.7 < 10.5\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 1.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 27 | log_prisec_length_5000\      | Mean (sd) : 11.3 (0.8)\                  | 852 distinct values | ![](tmp/ds0127.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 8.5 < 11.4 < 12.8\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 0.9 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 28 | log_prisec_length_10000\     | Mean (sd) : 12.4 (0.7)\                  | 867 distinct values | ![](tmp/ds0128.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.2 < 12.5 < 13.8\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 29 | log_prisec_length_15000\     | Mean (sd) : 13 (0.7)\                    | 869 distinct values | ![](tmp/ds0129.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.6 < 13.1 < 14.4\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 30 | log_prisec_length_25000\     | Mean (sd) : 13.8 (0.7)\                  | 870 distinct values | ![](tmp/ds0130.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 10.1 < 13.9 < 15.2\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 31 | log_nei_2008_pm25_sum_10000\ | Mean (sd) : 4 (2.4)\                     | 828 distinct values | ![](tmp/ds0131.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 4.3 < 9.1\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 3.5 (0.6)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 32 | log_nei_2008_pm25_sum_15000\ | Mean (sd) : 4.7 (2.2)\                   | 855 distinct values | ![](tmp/ds0132.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5 < 9.4\                             |                     |                     |        |         |
|    |                              | IQR (CV) : 2.9 (0.5)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 33 | log_nei_2008_pm25_sum_25000\ | Mean (sd) : 5.7 (2.1)\                   | 860 distinct values | ![](tmp/ds0133.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.9 < 9.7\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.6 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 34 | log_nei_2008_pm10_sum_10000\ | Mean (sd) : 4.3 (2.3)\                   | 829 distinct values | ![](tmp/ds0134.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 4.6 < 9.3\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 3.4 (0.5)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 35 | log_nei_2008_pm10_sum_15000\ | Mean (sd) : 5.1 (2.2)\                   | 855 distinct values | ![](tmp/ds0135.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.4 < 9.7\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.8 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 36 | log_nei_2008_pm10_sum_25000\ | Mean (sd) : 6.1 (2)\                     | 860 distinct values | ![](tmp/ds0136.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 6.4 < 9.9\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.4 (0.3)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 37 | popdens_county\              | Mean (sd) : 551.8 (1711.5)\              | 564 distinct values | ![](tmp/ds0137.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.3 < 156.7 < 26821.9\                   |                     |                     |        |         |
|    |                              | IQR (CV) : 470 (3.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 38 | popdens_zcta\                | Mean (sd) : 1279.7 (2757.5)\             | 840 distinct values | ![](tmp/ds0138.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 610.3 < 30418.8\                     |                     |                     |        |         |
|    |                              | IQR (CV) : 1281.4 (2.2)                  |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 39 | nohs\                        | Mean (sd) : 7 (7.2)\                     | 215 distinct values | ![](tmp/ds0139.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.1 < 100\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 6.1 (1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 40 | somehs\                      | Mean (sd) : 10.2 (6.2)\                  | 230 distinct values | ![](tmp/ds0140.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 9.4 < 72.2\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 8 (0.6)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 41 | hs\                          | Mean (sd) : 30.3 (11.4)\                 | 347 distinct values | ![](tmp/ds0141.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 30.8 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 12.3 (0.4)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 42 | somecollege\                 | Mean (sd) : 21.6 (8.6)\                  | 240 distinct values | ![](tmp/ds0142.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 21.3 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 7.2 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 43 | associate\                   | Mean (sd) : 7.1 (4)\                     | 157 distinct values | ![](tmp/ds0143.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 7.1 < 71.4\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 3.9 (0.6)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 44 | bachelor\                    | Mean (sd) : 14.9 (9.7)\                  | 301 distinct values | ![](tmp/ds0144.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 12.9 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 10.4 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 45 | grad\                        | Mean (sd) : 8.9 (8.6)\                   | 245 distinct values | ![](tmp/ds0145.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 6.7 < 100\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 7.1 (1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 46 | pov\                         | Mean (sd) : 15 (11.3)\                   | 345 distinct values | ![](tmp/ds0146.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 12.1 < 65.9\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 14.7 (0.8)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 47 | hs_orless\                   | Mean (sd) : 47.5 (16.8)\                 | 464 distinct values | ![](tmp/ds0147.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 48.7 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 21.2 (0.4)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 48 | urc2013\                     | Mean (sd) : 2.9 (1.5)\                   | 1 : 203 (23.2%)\    | ![](tmp/ds0148.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        | 2 : 163 (18.6%)\    |                     | (100%) | (0%)    |
|    |                              | 1 < 3 < 6\                               | 3 : 228 (26.0%)\    |                     |        |         |
|    |                              | IQR (CV) : 2 (0.5)                       | 4 : 123 (14.0%)\    |                     |        |         |
|    |                              |                                          | 5 : 101 (11.5%)\    |                     |        |         |
|    |                              |                                          | 6 :  58 ( 6.6%)     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 49 | urc2006\                     | Mean (sd) : 3 (1.5)\                     | 1 : 195 (22.3%)\    | ![](tmp/ds0149.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        | 2 : 162 (18.5%)\    |                     | (100%) | (0%)    |
|    |                              | 1 < 3 < 6\                               | 3 : 221 (25.2%)\    |                     |        |         |
|    |                              | IQR (CV) : 2 (0.5)                       | 4 : 127 (14.5%)\    |                     |        |         |
|    |                              |                                          | 5 : 115 (13.1%)\    |                     |        |         |
|    |                              |                                          | 6 :  56 ( 6.4%)     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 50 | aod\                         | Mean (sd) : 43.7 (19.6)\                 | 581 distinct values | ![](tmp/ds0150.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 5 < 40.2 < 143\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 18 (0.4)                      |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+


We can see that for many variables there are many low values as the distribution shows two peaks, one near zero and another with a higher value. This is true for the imp variables (measures of development), the nei variables (measures of emission sources) and the road density variables. We can also see that the range of some of the variables is very large, in particular the area and population related variables.


In prediction analyses, it is also useful to evaluate if any of the variables are correlated.

Intuitively we can expect some of our variables to be correlated.

We expect the development variables (imp) to be correlated with each other, we expect the road density variables to be correlated with each other, we expect the emission variables to be correlated with each other, and we also expect these variables to be correlated with one another and likely correlated with population density. We can get a nice visualization of correlation using the `ggcorr()` function and the `ggpairs()` function of the `GGally` package. to select our variables of interest we can use the `select()` function with the `contains()` function of the `tidyr` package. 

First let's look at the imp/development variables. 
```{r, out.width = "400px"}
select(pm, contains("imp")) %>%
  ggcorr(palette = "RdBu", label = TRUE)

select(pm, contains("imp")) %>%
  ggpairs()
  
```

Indeed, we can see that imp_a1000 and imp_a500 are perfectly correlated, as well as imp_a10000, imp_a15000.


Now let's take a look at the road density data:

```{r, fig.weight=12}
select(pm, contains("pri")) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

select(pm, contains("pri")) %>%
  ggpairs()
```

We can see that while some of the road density variables are highly correlated with one another (or anti-correlated), there arr some variables that are less correlated with one another.

Finally let's look at the emission variables.

```{r}
select(pm, contains("nei")) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

select(pm, contains("nei")) %>%
  ggpairs()
```

We would also expect the population density data might correlate with some of these variables. Let's take a look.

```{r}
pm %>%
select(log_nei_2008_pm25_sum_10000, popdens_county, log_pri_length_10000, imp_a10000) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

pm %>%
select(log_nei_2008_pm25_sum_10000, popdens_county, log_pri_length_10000, imp_a10000, county_pop) %>%
  ggpairs()
```


Interesting, so these variables don't appear to be highly correlated, therefore we might need variables from each of the categories to predict our monitor PM~2.5~ pollution values.

We seem to have some pretty extreme population values though, so let's see what happens when we take the log value.

```{r}
pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, log_pri_length_10000, imp_a10000) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
  mutate(log_pop_county = log(county_pop)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, log_pri_length_10000, imp_a10000, log_pop_county) %>%
  ggpairs()
```

Indeed this increased the correlation, but variables from each of these categories may still prove to be useful for prediction.


## Data Analysis

Now that we have a sense of what our data is like we can get started with data analysis.


### The tidymodels ecosystem

To perform our analysis we will be using the `tidymodels` suite of packages. You may be familiar with the older packages `caret` or `mlr` which are also for machine learning and modeling but are not a part of the `tidyverse`. [Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"} describes `tidymodels` like this:

> "Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: preprocessing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret.
The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do."


### The major benefits of tidymodels
1) standardized workflow/format/notation across different types of algorithms
2) can easily modify preprocessing, algorithm choice, and hyper-parameter tuning making optimization easy


### The machine learning process


### The tidymodels ecosystem - an overview

There are many packages in the tidymodels ecosystem which assist with the various steps of the machine learning process:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","simpletidymodels.png"))
```


This is the overall process:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","MachineLearning.png"))
```

There are two distinct goals of supervised machine learning:  

1) Prediction  
2) Classification  

We will be performing a prediction analysis (which is also refered to as regression), which aims to predict **continuous outcome** variables given a number of predictors/explanatory variables/features/parameters, as we have already described.

Classification on the other hand aims to discern or predict group identity for a **categorical outcome** based on a number of predictors/explanatory variables/features/parameters.

The overall process is the same in either case. 

### Splitting the Data

The first step after data exploration in machine learning analysis is to [split the data](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7){target="_blank"} into **training** and **testing** datasets. 

The training dataset will be used to build and tune our model. This is the data that the model "learns" on.

The testing set will be used to evaluate the performance of our model in a more generalizable way. What do we mean by "generalizable"?

Remember that our main goal is to use our model to be able to predict air pollution levels in areas where there are no gravimetric monitors. Therefore, if our model is super good at predicting air pollution with the data that we use to build it, it might not do the best job for the areas where there are few to no monitors. This would cause us to have really good prediction accuracy and we might assume that we were going to do a good job estimating air pollution any time we use our model, but in fact this would likely not be the case. This situation is what we call **[overfitting](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6){target="_blank"} **.

Overfitting happens when we end up modeling not only the major relationships in our data but also the noise within our data. 


```{r}
knitr::include_graphics("https://miro.medium.com/max/1110/1*tBErXYVvTw2jSUYK7thU2A.png")
```

##### [[source](https://miro.medium.com/max/1110/1*tBErXYVvTw2jSUYK7thU2A.png)]

If we get fairly good prediction with our testing set then we will know that our model can be applied to other data and will perform fairly well. We will discuss this more later.

We will not touch the testing set until we have completed optimizing our model with the training set. This will allow us to have a less biased evaluation of how well our model can do with other data besides the data used in the training set to build the model. Ideally you would also want a completely independent dataset to further test the performance of your model.

[Here](https://machinelearningmastery.com/difference-test-validation-datasets/){target="_blank"} is a great description of the differences between testing and training datasets.

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","split.png"))
```
We will use the `rsample` package to perform this step.

The`initial_split()` function allows us to specify how we want to split our data. Typically data is split into 3/4 for training and 1/4 for testing.This is the default proportion and does not need to be specified. However you can change the proportion using the `prop` argument, which we will do that here for illustrative purposes. You can also specify a variable to stratify by with the `strata` argument. This is useful if you have imbalanced categorical variables and you would like to intentionally make sure that there are similar number of samples of the rarer categories in both the testing and training sets. Otherwise the split is performed randomly. 

> The strata argument causes the random sampling to be conducted within the stratification variable. The can help ensure that the number of data points in the training data is equivalent to the proportions in the original data set.

In the case with our dataset, perhaps we would like our training set to have similar proportions of monitors from each of the states as in the initial data. This might be useful if we want our model to be generalizable across all of the states.

We can see that indeed there are different proportions of monitors in each state by using the `count()` function of the `dpyr` package. 

#### {.scrollable }
```{r}
# Scroll through the output!
count(pm, state) %>%
  print(n = 1e3)
```
####

If our dataset were large enough it might be nice then to stratify by state, but our data is unfortunately not large enough. We will show how one would do this though for illustrative purposes. This option is often more important for classification applications of machine learning than it is for prediction.

Since the split is performed randomly, it is a good idea to use the `set.seed()` base function to ensure that if your rerun your code that your split will be the same next time. We can see the number of monitors in our training, testing, and original data by typing in the name of our split object. The result will look like this:
<training data sample number, testing data sample number, original sample number> 

```{r}
set.seed(1234)
pm_split <-rsample::initial_split(data = pm, prop = 2/3)
pm_split

# If stratifying:
# pm_split_strata <-rsample::initial_split(data = pm, prop = 2/3, strata = "state")

```

Importantly the `initial_split` function only determines what rows of our pm data frame should be assigned for training or testing, it does not actually split the data. 

To extract the testing and training data we can use the `training()` and `testing()` functions also of the `rsample` package.

#### {.scrollable }
```{r}
 train_pm <-rsample::training(pm_split)
 test_pm <-rsample::testing(pm_split)
 
# Scroll through the output!
count(train_pm, state)
count(test_pm, state)
```
####



### Variable Role Assignment and Preprocessing

In tidymodels we will create a recipe, which is a standardized format for a sequence of steps for processing the data.

This can be very useful because it makes testing out different pre-processing steps or different algorithms with the same pre-processing very easy and reproducible.

**Creating a recipe specifies how a dataframe of predictors should be created  - it specifies what  variables to be used  and the  preprocessing steps  but it does not execute these steps or create the dataframe of predictors.**

#### List the ingredients / specify the variables with the `recipe()` function

The first thing to do to create a recipe is to specify which variables we will be using as our outcome and predictors using the `recipe()` function. In terms of the metaphor of baking, we can think of this as listing our ingredients. The naming convention for recipe object names is `*_rec` or `rec`. 

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","recipes1.png"))
```


In our case recall that our `value` variable, which is the average annual gravimetric monitor PM~2.5~ concentration in ug/m^3^. Our predictors are all the other variables except the monitor ID, which is an `id` variable.

The reason not to include this variable is because this variable includes the county number and a number designating which particular monitor the values came from of the monitors there are in that county. Since this number is arbitrary and the county information is also given in the data, and the fact that each monitor only has one value in the `value` variable, nothing is gained by including this variable and it may instead introduce noise. However, it is useful to keep this data to take a look at what is happening later. We will show you what to do in this case in just a bit.

The simplest recipe with no preprocessing steps, would be to simply list the outcome and predictor variables.

We can do so in two ways:  

1) Using formula notation  
2) Assigning roles to each variable  

Let's look at the first way using formula notation, which looks like this:  

outcome(s) ~ predictor(s)  

If in the case of multiple predictors or a multivariate situation with two outcomes, use a plus sign  

outcome1 + outcome2 ~ predictor1 + predictor2  

If we want to include all predictors we can use a period like so:  

outcome_variable_name ~ .  

Now with our data we will start by making a recipe for our training data. In the simplest case we might use all predictors like this:

```{r}

simple_rec <-train_pm %>%
  recipes::recipe(value ~ .)

simple_rec
```


However, to deal with the id variable we could use the `update_role()` function of the `recipes` package:

```{r}

simple_rec <-train_pm %>%
  recipes::recipe(value ~ .) %>%
  recipes::update_role(id, new_role = "id variable")

simple_rec
```

We could also specify the outcome and predictors in the same way as the id variable. Please see [here](https://tidymodels.github.io/recipes/reference/recipe.html) for examples of other roles for variables. The role can be actually be any value. 

The order is important here, as we first make all variables predictors and then override this role for the outcome and id variable. We will use the `everything()` function of the `dplyr` package to start with all of the variables in `train_pm`.

```{r}

simple_rec <-recipe(train_pm) %>%
    update_role(everything(), new_role = "predictor")%>%
    update_role(value, new_role = "outcome")%>%
    update_role(id, new_role = "id variable")

simple_rec

```

If we want to take a look at our formula from our recipe we can do use the `formula()` function of the `stats` package.

```{r}
formula(simple_rec)
```

We can also view our recipe in more detail using the base `summary()` function.

```{r}
summary(simple_rec)
```

#### List the preprocessing steps using the step functions of the `recipe` package

The other thing the recipes package allows for is specifying pre-processing steps using a variety of `step*()` functions.

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","recipes2.png"))
```


**This [link](https://tidymodels.github.io/recipes/reference/index.html){target="_blank"} and this [link](https://cran.r-project.org/web/packages/recipes/recipes.pdf){target="_blank"} show the many options for recipe step functions.**

<u>There are step functions for a variety of purposes:</u>

1) [**Imputation**](https://en.wikipedia.org/wiki/Imputation_(statistics)){target="_blank"}  -- which means filling in missing values based on the existing data 
2) [**Transformation**](https://en.wikipedia.org/wiki/Data_transformation_(statistics)){target="_blank"}  -- which means changing all values of a variable in the same way, typically to make it more normal or easier to interpret)  
3) [**Discretization**](https://en.wikipedia.org/wiki/Discretization_of_continuous_features) -- which means converting continuous values into discrete or nominal values - binning for example to reduce the number of possible levels)  (However this is generally not advisable!)
4) [**Encoding / Creating Dummy Variables**](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) -- which means creating a numeric code for categorical variables
[**More on Dummy Variables and one hot encoding**](https://medium.com/p/b5840be3c41a/responses/show)
5) [**Data type conversions**](https://cran.r-project.org/web/packages/hablar/vignettes/convert.html) -- which means changing from integer to factor or numeric to date etc.
6) [**Interaction**](https://statisticsbyjim.com/regression/interaction-effects/) term addition to the model -- which means that we would be modeling for predictors that would influence the capacity of each other to predict the outcome
7) [**Normalization**](https://en.wikipedia.org/wiki/Normalization_(statistics)) -- which means centering and scaling the data to a similar range of values
8) [**Dimensionality Reduction/ Signal Extraction**](https://en.wikipedia.org/wiki/Dimensionality_reduction) -- which means mathematically obtaining a new smaller set of variables that capture the variation or signal in the original variables (ex. Principal Component Analysis and Independent Component Analysis)
9) **Filtering** -- Filtering options for removing variables (ex. remove variables that are highly correlated to others or remove variables with very little variance and therefore likely little predictive capacity)
10) [**Row operations**](https://tartarus.org/gareth/maths/Linear_Algebra/row_operations.pdf) -- which means performing functions on the values within the rows  (ex. rearranging, filtering, imputing)
11) **Checking functions** -- Sanity checks to look for missing values, to look at the variable classes etc.

All of the step functions look like `step_*` except for the check functions which look like `check_*`.

There are several ways to select what variables to apply steps to:  
1) tidyselect methods: `contains()`, `matches()`, `starts_with()`, `ends_with()`, `everything()`, `num_range()`  
2) based on the type: `all_nominal()`, `all_numeric()` , `has_type()` 
3) based on the role: `all_predictors()`, `all_outcomes()`, `has_role()`
4) name - use the actual name of the variable/variables of interest  


Let's try adding some steps to our recipe.

We might consider log transforming our population and area variables (that aren't densities) - let's take a look at the range of these variables.
```{r}
pm %>%
  select(matches("_pop|_area")) %>%
  map(range)
```
We can see that the range for each of these variables is quite large, we can log transform this data using the `step_log()` function of the `recipes` package.

We would also want to potentially one hot encode some of our categorical variables so that they can be used with certain algorithms. We can do this with the `step_dummy()` function and the `one_hot = TRUE` argument. Our fips variable includes a numeric code for state and county - and therefore is essentially a proxy for county.  Since we already have county, we will just use it and keep the fips id as another ID variable.

We can remove the `fips` variable from the predictors using `update_role()` to make sure that the role is no longer `"predictor"`. We can make the role anything we want actually, so we will keep it something identifiable.

We might also want to remove variables that appear to be redundant and are highly correlated with others, as we know from our exploratory data analysis that many of our variables are correlated with one another. We can do this using the `step_corr()` function.

**It is important to add the steps to the recipe in an order that makes sense just like with a cooking recipe.**

```{r}
simple_rec %<>%
  #step_log(matches("_pop|_area")) %>%
  update_role("fips", new_role = "county id") %>%
  step_dummy(state, county, city, zcta) %>%
  step_corr(all_numeric(), - CMAQ, - aod)%>%
  step_nzv(all_numeric()) 
  #step_dummy(state, county, city, zcta)
#  step_dummy(state, county, city, zcta, one_hot = TRUE)
 # check_new_values(all_predictors()) %>%
  #check_range(all_predictors())

#can use step_rm() to remove predictors
  
simple_rec
```


### OPTIONAL: Running the preprocessing (to see it)

The next major function of the `recipes` package is `prep()`.

This function updates the recipe object based on the training data. It estimates parameters (estimating the required quantities and statistics required by the steps for the variables) for preprocessing and  updates the model terms, as some of the predictors may be removed, this allows the recipe to be ready to use on other datasets. It doesn't necessarily actually execute the preprocessing itself, however we will specify in argument for it to do this so that we can take a look at the preprocessed data.

There are some important arguments to know about:
1) training - you must supply a training data set to estimate parameters for preprocessing operations (recipe steps) - this may already be included in your recipe - as is the case for us
2) fresh - if TRUE - will retrain and estimate parameters for any previous steps that were already prepped if you add more steps to the recipe
3) verbose - if `TRUE` shows the progress as the steps are evaluated and the size of the preprocessed training set
4) retain - if `TRUE` then the preprocessed training set will be saved within the recipe (as template). This is good if you are likely to add more steps and don't want to rerun the `prep()` on the previous steps. However this can make the recipe size large. This is necessary if you want to actually look at the preprocessed data.


```{r}
prepped_rec <- prep(simple_rec, verbose = TRUE, retain = TRUE )
names(prepped_rec)
```

There are also lots of useful things to checkout in the output of `prep()`.
You can see:
1) the `steps` that were run  
2) the variable info (`var_info`)  
3) the model `term_info`
4) the new `levels` of the variables 
5) the original levels of the variables `orig_lvls`   
6) info about the training data set size and completeness (`tr_info`)

Note:  You may see the `prep.recipe()` function in material that you read about the `recipes` package. This is referring to the `prep()` function of the `recipes` package.

#### Extracting the preprocessed training data

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","recipes3.png"))
```

Since we retained our preprocessed training data, we can take a look at it like by using the `juice()` function of the `recipes` package like this:

#### {.scrollable }
```{r}
# Scroll therough the output!
prepped_train<- juice(prepped_rec)
glimpse(prepped_train)
```
####


For easy comparison sake - here is our original data:
#### {.scrollable }
```{r}
# Scroll therough the output!
glimpse(pm)
```
####

Notice how we only have 36 variables now instead of 50! Two of these are our ID variables (`fips` and the actual monitor ID (`id`)) and one is our outcome (`value`). Thus we only have 33 predictors now. We can also see that variables that we no longer have any categorical variables. Variables like `state` are gone and only `state_California` remains as it was the only state identity to have nonzero variance.  We can see that California had the largest number of monitors compared to the other states. We can also see that there were more monitors listed as `"Not in a city"` than any city. 

#### {.scrollable }
```{r}
pm %>% count(state)
pm %>% count(city)
```
####

**Note**:  Recall that you must specify `retain = TRUE` argument of the `prep()` function to use `juice()`.

#### Extracting the preprocessed testing data

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","recipes4.png"))
```

Later on if you want to look at the preprocessed testing data you would use the `bake()` function of the `recipes` package like this:

However you would want to leave your testing data alone at this point.

#### {.scrollable }
```{r}
# Scroll therough the output!
baked_rec <- recipes::bake(prepped_rec, new_data = test_pm)
glimpse(baked_rec)
```
####

> bake() takes a trained recipe and applies the operations to a data set to create a design matrix.
 for example:  it applies the centering to new data sets using these means used to create the recipe


Note: if you use the skip option for some of the preprocessing steps, be careful. `juice()` will show all of the results ignoring `skip = TRUE`. `bake()` will not necessarily conduct these steps on the new data. 



### Specifying the Model

So far we have used `rsample` to split the data and `recipes` to assign variable and to specify and prep our preprocessing (as well as to optionally extract the preprocessed data).

We will now use the `parsnip` package (which is similar to the previous `caret` package - and hence why it is named after the vegetable) to specify our model.

There are four aspects to define about our model:  
1) the **type** of model (using specific funtions in parsnip like `rand_forest()`, `logistic_reg()` etc.)  
2) the **mode** of learning - classification or regression (using the `set_mode()` function)  
3) the package or **engine** that we will use to implement the type of model selected (using the `set_engine()` function)  
4) any **arguments** necessary for the model/package selected (using the `set_args()`function -  for example the `mtry =` argument for random forest which is the number of variables to be used as options for splitting at each tree node)  

We are going to start our analysis with a linear regression but we will demonstrate how we can try different models.

The first thing we do is define what type of model we would like to use. See [here](https://tidymodels.github.io/parsnip/articles/articles/Models.html
){target="_blank"} for modeling [options]in parsnip.

```{r}
PM_model <- parsnip::linear_reg() #PM for particulate mater
PM_model
```

OK. So far all we have told `parsnip` is we want to use a linear regression...  Let's tell `parsnip` more about what we want.

We would like to use the ordinary least squares method to fit our linear regression. So we will tell `parsnip` that we want to use the `lm` package to implement our linear regression (there are many options actually- such as [`rstan`](https://cran.r-project.org/web/packages/rstan/vignettes/rstan.html) [`glmnet`](https://cran.r-project.org/web/packages/glmnet/index.html), [`keras`](https://keras.rstudio.com/), and [`sparklyr`](https://therinspark.com/starting.html#starting-sparklyr-hello-world)). We will do so by using the `set_engine()` function of the `parsnip` package.

```{r}
lm_PM_model <- 
  PM_model  %>%
  parsnip::set_engine("lm")

lm_PM_model

```

In some cases some packages can do either classification or prediction, so it is a good idea to specify which mode you intend to perform. You can do this with the `set_mode()` function of the `parsnip` package, by using either `set_mode("classification")` or `set_mode("regression")`.

```{r}
lm_PM_model <- 
  PM_model  %>%
  parsnip::set_engine("lm") %>%
  set_mode("regression")

lm_PM_model

```

### Fitting the Model with `workflows` and `parsnip`

To fit our model we could use `parsnip` and then assess our fit using the `yardstick` package.

However a newer package called `workflows` allows us to keep track of both our preprocessing steps and our model specification. It also allows us to implement fancier optimizations in an automated way and it is currently being developed to also handle post-processing operations, so it is good to learn about it!

So we will now create a workflow with the recipe (our preprocessing specifications) that we made and the model that we just specified.

First we use the `workflow()` function of the `workflows` package to create a workflow.

Then we add our recipe with the `add_recipe()` function and we add our model with the `add_model()` function of the `workflows` package. 

Note: We do not need to actually prep our recipe before using workflows!

```{r}
PM_wflow <-workflows::workflow() %>%
           workflows::add_recipe(simple_rec) %>%
           workflows::add_model(lm_PM_model)
PM_wflow
```

Ah, nice. Notice how it tells us about both our preprocessing steps and our model specifications.

Now we can prepare the recipe (estimate the parameters) and fit the model to our training data all at once. Printing the output we can see the coefficients of the model.

```{r}
PM_wflow_fit <- parsnip::fit(PM_wflow, data = train_pm)
PM_wflow_fit
```

Otherwise we could have done this without the `workflows` package:
```{r}
PM_fit <- lm_PM_model %>% 
  parsnip::fit(value ~., data =prepped_train)

```

### Looking at model fit with `broom`

The `broom` package allows for an easy/tidy way to look at the fitted model:  

`tidy()` grabs the coefficients from the model  
`glance()` summarizes the model fit  
`augment()` gives a 150 row observation level summary  

These `broom` functions currently only work with `parsnip` objects not raw `workflows` objects. To use these functions with `workflows` we need to first use the `pull_workflow_fit()` function.

```{r}
broom::tidy(PM_fit)
broom::glance(PM_fit$fit)
broom::augment(PM_fit$fit)


PM_wflow_fit %>% 
  pull_workflow_fit() %>% 
  tidy()
```


OK, so we have fit our model on our training data, which means we have created a model to predict values of air pollution based on the predictors that we have included. Yay!


At this point we could take a look at the accuracy of our model performance, but we will only have one reference point: our testing dataset. And we haven't done any tuning of our model or cross validation... so generally speaking you <u>**should not**</u> do this. 

However if you were short on time you could continue like this.

need tune_*(), fit_resamples() or last_fit() to get any assessment of fit.


We could stop here and use the `yardstick` package to evaluate performance. First we would want to fit the model with hmmm not sure if I will do this or just skip and use tune even if we dont have any tuning??

Using parsnip we need to use the baked data testing data... or not???
```{r}
# lm_fit <-lm_PM_model %>%
#   parsnip::fit(value ~.,data = juiced_rec)
#yardstick::metrics(lm_fit, truth, predicted)

# values_pred <- 
#   predict(PM_wflow_fit, baked_rec) %>% 
#   bind_cols(baked_rec %>% select(value, fips)) 
# values_pred


values_pred <- 
  predict(PM_wflow_fit, test_pm) %>% 
  bind_cols(test_pm %>% select(value, fips)) 
values_pred
#With Tune package:
#overallfit <-tune::last_fit(PM_wflow, pm_split)
#count(values_pred, .pred)
```


Say we were done optimizing our parameters we could then use the `last_fit()` function of the `workflows` package.

#### {.scrollable }
```{r}
#overallfit <-tune::last_fit(PM_wflow, pm_split)
 # or
#overallfit <-PM_wflow %>%
#  tune::last_fit(pm_split)

##overallfit
```

We could check out test performance using the `collect_metrics()` function of the `tune` package.

model performance :https://medium.com/@limavallantin/metrics-to-measure-machine-learning-model-performance-e8c963665476

```{r}
#test_performance <- overallfit  %>%tune::collect_metrics()
#test_performance
```

Here you can see the predictions for the test set (the 292 rows with predicitions out of the 876 original monitor values) also using the `tune` package with the `collect_predictions()` function.

```{r}
#test_predictions <- overallfit  %>%tune::collect_predictions()
#print(test_predictions, n=1e3)
```
####


However we would really want to tune parameters and use cross validation for this.



yardstick can't talk directly to workflows... need tune for that...



### Cross validation sample splitting

We will use the `rsample` package again in order to further implement what are called [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} techniques. This is also called **resampling** or **repartioning**.
Note: we are not actually getting new samples from the underlying distribution so the term resampling is a bit of a misnomer.

[Cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} splits our training data into multiple training data sets to allow for an assessment of the accuracy of the model.

Here is a visualization of the concept for cross validation/resampling/repartitioning from [Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"}:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","resampling.png"))
```

Technically creating our testing and training set out of our original training data is sometimes considered a form of [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} called the holdout method. As we just learned this can give us a better sense of the accuracy of our data in a more generalizable way. 

However, we can do a better job of optimizing our model for accuracy if we also perform another type of [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} on the newly defined training set that we just created. There are many [cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)){target="_blank"} methods and most can be easily implemented using `rsamples` package. We will use a very popular method called either [k-fold or v-fold cross validation](https://machinelearningmastery.com/k-fold-cross-validation/){target="_blank"}. 

This method involves essentially preforming the hold out method iteratively with the training data. 

First the training set is divided into k or v equally sized smaller pieces. 

Then the model is trained on the model on k-1 or v-1 subsets of the data iteratively (removing a different v or k untill all possible k-1 or v-1 sets have been evaluated) to get a sense of the performance of the model. This is really useful for fine tuning specific aspects of the model in a process called model tuning.


Here is a visualization of how the folds are created:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","vfold.png"))
```


Note: People typically ignore spatial dependence with cross validation of air pollution monitoring data in the air pollution field, so we will do the same.  However, it might make sense to leave out blocks of monitors rather than  random individual monitors to help account for some spatial dependence.

The [`vfold_cv()`](https://tidymodels.github.io/rsample/reference/vfold_cv.html){target="_blank"} function of the `rsample` package can be used to parse the training data into folds for k-fold/v-fold cross validation.

The `v` argument specifies the number of folds to create.
The `repeats` argument specifies if any samples should be repeated across folds - defualt is `FALSE`
The `strata` argument specifies a variable to stratify samples across folds (just like in `initial_split()`).

Again because these are created at random, we need to use the base `set.seed()` function in order to obtain the same results each time we knit this document.

```{r}
set.seed(1234)

vfold_pm <-rsample::vfold_cv(data = train_pm, v = 10 )

vfold_pm


vfold_pm$splits$`1`
vfold_pm$splits$`2`
```

Once the folds are created they can be used to evaluate peformance by fitting the model to each of the resamples that we created:


```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","cross_validation.png"))
```


We can fit the model to our cross validation folds using the `fit_resamples()` function of the `tune` package, by specifying our workflow object and the cross validation fold object we just created. See [here](https://tidymodels.github.io/tune/reference/fit_resamples.html) for more information.

```{r}
control <- control_resamples(save_pred = TRUE)
resample_fit <-tune::fit_resamples(PM_wflow, vfold_pm)

```

We can now take a look at various metrics of performance based on the fit of our 10 cross validation "resamples". To do this we will use the `show_best()` function of the `tune` package.

```{r}
tune::show_best(resample_fit, metric = "rmse", "rsq")
```

### Tuning

Now let's try some tuning.

Let's take a closer look at how the monitor values vary with latitude and longitude.

```{r}

train_pm %>% 
  dplyr::select(value, lon, lat) %>% 
  tidyr::pivot_longer(cols = c(lon, lat), 
                      names_to = "predictor", values_to = "loc_value") %>% 
  ggplot(aes(x = loc_value, value)) + 
  geom_point(alpha = .2) + 
  geom_smooth(se = FALSE) + 
  #scale_y_log10() +
  facet_wrap(~ predictor, scales = "free_x")
```

We can see that there does not appear to be a single linear relationship for either of these predictors. Thus we might want to think about using  [splines](https://www.math.uh.edu/~jingqiu/math4364/spline.pdf) or this(https://towardsdatascience.com/numerical-interpolation-natural-cubic-spline-52c1157b98ac) or just this(https://tidymodels.github.io/tune/articles/getting_started.html) or this(https://www.psych.mcgill.ca/misc/fda/ex-basis-b1.html) to model the relationship in our training data more closely. For example for the latitude plot (left) if we had 2 lines and one breakpoint called a knot around 40, with the first line having a positive slope and the second with a negative slope this would fit the data more similarly to the blue line shown in the figure.

We can tune for the number of knots by using a step function in the `recipes` package called `step_ns()` where ns stands for natural splines. In order to tune for the number of knots or degrees of freedom, we can set the `deg_free` argument to `tune()`. This is helpful, becuase we aren't exactly sure how closely we should be following the relationship with the value and our longitude and latitude data in our training data to achieve good accuracy yet keep our model generalizable for other data. 

This is when our cross validation methods become really handy. We can test out different values for the `deg_free` argument and see how our model performance varies across our training folds to try to find the optimal value.

We will update our recipe to add these steps. It is a good idea to do this for individual predictors because you can name each with the `tune` argument so that you can keep track of it later. We can see what we intend to tune with the `parameters()` function of the `dials` package. 

See [here](this(https://tidymodels.github.io/tune/articles/getting_started.html)) for more information about implementing this in tidymodels.

```{r}

simple_rec %<>%
  step_ns(lon, deg_free = tune("lon df")) %>%
  step_ns(lat,  deg_free = tune("lat df"))
# simple_rec %<>%
#   step_ns(lat,  deg_free = tune())
# simple_rec

pm_param <-dials::parameters(simple_rec)
pm_param
```
Genearlly you could use the `grid_*()` functions of the `dials` package to create the different combinations of degrees of freedom to test for both variables to optimize the model. In our case we can visably see that if we add more than say 4 or 5 degrees of freedom we will likely overfit the data. So instead of using these functions we will create our own grid using the base `seq()` and `expand.grid()` functions.

```{r}
#an example of what you could do:
#spline_grid <-dials::grid_regular(pm_param, levels = 3)
df_vals <- seq(1, 5, by = 2)
spline_grid <- expand.grid(`lon df` = df_vals, `lat df` = df_vals)
spline_grid
```


Now we will tune this hyper-parameter (degrees of freedom) for both the `lat` and `lon` variables using our cross validation folds. To do this we will use the `tune_grid()` function of the `tune` package.

```{r}

df_tuning <-lm_PM_model %>% 
  tune::tune_grid(simple_rec, resamples =vfold_pm, 
                  grid = spline_grid)

#df_tuning <-PM_wflow %>% tune::tune_grid(resamples =vfold_pm, 
 #                                        grid = spline_grid, 
  
#                                       param_set =pm_param)

df_tuning
```



```{r}

df_tuning %>%
  collect_metrics()

collect_metrics(df_tuning)
show_best(df_tuning, metric = "rmse", n =1)
```


Let's review everything:

```{r, echo=FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","tidymodels_ecosystem.png"))
```




> check_new_values: This check will break the bake function if any of the checked columns does contain values it did
not contain when prep was called on the recipe. If the check passes, nothing is changed to the data

> check_range creates a specification of a recipe check that will check if the range of a numeric
variable changed in the new data.



[parameter vs hyperparameter](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models){target="_blank"}



prepper for cross validation?






great blog about cross validation etc https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6

## Data Visualization

## Summary

We have evaluated average consumption estimates of 15 dietary factors with probably non-communicable disease (NCD) risk from 195 different countries around the world. To do so we imported data from a PDF using the `pdftools` package, as well as data from two CSV files using readr. We used `tidyverse` packages such as `dplyr`, `stringr`, and `tidy` to clean and join the data from the PDF with the CSV files. 

We learned that Regression is a powerful and flexible statistical tool that simplifies or estimates the relationships between variables using a mathematical model. We learned about the utility of the regression to compare groups, look for associations between variables, and to predict outcomes based on multiple predictor or explanatory variables. We then compared this to other popular tests like the $t$-test and the ANOVA. We learned that these tests are actually equivalent to specialized types of regressions.

Our statistical analysis focused on evaluating differences in the consumption of red meat around the world between females and males and across different age groups. First we looked at the assumptions of [$t$-tests](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20$t$-test){target="_blank"} and regressions, and determined that the relative percentage data of red meat consumption to the optimal guideline suggested amount was right skewed. We learned that we could transform the data by taking the log of these values to achieve more normally distributed data. To compare males and females we used a $t$-test and learned that a $t$-test is a specialized form of a linear regression. To compare the 15 different age groups we used an ANOVA and learned that the ANOVA is also a specialized form of linear regression. We examined how we obtained the same results using either statistical test. This was also the case if we looked at the effect of gender and controlled for the paired country structure in the data by either including `location_name` in the model as another term or by using a mixed effects model to control for this structure as a random effect but not specifically test for the influence of `location_name` on red meat consumption estimates. We learned that fixed effects are those that we wish to evaluate, while random effects are those that may influence the relationships of our variables of interest but that we do not wish to actively evaluate. Using these tests and models, we determined that males consume more red meat than women on average around the world. 

Our ANOVA analysis of age determined that indeed there at least one age group consumed a significantly different amount of red meat compared to the other age groups, and this was still the case when we controlled `location_name`. However, we learned that we could that the ANOVA does not provide information about which age groups are different. We learned how the regression however could provide some comparisons of different age groups relative to the reference age group. Furthermore, our data visualizations allowed us to determine that in general red met consumption appears to be higher in the younger age groups relative to the older age groups. 

Finally, we also looked at differences in red meat consumption between the different countries and saw in our ANOVA analysis and our regression analysis that there were significant differences. We were able to use a regression that included `sex`, `age_group_name`, and `location_name` to evaluate the influence of each of the three demographic factors on consumption while controlling or accounting for the other two. Our results demonstrated that all three influenced or were associated with red meat consumption.

In preforming our statistical analyses we learned about the assumptions of the $t$-test, the regression, and the ANOVA. We also learned about important methods to tests these assumptions.

#### $t$-test assumptions:

1) Normality of the data for both groups (this is not as much of an issue if the number of observations is relatively large total n>30 - can evaluate by plotting the distribution and by creating [Q-Q plots]([Q-Q Plots](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html){target="_blank"}))
2) Equal variance between the two groups (make sure you do the correct test if the data is not normal)
3) Balanced sample sizes of the two groups
4) Independent observations (or independent paired observations)

We can evaluate if our data is [normally distributed](https://www.physiology.org/doi/full/10.1152/advan.00064.2017){target="_blank"} by plotting the distribution and by creating [Q-Q plots]([Q-Q Plots](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html){target="_blank"} to compare the distribution of our data to the theoretical [normal distribution](https://www.physiology.org/doi/full/10.1152/advan.00064.2017){target="_blank"}.

<u>If our data is not normally distributed, we can consider these options:</u>

1) We can still perform a $t$-test if our n is large
2) We can transform the data before performing a $t$-test
3) We can use a nonparametric test (Wilcoxon signed rank test, the Wilcoxon rank sum test, and the Two-sample Kolmogorov-Smirnov (KS) test)
4) We can perform a $t$-test with resampling methods (which should be especially considered when the groups are imbalanced)

See this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity){target="_blank"} for more information on $t$-test assumptions.

We learned that we can test if 2 groups have equal variance using:  
1) the F test with `var.test()`  
2) Mood's test using [`mood.test()`](https://files.eric.ed.gov/fulltext/ED065559.pdf) (use if the data is not normally distributed)  

#### Linear regression assumptions:

L (linear) - There is a linear relationship between the variables.  
I (independent) - The samples are independent from one another.  
N (normal) - The residuals are normally distributed.  
E (equal variances) - The variance of the residuals of the groups is similar.   
It is also important that predictor variables are not correlated with one another.  

#### ANOVA assumptions:

1) Normality of the data for all tested groups (less of an issue if the number of observations is relatively large total n>30) 
2) Equal variance between the groups - aka Homogeneity of variance (make sure you do the correct test if the data is not normal) 
3) Balanced sample sizes of the groups 
4) Independent observations (or independent paired observations) 

We learned about three tests for examining the equality of variance of 3 or more groups:
1) [Bartlett's test](https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm){target="_blank"} (works well if the data appears to be normally distributed) 
2) [Fligner-Killeen](http://wiki.stat.ucla.edu/socr/index.php/AP_Statistics_Curriculum_2007_NonParam_VarIndep){target="_blank"} test is nonparametric and does not assume normality 
3) [Levene's test](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm){target="_blank"}, is more robust to violations of normality than the Bartlett's test, but not as robust as the Fligner- Killeen test 


Using the `ggplot2` package we were able to visualize trends in the data and to compare consumption of these dietary factors in the US with that of the other countries.
We see that the populations in many countries are over-consuming foods that are associated with health risk when over-consumed. In particular processed meat and sugar-sweetened beverages appear to be the most over consumed. Importantly both of these appear to be consumed at higher quantities by males and younger adults. People in the US  appear to consume less sugar-sweetened beverages than other countries, however, people are still over-consuming. Processed meat however appears to be especially bad in the US. In terms of food that need to be consumed in adequate amounts to overcome health risk, nearly all countries for all factors are not reaching guideline levels. However, there are some countries consuming more than adequate amounts of legumes, vegetables, fruits and fiber. People in the US appear to eat more milk products and consume more omega-3 fatty acid and calcium rich foods than other countries. All countries including the US consume very low levels of polyunsaturated fatty acids. These [polyunsaturated fatty acids](https://en.wikipedia.org/wiki/Polyunsaturated_fat) are abundant in seeds, nuts, avocados, as well as fish. Likely the low level of consumption of nuts and seeds contributes to these low polyunsaturated fatty acid estimates. The supplementary table included in the article suggests that poor consumption of polyunsaturated fatty acids is associated with ischemic heart disease. The article takes this data further to evaluate the association of consumption levels of these foods with mortality.

Analyses like the one in our case study are important for defining which groups could benefit the most from interventions, education, and policy changes when attempting to mitigate public health challenges. You can see in the [article](https://www.thelancet.com/action/showPdf?pii=S0140-6736%2819%2930041-8){target="_blank"} however that many additional considerations would be involved to perform a more thorough analysis to adequately understand the data enough to recommend policy changes.


### Suggested Homework

Students can evaluate consumption estimates of another dietary factor besides red meat.

### Helpful Links

review of [tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/){target="_blank"} 

guide for [preprocessing with recipes](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)

[guide](https://briatte.github.io/ggcorr/) for using GGally to create correlation plots
[guide](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/) for using parsnip to try different algorithms or engines
[recipe functions](https://tidymodels.github.io/recipes/reference/index.html)

<u>Terms and concepts covered:</u>  

[Tidyverse](https://www.tidyverse.org/){target="_blank"}  
[RStudio cheatsheets](https://rstudio.com/resources/cheatsheets/){target="_blank"}  
[Inference](https://www.britannica.com/science/inference-statistics){target="_blank"}  
[Regression](https://lindeloev.github.io/tests-as-linear/){target="_blank"}  
[Different types of regression](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/){target="_blank"}  
[Ordinary least squares method](http://setosa.io/ev/ordinary-least-squares-regression/){target="_blank"}  
[Residual](https://www.statisticshowto.datasciencecentral.com/residual/){target="_blank"}  

<u>Packages used in this case study: </u>

 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data  
[readr](https://readr.tidyverse.org/){target="_blank"}      | to import the CSV file data  
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to arrange/filter/select/compare specific subsets of the data  
[skimr](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data   
[summarytools](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data in a different style   
[pdftools](https://cran.r-project.org/web/packages/pdftools/pdftools.pdf){target="_blank"}   | to read a PDF into R   
[magrittr](https://magrittr.tidyverse.org/articles/magrittr.html){target="_blank"}   | to use the `%<>%` pipping operator  
[purrr](https://purrr.tidyverse.org/){target="_blank"}      | to perform functions on all columns of a tibble   
[tibble](https://tibble.tidyverse.org/){target="_blank"}     | to create data objects that we can manipulate with  dplyr/stringr/tidyr/purrr  
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to separate data within a column into multiple columns 
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}    | to make visualizations with multiple layers  


Do I need to talk about machine learning in general more... classification vs prediction?