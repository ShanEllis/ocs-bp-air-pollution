---
title: "Open Case Studies : Predicting Annual Air Pollution "
css: style.css
output:
  html_document:
    self_contained: yes
    code_download: yes
    highlight: tango
    number_sections: no
    theme: cosmo
    toc: yes
    toc_float: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes

---
<style>
#TOC {
  background: url("https://opencasestudies.github.io/img/logo.jpg");
  background-size: contain;
  padding-top: 240px !important;
  background-repeat: no-repeat;
}
</style>




```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE, comment = NA, echo = TRUE,
                      message = FALSE, warning = FALSE, cache = FALSE,
                      fig.align = "center", out.width = '90%')
library(here)
library(knitr)
```


#### {.outline }
```{r, echo = FALSE, out.width = "800 px"}
knitr::include_graphics(here::here("img", "mainplot.png"))
```

####

## {.disclaimer_block}

**Disclaimer**: The purpose of the [Open Case Studies](https://opencasestudies.github.io){target="_blank"} project is **to demonstrate the use of various data science methods, tools, and software in the context of messy, real-world data**. A given case study does not cover all aspects of the research process, is not claiming to be the most appropriate way to analyze a given data set, and should not be used in the context of making policy decisions without external consultation from scientific experts. 

## Motivation
A variety of different sources contribute different types of pollutants to what we call air pollution. 
Some sources are natural while others are anthropogenic (human derived):

<p align="center">
  <img width="600" src="https://www.nps.gov/subjects/air/images/Sources_Graphic_Huge.jpg?maxwidth=1200&maxheight=1200&autorotate=false">
</p>

##### [[source](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.nps.gov%2Fsubjects%2Fair%2Fsources.htm&psig=AOvVaw2v7AVxSF8ZSAPEhNudVtbN&ust=1585770966217000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPDN66q_xegCFQAAAAAdAAAAABAD)]

#### Major types of air pollutants

1) **Gaseous** - Carbon Monoxide (CO), Ozone (O~3~), Nitrogen Oxides(NO, NO~2~), Sulpher Dioxide (SO~2~)
2) **Particulate** - small liquids and solids suspended in the air (includes lead- can include cetrain types of dust)
3) **Dust** - small solids (larger than particualtes) that can be suspended in the air for some time but eventually settle
4) **Biological** - pollen, bacteria, viruses, mold spores

See [here])http://www.redlogenv.com/worker-safety/part-1-dust-and-particulate-matter) for more detail on the types of pollutants in the air.


#### Particulate pollution 

Air pollution particulates are generally described by their **size**.

There are 3 major categories:

1) **Large Coarse** Particulate Mater - has diameter of >10 micrometers (10 µm) 

2) **Coarse** Particulate Mater (called **PM~10-2.5~**) - has diameter of between 2.5 µm and 10 µm

3) **Fine** Particulate Mater (called **PM~2.5~**) - has diameter of < 2.5 µm 

**PM~10~** includes any particulate mater <10 µm (both coarse and fine particulate mater)

Here you can see how these sizes compare with a human hair:

```{r, echo = FALSE, out.width= "600 px"}
knitr::include_graphics(here::here("img", "pm2.5_scale_graphic-color_2.jpg"))
```

##### [[source](https://www.epa.gov/pm-pollution/particulate-matter-pm-basics)]

<!-- <p align="center"> -->
<!--   <img width="500" src="https://www.sensirion.com/images/sensirion-specialist-article-figure-1-cdd70.jpg"> -->
<!-- </p> -->


<u>The following plot and table show the relative sizes of these different pollutants in micrometers(µm):</u>

<p align="center">
  <img width="600" src="https://upload.wikimedia.org/wikipedia/commons/thumb/d/df/Airborne-particulate-size-chart.svg/800px-Airborne-particulate-size-chart.svg.png">
</p>

##### [[source](https://en.wikipedia.org/wiki/Particulates)]


<p align="center">
  <img width="500" src="https://www.frontiersin.org/files/Articles/505570/fpubh-08-00014-HTML/image_m/fpubh-08-00014-t002.jpg">
</p>

##### [[source](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full)]


<u>This table shows how deeply some of the smaller fine particles can penetrate within the human body:</u>

<p align="center">
  <img width="500" src="https://www.frontiersin.org/files/Articles/505570/fpubh-08-00014-HTML/image_m/fpubh-08-00014-t001.jpg">
</p>

##### [[source](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full)]


#### Negative Impact of Particulate Exposure on Health 

Exposure to air pollution is associated with higher rates of [mortality](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5783186/){target="_blank"} in older adults and is known to be a risk factor for many diseases and conditions including but not limited to:

1) [Asthma](https://www.ncbi.nlm.nih.gov/pubmed/29243937){target="_blank"} - fine particle exposure (**PM~2.5~**) was found to be associated with higher rates of asthma in children
2) [Inflammation in type 1 diabetes](https://www.ncbi.nlm.nih.gov/pubmed/31419765){target="_blank"} - fine particle exposure (**PM~2.5~**) from traffic-related air pollution was associated with increased measures of inflammatory markers in youths with type 1 diabetes
3) [Lung function and emphysema](https://www.ncbi.nlm.nih.gov/pubmed/31408135){target="_blank"} - higher concentrations of ozone (O~3~), nitrogen oxides (NO~x~), black carbon, and fine particle exposure **PM~2.5~** , at study baseline were significantly associated with greater increases in percent emphysema per 10 years 
4) [Low birthweight](https://www.ncbi.nlm.nih.gov/pubmed/31386643){target="_blank"} - fine particle exposure(**PM~2.5~**) was associated with lower birth weight in full-term live births
5) [Viral Infection](https://www.tandfonline.com/doi/full/10.1080/08958370701665434){target="_blank"} - higher rates of infection and increased severity of infection are associated with higher exposures to pollution levels including fine particle exposure (**PM~2.5~**)

See this [review article](https://www.frontiersin.org/articles/10.3389/fpubh.2020.00014/full){target="_blank"} for more information about sources of air pollution and the influence of air pollution on health.

#### Sparse Monitoring is Problematic for Public Health

Historically epidemiological studies would assess the influence of air pollution on health outcomes by relying on a number of monitors located around the country. However as can be seen in the following figure, these monitors remain to be relatively sparse in certain regions of the country. Furthermore, dramatic differences in pollution rates can be seen even within the same city.

<p align="center">
  <img width="400" src="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4137272/bin/1476-069X-13-63-1.jpg">
</p>

##### [[source](https://ehjournal.biomedcentral.com/articles/10.1186/1476-069X-13-63)]

This lack of granularity in air pollution monitoring has hindered our ability to discern the full impact of air pollution on health and to identify at-risk locations. 


#### Machine Learning Offers a Solution

An [article](https://ehjournal.biomedcentral.com/articles/10.1186/1476-069X-13-63){target="_blank"} published in the *Environmental Health* journal dealt with this issue by using data about population density, road density, among other features to model or predict air pollution levels at a more localized scale using machine learning methods. 

```{r, echo = FALSE, out.width= "800 px"}
knitr::include_graphics(here::here("img", "thepaper.png"))
```

#### {.reference_block}
Yanosky, J. D. et al. Spatio-temporal modeling of particulate air pollution in the conterminous United States using geographic and meteorological predictors. *Environ Health* 13, 63 (2014).

####

The authors of this article state that:

> "Exposure to atmospheric particulate matter (PM) remains an important public health concern,
although it remains difficult to quantify accurately across large geographic areas with sufficiently high spatial
resolution. Recent epidemiologic analyses have demonstrated the importance of spatially- and temporally-resolved
exposure estimates, which show larger PM-mediated health effects as compared to nearest monitor or
county-specific ambient concentrations." 


```{r, echo = FALSE, out.width= "700 px", eval = FALSE}
knitr::include_graphics(here::here("img", "deaths.png"))
```

The article above explains that machine learning methods can be used to predict air pollution levels when traditional monitoring systems are not available in a particular area or when there is not enough spatial granularity with current monitoring systems. We will use similar methods to predict annual air pollution levels spatially within the US.


### Main Questions

#### {.main_question_block}
<b><u> Our main question: </u></b>

1) Can we predict annual average air pollution concentrations at the granularity of zip code regional levels using predictors such as data about population density, urbanization, road density, as well as, satellite pollution data and chemical modeling data?

####

### Learning Objectives 

In this case study, we will walk you through importing data from CSV files and performing machine learning methods to predict our outcome variable of interest (in this case annual fine particle air pollution estimates). We will especially focus on using packages and functions from the [`Tidyverse`](https://www.tidyverse.org/){target="_blank"}, and more specifically the [`tidymodels`](https://cran.r-project.org/web/packages/tidymodels/tidymodels.pdf){target="_blank"} package/ecosystem primarily developed and maintained by [Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"} and [Davis Vaughan](https://resources.rstudio.com/authors/davis-vaughan){target="_blank"}. This package loads more modeling related packages like `rsample`, `recipes`, `parsnip`, `yardstick`,  and `dials`. We will also briefly cover the `workflows` and `tune` packages. The tidyverse is a library of packages created by RStudio. While some students may be familiar with previous R programming packages, these packages make data science in R especially efficient.


```{r, out.width = "20%", echo = FALSE, fig.align ="center"}
include_graphics("https://tidyverse.tidyverse.org/logo.png")
```

```{r, out.width = "100px", echo = FALSE, fig.align ="center"}
include_graphics("https://pbs.twimg.com/media/DkBFpSsW4AIyyIN.png")
```


We will begin by loading the packages that we will need:

```{r}
library(here)
library(readr)
library(dplyr)
library(skimr)
library(summarytools)
library(magrittr)
library(GGally)

library(tidymodels)# broom, dials, infer, parsnip, purrr, recipes, rsample, tibble, yardstick
library(workflows)
library(tune)

# library(stringr)
# library(purrr)
# library(tibble)
# library(tidyr)
# library(ggplot2)
# library(ggpubr)
# library(forcats)
# library(lmerTest)
# library(car)
# library(ggiraph)
# library(ggforce)
# library(viridis)
# library(cowplot)
```


 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data
[readr](https://readr.tidyverse.org/){target="_blank"}      | to import the CSV file data
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to view/arrange/filter/select/compare specific subsets of the data 
[skimr](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data
[summarytools](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data in a different style
[magrittr](https://magrittr.tidyverse.org/articles/magrittr.html){target="_blank"}   | to use the `%<>%` pipping operator 
[GGally](https://cran.r-project.org/web/packages/GGally/GGally.pdf){target="_blank"} | to create correlation plots  
[rsample](https://tidymodels.github.io/rsample/articles/Basics.html){target="_blank"}   | to split the data into testing and training sets and to split the training set for cross-validation  
[recipes](https://tidymodels.github.io/recipes/){target="_blank"}   | to pre-process data for modeling in a tidy and reproducible way and to extract pre-processed data (major functions are `recipe()` , `prep()` and various transformation `step_*()` functions, as well as `juice()` - extracts final preprocessed training data and `bake()` - applies recipe steps to testing data). See [here](https://cran.r-project.org/web/packages/recipes/recipes.pdf){target="_blank"}  for more info.
[parsnip](https://tidymodels.github.io/parsnip/){target="_blank"}   | an interface to create models (major functions are  `fit()`, `set_engine()`)
[yardstick](https://tidymodels.github.io/yardstick/){target="_blank"}   | to evaluate the performance of models
 


[stringr](https://stringr.tidyverse.org/articles/stringr.html){target="_blank"}    | to manipulate the text within the PDF of the data

[purrr](https://purrr.tidyverse.org/){target="_blank"}      | to perform functions on all columns of a tibble
[tibble](https://tibble.tidyverse.org/){target="_blank"}     | to create data objects that we can manipulate with dplyr/stringr/tidyr/purrr
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to separate data within a column into multiple columns
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}    | to make visualizations with multiple layers
[ggpubr](https://cran.r-project.org/web/packages/ggpubr/index.html){target="_blank"}    | to easily add regression line equations to plots
[forcats](https://forcats.tidyverse.org/){target="_blank"}    | to change details about factors (categorical variables)
[lmerTest](https://cran.r-project.org/web/packages/lmerTest/lmerTest.pdf)| to perform linear mixed model testing
[car](https://cran.r-project.org/web/packages/car/car.pdf)| to perform Levene's Test of Homogeneity of Variances
[ggiraph](https://cran.r-project.org/web/packages/ggiraph/index.html)| to make plots interactive
[ggforce](https://cran.r-project.org/web/packages/ggforce/ggforce.pdf)| to modify facets in plots
[viridis](https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html)| to plot in color palette that is easily interpreted by colorblind individuals
[cowplot](https://cran.r-project.org/web/packages/cowplot/vignettes/introduction.html){target="_blank"} | to allow plots to be combined
___



The first time we use a function, we will use the `::` to indicate which package we are using. Unless we have overlapping function names, this is not necessary, but we will include it here to be informative about where the functions we will use come from.


### Context

The [State of Global Air](https://www.stateofglobalair.org/){target="_blank"} is a report released every year to communicate the impact of air pollution on public health. 

The [State of Global Air 2019 report](https://www.stateofglobalair.org/sites/default/files/soga_2019_report.pdf){target="_blank"}
which uses data from 2017 stated that:

> Air pollution is the **fifth** leading risk factor for mortality worldwide. It is responsible for more
deaths than many better-known risk factors such as malnutrition, alcohol use, and physical inactivity.
Each year, **more** people die from air pollution–related disease than from road **traffic injuries** or **malaria**.

<p align="center">
  <img width="600" src="https://www.healtheffects.org/sites/default/files/SoGA-Figures-01.jpg">
</p>

The report also stated that:

>In 2017, air pollution is estimated to have contributed to close to 5 million
deaths globally — nearly **1 in every 10 deaths**.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","2017deaths.png"))
```
##### [[source]](https://www.stateofglobalair.org/sites/default/files/soga_2019_fact_sheet.pdf)

The [State of Global Air 2018 report](https://www.stateofglobalair.org/sites/default/files/soga-2018-report.pdf){target="_blank"}  using data from 2016 which separated different types of air pollution, found that **particulate pollution was particularly associated with mortality**.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","2017mortality.png"))
```

The 2019 report shows that the highest levels of fine particulate pollution occurs in Africa and Asia and that:

> More than **90%** of people worldwide live in areas **exceeding** the World Health Organization (WHO) **Guideline** for healthy air. More than half live in areas that do not even meet WHO’s least-stringent air quality target.

```{r, echo = FALSE, out.width="800px"}
knitr::include_graphics(here::here("img","PMworld.png"))
```

Looking at the US specifically, air pollution levels are generally improving. The US Enviornmental Protection Agency (EPA) also releases a report about air pollution levels called [*Our Nation's Air*](https://gispub.epa.gov/air/trendsreport/2019/#home){target="_blank"}.

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "US.png"))
```

##### [[source]](https://gispub.epa.gov/air/trendsreport/2019/documentation/AirTrends_Flyer.pdf)

However, air pollution **continues to contribute to health risk for Americans**, in particular in **regions with higher than national average rates** of pollution that actually at time exceed the world health organization's recommended level. Thus it is necessary to obtain high spatial granularity in estimates of air pollution in order to identify locations where populations are experiencing harmful levels of exposure.


You can see that current air quality conditions at this [website](https://aqicn.org/city/usa/){target="_blank"} and you will notice variation across different cities.

Here were the conditions in Topeka Kansas when this was written:

```{r, echo = FALSE}
knitr::include_graphics(here::here("img", "Kansas.png"))
```

It reports particulate values using what is called the [Air Quality Index](https://www.airnow.gov/index.cfm?action=aqibasics.aqi){target="_blank"} scale (AQI), this [calculator](https://airnow.gov/index.cfm?action=airnow.calculator){target="_blank"} indicates that 114 AQI is equivalent to 40.7 ug/m^3^ and is considered unhealthy for sensitive individuals. Thus some areas very much exceed the World Health Organization (WHO)  annual exposure guideline (10 ug/m^3^) at certain times and this may adversly affect the health of people living in these locations.

Furthermore, adverse health effects have been associated with populations experiencing higher pollution exposure despite the levels being below suggested guidelines. Secondly, it appears that the composition of the particulate mater and the influence of other demographic factors may make specific populations more at risk for adverse health effects due to air pollution. See this [article](https://www.nejm.org/doi/full/10.1056/NEJMoa1702747){target="_blank"} for more details.

The monitor data that we will use in this case study comes from a system of monitors in which roughly 90% are located within cities. Thus there is an **equity issue** in terms of capturing the air pollution levels of more rural areas. Therefore, to get a better sense of the pollution exposures for the individuals living in these areas, methods like machine learning can be very useful to estimate air pollution levels in **areas with little to no monitoring**.

Indeed, machine learning methods are infact used to be able to estimate air pollution in these low monitoring areas so that we can make a map like this where we have annual estimates for all of the contiguous US:

<p align="center">
  <img width="600" src="https://arc-anglerfish-washpost-prod-washpost.s3.amazonaws.com/public/SAWOEGBXMVGQ7AS5PZ6UUOX6FY.png">
</p>


##### [[source]](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.washingtonpost.com%2Fbusiness%2F2019%2F10%2F23%2Fair-pollution-is-getting-worse-data-show-more-people-are-dying%2F&psig=AOvVaw3v-ZDTBPnLP2MYtKf3Undj&ust=1585784479068000&source=images&cd=vfe&ved=0CAIQjRxqFwoTCPCyn9fxxegCFQAAAAAdAAAAABAd)

This is what we aim to achieve in this case study.

### Limitations

There are some important considerations regarding this data analysis to keep in mind: 

1) The data in our analysis does not include information about the composition of particulate mater. Different types of particulates may be more benign or deleterious for health outcomes.

2) Outdoor pollution levels are not necessarily an indication of of individual exposures. People spend differing amounts of time indoors and outdoors and are exposed to different pollution levels indoors. People are now developing personal monitoring systems to track air pollution levels on the personal level.

Our analysis will use annual mean estimates, however pollution levels can vary greatly by season, day and even hour. There are data sources that have finer levels of temporal data, however we are interested in longterm exposures, as these appear to be the most influential for health outcomes, so we chose to use annual level data. 


## What are the data?

In Machine Learning for prediction, there are two main types of variables:

1) Outcome variable
2) Predictor variables

The **outcome variable** is what are trying to **predict**. In building our model we actually have the outcome variable data, but we want to see how well our predictor varaibles can explain the variation in our outcome data. This gives us a sense of how well we can use the predictor variable data to predict our outcome variable levels when we in fact do not have data about the outcome.

As a simpler example, imagine that we have data about the sales and characterisitcs of cars from last year and we want to predict which cars might sell well this year. We do not have the sales data yet for this year, but we do know the characteristics of our cars for this year. We can use a model of the charateristics that explained sales last year to estimate what cars might sell well this year. In this case, our outcome variable is the sale performance of the cars, while the different characteristics of the cars make up our predictor or explanitory variables.

In this case study, we will evaluate air pollution monitor data of fine particulate mater (PM~2.5~) in the contiguous US from 2008, as well as data about population density, road density, urbanization levels, and NASA satellite data to develop models to predict localized air pollution levels. 

The monitor data will be our **outcome variable**.  We want to determine if we can **predict** air pollution levels based on other types of data, like road density and population density to see if we can use these data to predict air pollution in areas where there are no monitors. 


### Our outcome variable

The monitor data that we will be using comes from **[gravimetric monitors](https://publiclab.org/wiki/filter-pm){target="_blank"}** opperated by the US [Enivornmental Protection Agency (EPA)](https://www.epa.gov/){target="_blank"}. These monitors use a filtration system to specifically capture fine particulate matter. The weight of this matter is manually measured daily or weekly. See [here](https://www3.epa.gov/ttnamti1/files/ambient/pm25/spec/RTIGravMassSOPFINAL.pdf){target="_blank"} for the EPA standard operating procedure for PM gravimetric analysis in 2008.



```{r, echo = FALSE, out.width="150px"}
knitr::include_graphics(here::here("img","filter.png"))
```

##### [[source](https://publiclab.org/wiki/filter-pm)]

Here is an image of what the gravimetric monitors look like:

```{r, echo = FALSE, out.width="100px"}
knitr::include_graphics(here::here("img","monitor.png"))
```


Gravimetric analysis is also used for [emission testing](https://www.mt.com/us/en/home/applications/Laboratory_weighing/emissions-testing-particulate-matter.html){target="_blank"}. The same idea applies: a fresh filter is applied and the desired amount of time passes, then the fitler is removed and weighed. 

There are [other monitoring systems](https://www.sensirion.com/en/about-us/newsroom/sensirion-specialist-articles/particulate-matter-sensing-for-air-quality-measurements/){target="_blank"} that can provide hourly measurements, but we will not be using data from these monitors in our analysis. Gravimetric analysis is considered to be among the most accurate methods.

In our csv, the **value** column indicates the PM~2.5~ monitor average for 2008 in mass of fine partcles/volume of air for 876 galvimetric monitors. The units are micrograms of fine particulate mater (PM) that is less than 2.5 micrometers in diameter per cubic meter of air - mass concentration (ug/m^3^).  Recall the the WHO exposure  guideline is < 10 ug/m^3^ on average annually for PM~2.5~.

### Our predictor variables

There are 48 predictor variables with values for each of the 876 monitors included in our outcome variable. The data comes from the US [Enivornmental Protection Agency (EPA)](https://www.epa.gov/){target="_blank"}, the [National Aeronautics and Space Administration (NASA)](https://www.nasa.gov/){target="_blank"}, the US [Census](https://www.census.gov/about/what/census-at-a-glance.html){target="_blank"}, and the [National Center for Health Statistics (NCHS)](https://www.cdc.gov/nchs/about/index.htm){target="_blank"}.

Variable   | Details                                                                        
---------- |-------------
**id**  | Monitor number  <br> -- the county number is indicated before the decimal <br> -- the monitor number is indicated after the decimal <br>  **Example**: 1073.0023  is Jefferson county (1073) and .0023 one of 8 monitors 
**fips** | Federal information processing standard number for the county where the monitor is located <br> -- 5 digit id code for counties (zero is often the first value and sometimes is not shown) <br> -- the first 2 numbers indicate the state <br> -- the last three numbers indicate the county <br>  **Example**: Alabama's state code is 01 because it is first alphabetically <br> (note: Alaska and Hawaii are not included because they are not part of the contigous US)  
**Lat** | Latitude of the monitor in degrees  
**Lon** | Longitude of the monitor in degrees  
**state** | State where the monitor is located
**county** | County where the monitor is located
**city** | City where the monitor is located
**wrfchem**  | Estimated values of air pollution from a computational model called [**Community Multiscale Air Quality (CMAQ)**](https://www.epa.gov/cmaq){target="_blank"} <br> --  A monitoring system that simulates the physics of the atmosphere using chemistry and weather data to predict the air pollution <br> -- ***Does not use any of the PM~2.5~ gravimetric monitoring data.*** (There is a version that does use the gravimetric monitoring data, but not this one!) <br> -- Data from the EPA
**zcta** | [Zip Code Tabulation Area](https://www2.census.gov/geo/pdfs/education/brochures/ZCTAs.pdf){target="_blank"} where the monitor is located <br> -- Postal Zip codes are converted into "generalized areal representations" that are nonoverlapping  <br> -- Data from the 2010 Census  
**zcta_area** | Land area of the zipcode area in meters squared  <br> -- Data from the 2010 Census  
**zcta_pop** | Population in the zipcode area  <br> -- Data from the 2010 Census  
**imp_a500** | Impervious surface measure <br> -- Within a circle with a radius of 500 meters around the monitor <br> -- Impervious surface are roads, concrete, parking lots, buildings <br> -- This is a measure of development 
**imp_a1000** | Impervious surface measure <br> --  Within a circle with a radius of 1000 meters around the monitor
**imp_a5000** | Impervious surface measure <br> --  Within a circle with a radius of 5000 meters around the monitor  
**imp_a10000** | Impervious surface measure <br> --  Within a circle with a radius of 10000 meters around the monitor   
**imp_a15000** | Impervious surface measure <br> --  Within a circle with a radius of 15000 meters around the monitor  
**county_area** | Land area of the county of the monitor in meters squared  
**county_pop** | Population of the county of the monitor  
**Log_dist_to_prisec** | Log (Natural log) distance to a primary or secondary road from the monitor <br> -- Highway or major road  
**log_pri_length_5000** | Count of primary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_10000** | Count of primary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_15000** | Count of primary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log) <br> -- Highways only  
**log_pri_length_25000** | Count of primary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log) <br> -- Highways only  
**log_prisec_length_500** | Count of primary and secondary road length in meters in a circle with a radius of 500 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_1000** | Count of primary and secondary road length in meters in a circle with a radius of 1000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_5000** | Count of primary and secondary road length in meters in a circle with a radius of 5000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_10000** | Count of primary and secondary road length in meters in a circle with a radius of 10000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_15000** | Count of primary and secondary road length in meters in a circle with a radius of 15000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads  
**log_prisec_length_25000** | Count of primary and secondary road length in meters in a circle with a radius of 25000 meters around the monitor (Natural log)  <br> -- Highway and secondary roads      
**log_nei_2008_pm25_sum_10000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)    
**log_nei_2008_pm25_sum_15000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)     
**log_nei_2008_pm25_sum_25000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)     
**log_nei_2008_pm10_sum_10000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 10000 meters of distance around the monitor (Natural log)      
**log_nei_2008_pm10_sum_15000**| Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 15000 meters of distance around the monitor (Natural log)      
**log_nei_2008_pm10_sum_25000** | Tons of emissions from major sources data base (annual data) sum of all sources within a circle with a radius of 25000 meters of distance around the monitor (Natural log)      
**popdens_county** | Population density (number of people per kilometer squared area of the county)
**popdens_zcta** | Population density (number of people per kilometer squared area of zcta)
**nohs** | Percentage of people in zcta area where the monitor is that **do not have a high school degree** <br> -- Data from the Census
**somehs** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was **some high school education** <br> -- Data from the Census
**hs** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing a **high school degree** <br> -- Data from the Census  
**somecollege** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing **some college education** <br> -- Data from the Census 
**associate** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was completing an **associate degree** <br> -- Data from the Census 
**bachelor** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **bachelor's degree** <br> -- Data from the Census 
**grad** | Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **graduate degree** <br> -- Data from the Census 
**pov** | Percentage of people in zcta area where the monitor is that lived in [**poverty**](https://aspe.hhs.gov/2008-hhs-poverty-guidelines) in 2008 - or would it have been 2007 guidelines??https://aspe.hhs.gov/2007-hhs-poverty-guidelines <br> -- Data from the Census  
**hs_orless** |  Percentage of people in zcta area where the monitor whose highest formal educational attainment was a **high school degree or less** (sum of nohs, somehs, and hs)  
**urc2013** | [2013 Urban-rural classification](https://www.cdc.gov/nchs/data/series/sr_02/sr02_166.pdf){target="_blank"} of the county where the monitor is located <br> -- 6 category variable - 1 is totally urban 6 is completely rural <br>  -- Data from the National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm){target="_blank"}     
**urc2006** | [2006 Urban-rural classification](https://www.cdc.gov/nchs/data/series/sr_02/sr02_154.pdf){target="_blank"} of the county where the monitor is located <br> -- 6 category variable - 1 is totally urban 6 is completely rural <br> -- Data from the [National Center for Health Statistics](https://www.cdc.gov/nchs/index.htm){target="_blank"}     
**aod** | Aerosol Optical Depth measurement from a NASA satellite <br> -- based on the defraction of a laser <br> -- used as a proxy of particulate pollution <br> -- unitless - higher value indicates more pollution <br> -- Data from NASA  

Many of these predictor variables have to do with the circular area around the monitor called the "buffer". These are illustrated in the following figure:

```{r, echo = FALSE, out.width = "800px",}
knitr::include_graphics(here::here("img", "regression.png"))
```

##### [[source](https://www.ncbi.nlm.nih.gov/pubmed/15292906)]



## Data Import

We have one CSV file that contains both our single **outcome variable** and all of our **predictor variables**.

Let's import our data into R now so that we can explore the data further. We will call our data object `pm` for particulate matter.

```{r}
pm <-readr::read_csv(here("docs", "pm25_data.csv"))
```

## Data Exploration and Wrangling

The first step in performing a machine learning analysis is to explore the data to better understand the variables  included in the data, as we may learn about important details about the data that we should keep in mind as we try to predict our outcome variable.

First let's just get a general sense of our data. We can do that using the `glimpse()` function of the `dplyr` package (it is also in the `tibble` package).

We will also use the `%>%` pipe which can be used to define the input for later sequential steps. This will make more sense when we have multiple sequential steps using the same data object. To use the pipe notation we need to install and load dplyr as well.

For example here we will first grab the `pm` data object, then we use the `glimpse()` function on it based on the pipe notation.


```{r}
pm %>%
  dplyr::glimpse()
```

We can see that there are 876 monitors and that we have 50 total variables - one of which is the outcome. In this case our outcome variable is called `value`. 

Notice that some of the variables that we would think of as factors (categorical) are currently of class double as indicated by the `<dbl>` just to the right of the column names/variable names in the `glimpse()` output. For example the monitor ID (id), the Federal Information Processing Standard number for the county where the monitor was located (fips), as well as the zcta

Let's convert these variables into factors. We can do this using the `mutate_at()` function of the `dplyr` package and the `as.factor()` base function. 

In this case we are also using the magrittr assignment pipe or double pipe that looks like this `%<>%` of the `magrittr` package. This allows us use the `pm` data as input but also reassign the output to the same data object name.

```{r}
pm %<>%
  dplyr::mutate_at(vars(id, fips, zcta), as.factor) 

glimpse(pm)
```

Great! Now we can see that these variables are now factors as indicated by `<fct>` after the variable name.

The `skim()` function of the `skimr` package is also really helpful for getting a general sense of your data.

```{r}
skim(pm)
```

Notice how there is a column called `n_missing` about the number of values that are missing. It looks like our data is very complete and we do not have any missing data. This is also indicated by the `complete_rate` variable, which shows the ratio of completeness, in our case all variabels have a value of 1 indicating they are fully complete.

The `n_unqiue` column shows us the number of unique values for each of our columns. We can see that there are 49 states represented in the data, and we know that the data should be of the continguous states. Let's take a look to see which states are included:

#### {.scrollable }
```{r}
# Scroll through the output!
pm %>% 
  distinct(state) %>%
  print(n = 1e3)
```
####

Looks like "District of Columbia" is being included as a state. We can see that indeed Alaska and Hawaii are not incldued in the data.

Here is another method of looking at the data using the `dfSummary()` function of the `summarytools`package. We need to copy and paste the output into the rmarkdown.

```{r, eval = FALSE}
dfSummary(pm, plain.ascii = FALSE, style = "grid", 
          graph.magnif = 0.45,  tmp.img.dir = "tmp")
```

**Dimensions:** 876 x 50  
**Duplicates:** 0  

+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| No | Variable                     | Stats / Values                           | Freqs (% of Valid)  | Graph               | Valid  | Missing |
+====+==============================+==========================================+=====================+=====================+========+=========+
| 1  | id\                          | 1\. 1003.001\                            | 1 ( 0.1%)\          | ![](tmp/ds0101.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1027.0001\                           | 1 ( 0.1%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1033.1002\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1049.1003\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 5\. 1055.001\                            | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1069.0003\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1073.0023\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 8\. 1073.1005\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 1073.1009\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 10\. 1073.101\                           | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 866 others ]                           | 866 (98.9%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 2  | value\                       | Mean (sd) : 10.8 (2.6)\                  | 875 distinct values | ![](tmp/ds0102.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 3 < 11.2 < 23.2\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 3.1 (0.2)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 3  | fips\                        | 1\. 1003\                                | 1 ( 0.1%)\          | ![](tmp/ds0103.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1027\                                | 1 ( 0.1%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1033\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1049\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 5\. 1055\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1069\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1073\                                | 8 ( 0.9%)\          |                     |        |         |
|    |                              | 8\. 1089\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 1097\                                | 2 ( 0.2%)\          |                     |        |         |
|    |                              | 10\. 1101\                               | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 559 others ]                           | 858 (98.0%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 4  | lat\                         | Mean (sd) : 38.5 (4.6)\                  | 876 distinct values | ![](tmp/ds0104.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 25.5 < 39.3 < 48.4\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 6.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 5  | lon\                         | Mean (sd) : -91.7 (15)\                  | 876 distinct values | ![](tmp/ds0105.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | -124.2 < -87.5 < -68\                    |                     |                     |        |         |
|    |                              | IQR (CV) : 18.5 (-0.2)                   |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 6  | state\                       | 1\. California\                          | 85 ( 9.7%)\         | ![](tmp/ds0106.png) | 876\   | 0\      |
|    | [character]                  | 2\. Ohio\                                | 44 ( 5.0%)\         |                     | (100%) | (0%)    |
|    |                              | 3\. Illinois\                            | 38 ( 4.3%)\         |                     |        |         |
|    |                              | 4\. Indiana\                             | 36 ( 4.1%)\         |                     |        |         |
|    |                              | 5\. North Carolina\                      | 35 ( 4.0%)\         |                     |        |         |
|    |                              | 6\. Pennsylvania\                        | 32 ( 3.7%)\         |                     |        |         |
|    |                              | 7\. Michigan\                            | 30 ( 3.4%)\         |                     |        |         |
|    |                              | 8\. Florida\                             | 29 ( 3.3%)\         |                     |        |         |
|    |                              | 9\. Georgia\                             | 28 ( 3.2%)\         |                     |        |         |
|    |                              | 10\. Texas\                              | 27 ( 3.1%)\         |                     |        |         |
|    |                              | [ 39 others ]                            | 492 (56.2%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 7  | county\                      | 1\. Jefferson\                           | 18 ( 2.1%)\         | ![](tmp/ds0107.png) | 876\   | 0\      |
|    | [character]                  | 2\. Cook\                                | 12 ( 1.4%)\         |                     | (100%) | (0%)    |
|    |                              | 3\. Hamilton\                            | 11 ( 1.3%)\         |                     |        |         |
|    |                              | 4\. Lake\                                | 11 ( 1.3%)\         |                     |        |         |
|    |                              | 5\. Los Angeles\                         | 10 ( 1.1%)\         |                     |        |         |
|    |                              | 6\. Wayne\                               | 10 ( 1.1%)\         |                     |        |         |
|    |                              | 7\. Washington\                          | 9 ( 1.0%)\          |                     |        |         |
|    |                              | 8\. Cuyahoga\                            | 7 ( 0.8%)\          |                     |        |         |
|    |                              | 9\. Jackson\                             | 7 ( 0.8%)\          |                     |        |         |
|    |                              | 10\. Madison\                            | 7 ( 0.8%)\          |                     |        |         |
|    |                              | [ 461 others ]                           | 774 (88.4%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 8  | city\                        | 1\. Not in a city\                       | 103 (11.8%)\        | ![](tmp/ds0108.png) | 876\   | 0\      |
|    | [character]                  | 2\. New York\                            | 9 ( 1.0%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. Cleveland\                           | 6 ( 0.7%)\          |                     |        |         |
|    |                              | 4\. Baltimore\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 5\. Chicago\                             | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 6\. Detroit\                             | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 7\. Milwaukee\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 8\. New Haven\                           | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 9\. Philadelphia\                        | 5 ( 0.6%)\          |                     |        |         |
|    |                              | 10\. Springfield\                        | 5 ( 0.6%)\          |                     |        |         |
|    |                              | [ 597 others ]                           | 723 (82.5%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 9  | wrfchem\                     | Mean (sd) : 8.4 (3)\                     | 601 distinct values | ![](tmp/ds0109.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 1.6 < 8.6 < 23.1\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 3.7 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 10 | zcta\                        | 1\. 1022\                                | 1 ( 0.1%)\          | ![](tmp/ds0110.png) | 876\   | 0\      |
|    | [factor]                     | 2\. 1103\                                | 2 ( 0.2%)\          |                     | (100%) | (0%)    |
|    |                              | 3\. 1201\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 4\. 1608\                                | 2 ( 0.2%)\          |                     |        |         |
|    |                              | 5\. 1832\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 6\. 1840\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 7\. 1863\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 8\. 1904\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 9\. 2113\                                | 1 ( 0.1%)\          |                     |        |         |
|    |                              | 10\. 2119\                               | 1 ( 0.1%)\          |                     |        |         |
|    |                              | [ 832 others ]                           | 864 (98.6%)         |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 11 | zcta_area\                   | Mean (sd) : 183173481.9 (542598878.5)\   | 842 distinct values | ![](tmp/ds0111.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 15459 < 37653560.5 < 8164820625\         |                     |                     |        |         |
|    |                              | IQR (CV) : 145836906.5 (3)               |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 12 | zcta_pop\                    | Mean (sd) : 24227.6 (17772.2)\           | 837 distinct values | ![](tmp/ds0112.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 22014 < 95397\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 25207.8 (0.7)                 |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 13 | imp_a500\                    | Mean (sd) : 24.7 (19.3)\                 | 816 distinct values | ![](tmp/ds0113.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 25.1 < 69.6\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 36.5 (0.8)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 14 | imp_a1000\                   | Mean (sd) : 24.3 (18)\                   | 860 distinct values | ![](tmp/ds0114.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 24.5 < 67.5\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 33.3 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 15 | imp_a5000\                   | Mean (sd) : 19.9 (14.7)\                 | 870 distinct values | ![](tmp/ds0115.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 19.1 < 74.6\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 23.3 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 16 | imp_a10000\                  | Mean (sd) : 15.8 (13.8)\                 | 870 distinct values | ![](tmp/ds0116.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 12.4 < 72.1\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 19.6 (0.9)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 17 | imp_a15000\                  | Mean (sd) : 13.4 (13.1)\                 | 870 distinct values | ![](tmp/ds0117.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.1 < 9.7 < 71.1\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 17.3 (1)                      |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 18 | county_area\                 | Mean (sd) : 3768701992.1 (6212829553.6)\ | 564 distinct values | ![](tmp/ds0118.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 33703512 < 1690826566.5 < 51947229509\   |                     |                     |        |         |
|    |                              | IQR (CV) : 1761655911.5 (1.6)            |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 19 | county_pop\                  | Mean (sd) : 687298.4 (1293488.7)\        | 564 distinct values | ![](tmp/ds0119.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 783 < 280730.5 < 9818605\                |                     |                     |        |         |
|    |                              | IQR (CV) : 642211 (1.9)                  |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 20 | log_dist_to_prisec\          | Mean (sd) : 6.2 (1.4)\                   | 870 distinct values | ![](tmp/ds0120.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | -1.5 < 6.4 < 10.5\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1.7 (0.2)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 21 | log_pri_length_5000\         | Mean (sd) : 9.8 (1.1)\                   | 586 distinct values | ![](tmp/ds0121.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 8.5 < 10.1 < 12\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 2.2 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 22 | log_pri_length_10000\        | Mean (sd) : 10.9 (1.1)\                  | 687 distinct values | ![](tmp/ds0122.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.2 < 11.2 < 13\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 2 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 23 | log_pri_length_15000\        | Mean (sd) : 11.5 (1.1)\                  | 726 distinct values | ![](tmp/ds0123.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.6 < 11.7 < 13.6\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1.5 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 24 | log_pri_length_25000\        | Mean (sd) : 12.2 (1.1)\                  | 787 distinct values | ![](tmp/ds0124.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 10.1 < 12.5 < 14.4\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 1.4 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 25 | log_prisec_length_500\       | Mean (sd) : 7 (1)\                       | 382 distinct values | ![](tmp/ds0125.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 6.2 < 6.2 < 9.4\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 1.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 26 | log_prisec_length_1000\      | Mean (sd) : 8.6 (0.8)\                   | 591 distinct values | ![](tmp/ds0126.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 7.6 < 8.7 < 10.5\                        |                     |                     |        |         |
|    |                              | IQR (CV) : 1.6 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 27 | log_prisec_length_5000\      | Mean (sd) : 11.3 (0.8)\                  | 852 distinct values | ![](tmp/ds0127.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 8.5 < 11.4 < 12.8\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 0.9 (0.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 28 | log_prisec_length_10000\     | Mean (sd) : 12.4 (0.7)\                  | 867 distinct values | ![](tmp/ds0128.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.2 < 12.5 < 13.8\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 29 | log_prisec_length_15000\     | Mean (sd) : 13 (0.7)\                    | 869 distinct values | ![](tmp/ds0129.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 9.6 < 13.1 < 14.4\                       |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 30 | log_prisec_length_25000\     | Mean (sd) : 13.8 (0.7)\                  | 870 distinct values | ![](tmp/ds0130.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 10.1 < 13.9 < 15.2\                      |                     |                     |        |         |
|    |                              | IQR (CV) : 1 (0.1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 31 | log_nei_2008_pm25_sum_10000\ | Mean (sd) : 4 (2.4)\                     | 828 distinct values | ![](tmp/ds0131.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 4.3 < 9.1\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 3.5 (0.6)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 32 | log_nei_2008_pm25_sum_15000\ | Mean (sd) : 4.7 (2.2)\                   | 855 distinct values | ![](tmp/ds0132.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5 < 9.4\                             |                     |                     |        |         |
|    |                              | IQR (CV) : 2.9 (0.5)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 33 | log_nei_2008_pm25_sum_25000\ | Mean (sd) : 5.7 (2.1)\                   | 860 distinct values | ![](tmp/ds0133.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.9 < 9.7\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.6 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 34 | log_nei_2008_pm10_sum_10000\ | Mean (sd) : 4.3 (2.3)\                   | 829 distinct values | ![](tmp/ds0134.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 4.6 < 9.3\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 3.4 (0.5)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 35 | log_nei_2008_pm10_sum_15000\ | Mean (sd) : 5.1 (2.2)\                   | 855 distinct values | ![](tmp/ds0135.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.4 < 9.7\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.8 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 36 | log_nei_2008_pm10_sum_25000\ | Mean (sd) : 6.1 (2)\                     | 860 distinct values | ![](tmp/ds0136.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 6.4 < 9.9\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 2.4 (0.3)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 37 | popdens_county\              | Mean (sd) : 551.8 (1711.5)\              | 564 distinct values | ![](tmp/ds0137.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0.3 < 156.7 < 26821.9\                   |                     |                     |        |         |
|    |                              | IQR (CV) : 470 (3.1)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 38 | popdens_zcta\                | Mean (sd) : 1279.7 (2757.5)\             | 840 distinct values | ![](tmp/ds0138.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 610.3 < 30418.8\                     |                     |                     |        |         |
|    |                              | IQR (CV) : 1281.4 (2.2)                  |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 39 | nohs\                        | Mean (sd) : 7 (7.2)\                     | 215 distinct values | ![](tmp/ds0139.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 5.1 < 100\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 6.1 (1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 40 | somehs\                      | Mean (sd) : 10.2 (6.2)\                  | 230 distinct values | ![](tmp/ds0140.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 9.4 < 72.2\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 8 (0.6)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 41 | hs\                          | Mean (sd) : 30.3 (11.4)\                 | 347 distinct values | ![](tmp/ds0141.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 30.8 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 12.3 (0.4)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 42 | somecollege\                 | Mean (sd) : 21.6 (8.6)\                  | 240 distinct values | ![](tmp/ds0142.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 21.3 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 7.2 (0.4)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 43 | associate\                   | Mean (sd) : 7.1 (4)\                     | 157 distinct values | ![](tmp/ds0143.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 7.1 < 71.4\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 3.9 (0.6)                     |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 44 | bachelor\                    | Mean (sd) : 14.9 (9.7)\                  | 301 distinct values | ![](tmp/ds0144.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 12.9 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 10.4 (0.7)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 45 | grad\                        | Mean (sd) : 8.9 (8.6)\                   | 245 distinct values | ![](tmp/ds0145.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 6.7 < 100\                           |                     |                     |        |         |
|    |                              | IQR (CV) : 7.1 (1)                       |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 46 | pov\                         | Mean (sd) : 15 (11.3)\                   | 345 distinct values | ![](tmp/ds0146.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 12.1 < 65.9\                         |                     |                     |        |         |
|    |                              | IQR (CV) : 14.7 (0.8)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 47 | hs_orless\                   | Mean (sd) : 47.5 (16.8)\                 | 464 distinct values | ![](tmp/ds0147.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 0 < 48.7 < 100\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 21.2 (0.4)                    |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 48 | urc2013\                     | Mean (sd) : 2.9 (1.5)\                   | 1 : 203 (23.2%)\    | ![](tmp/ds0148.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        | 2 : 163 (18.6%)\    |                     | (100%) | (0%)    |
|    |                              | 1 < 3 < 6\                               | 3 : 228 (26.0%)\    |                     |        |         |
|    |                              | IQR (CV) : 2 (0.5)                       | 4 : 123 (14.0%)\    |                     |        |         |
|    |                              |                                          | 5 : 101 (11.5%)\    |                     |        |         |
|    |                              |                                          | 6 :  58 ( 6.6%)     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 49 | urc2006\                     | Mean (sd) : 3 (1.5)\                     | 1 : 195 (22.3%)\    | ![](tmp/ds0149.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        | 2 : 162 (18.5%)\    |                     | (100%) | (0%)    |
|    |                              | 1 < 3 < 6\                               | 3 : 221 (25.2%)\    |                     |        |         |
|    |                              | IQR (CV) : 2 (0.5)                       | 4 : 127 (14.5%)\    |                     |        |         |
|    |                              |                                          | 5 : 115 (13.1%)\    |                     |        |         |
|    |                              |                                          | 6 :  56 ( 6.4%)     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+
| 50 | aod\                         | Mean (sd) : 43.7 (19.6)\                 | 581 distinct values | ![](tmp/ds0150.png) | 876\   | 0\      |
|    | [numeric]                    | min < med < max:\                        |                     |                     | (100%) | (0%)    |
|    |                              | 5 < 40.2 < 143\                          |                     |                     |        |         |
|    |                              | IQR (CV) : 18 (0.4)                      |                     |                     |        |         |
+----+------------------------------+------------------------------------------+---------------------+---------------------+--------+---------+


We can see that for many variables there are many low values as the distribution shows two peaks, one near zero and another with a higher value. This is true for the imp variables (measures of development), the nei variables (measures of emission sources) and the road denisty variables. We can also see that the range of some of the variables is very large, in particular the area and population related variables.


In prediction analyses, it is also useful to evaluate if any of the variables are correlated.

Intuitively we can expect some of our variables to be correlated.

We expact the development variables (imp) to be correlated with eachother, we expect the road density variables to be correlated with eachother, we expect the emission variables to be correlated with eachother, and we also expect these variables to be correlated with one another and likely correlated with population density. We can get a nice visualization of correlation using the `ggcorr()` function and the `ggpairs()` function of the `GGally` package. to select our variables of interest we can use the `select()` function with the `contains()` function of the `tidyr` package. 

First let's look at the imp/development variables. 
```{r, out.width = "400px"}
select(pm, contains("imp")) %>%
  ggcorr(palette = "RdBu", label = TRUE)

select(pm, contains("imp")) %>%
  ggpairs()
  
```

Indeed, we can see that imp_a1000 and imp_a500 are perfectly correlated, as well as imp_a10000, imp_a15000.


Now let's take a look at the road denisty data:

```{r, fig.weight=12}
select(pm, contains("pri")) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

select(pm, contains("pri")) %>%
  ggpairs()
```

We can see that while some of the road density variables are highly correlated with one another (or anticorrelated), there arr some variables that are less correlated with one another.

Finally let's look at the emission variables.

```{r}
select(pm, contains("nei")) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

select(pm, contains("nei")) %>%
  ggpairs()
```

We would also expect the population density data might correlate with some of these variables. Let's take a look.

```{r}
pm %>%
select(log_nei_2008_pm25_sum_10000, popdens_county, log_pri_length_10000, imp_a10000) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

pm %>%
select(log_nei_2008_pm25_sum_10000, popdens_county, log_pri_length_10000, imp_a10000) %>%
  ggpairs()
```


Interesting, so these variables dont appear to be highly correlated, therefore we might need variables from each of the categories to predict our monitor PM~2.5~ pollution values.

We seem to have some pretty extreme population values though, so let's see what happens when we take the log value.

```{r}
pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, log_pri_length_10000, imp_a10000) %>%
  ggcorr(palette = "RdBu",  hjust = .85, size = 3,
       layout.exp=2, label = TRUE)

pm %>%
  mutate(log_popdens_county= log(popdens_county)) %>%
select(log_nei_2008_pm25_sum_10000, log_popdens_county, log_pri_length_10000, imp_a10000) %>%
  ggpairs()
```

Indeed this increased the correlation, but variables from each of these categories may still prove to be useful for prediction.

Now that we have a sense of what our data is like we can get started.

## Data Analysis

### The tidymodels ecosystem

To perform our analysis we will be using the `tidymodels` suite of packages. You may be familiar with the older packages `caret` or `mlr` which are also for machine learning and modeling but are not a part of the `tidyverse`. [Max Kuhn](https://resources.rstudio.com/authors/max-kuhn){target="_blank"} describes `tidymodles` like this:

> "Other packages, such as caret and mlr, help to solve the R model API issue. These packages do a lot of other things too: preprocessing, model tuning, resampling, feature selection, ensembling, and so on. In the tidyverse, we strive to make our packages modular and parsnip is designed only to solve the interface issue. It is not designed to be a drop-in replacement for caret.
The tidymodels package collection, which includes parsnip, has other packages for many of these tasks, and they are designed to work together. We are working towards higher-level APIs that can replicate and extend what the current model packages can do."


### The major benefits of tidymodels
1) standardized workflow/format across different types of algorithms
2) can perform inline transformations of the data making it easy and reproducible to modify analysis



### An illustration of the overall tidymodels ecosystem

<p align="center">
  <img width="600" src="https://rviews.rstudio.com/post/2019-06-14-a-gentle-intro-to-tidymodels_files/figure-html/tidymodels.png">
</p>


### Splitting the Data

The first step after data exploration in machine learning analysis is to [split the data](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7){target="_blank"} into **training** and **testing** datasets. 

The training dataset will be used to build and tune our model. This is the data that the model "learns"on.

The testing set will be used to evaluate the performance of our model in a more generalizable way. What do we mean by "generalizable".

Remeber that our main goal is to use our model to be able to predict air pollution levels in areas where there are no gravimetric monitors. Therefore, if our model is super good at predicting air pollution with the data that we use to build it, it might not do the best job for the areas where there are few to no monitors. This would cause us to have really good prediction accuracy and we might assume that we were going to do a good job estimating air pollution any time we use our model, but in fact this would likely not be the case. This situation is what we call **[overfitting](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6){target="_blank"} **.

Overfitting happens when we end up modeling not only the major relationships in our data but also the noise within our data. 


```{r}
knitr::include_graphics("https://miro.medium.com/max/1110/1*tBErXYVvTw2jSUYK7thU2A.png")
```

##### [[source](https://miro.medium.com/max/1110/1*tBErXYVvTw2jSUYK7thU2A.png)]

If we get fairly good prediction with our testing set then we will know that our model can be applied to other data and will perform fairly well. We will discuss the more later.

We will not touch the testing set until we have completed tuning or model with the training set. This will allow us to have a less biased evaluation of how well our model can do with other data besides the data used in the training set to build the model. Ideally you would also want a completely independent dataset to further test the preformance of your model.

[Here](https://machinelearningmastery.com/difference-test-validation-datasets/){target="_blank"} is a great description of the differences between testing nd training datasets.

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","split.png"))
```
We will use the `rsample` package to peform this step.

The`initial_split()` function allows us to specify how we want to split our data. Typically data is split into 3/4 for training and 1/4 for testing.This is the default preportion and does not need to be specified. However you can change the proportion using the `prop` argument, we will do that here for illustrative purposes. You can also specify a variable to stratify by with the `strata` argument. This is useful if you have imbalanced categorical variables and you would like to intentionally make sure that there are similar number of samples of the rarer categories in both the testing and training sets. Otherwise the split is performed randomly. 

> The strata argument causes the random sampling to be conducted within the stratification variable. The can help ensure that the number of data points in the training data is equivalent to the proportions in the original data set.

In the case with our dataset, perhaps would like our training set to have similar proportions of monitors from each of the states as in the initial data. This might be useful if we want our model to be generalizable across all of the states.

We can see that indeed there are different proportions of monitors in each state by using the `count()` function of the `dpyr` package. 

#### {.scrollable }
```{r}
# Scroll through the output!
count(pm, state) %>%
  print(n = 1e3)
```
####

If our dataset were large enough it might be nice then to stratify by state, but our data is not unfortuantely not large enough. We will show how one would do this though for illustrative purposes. This option is more important for classifcation applications of machine learning than it is for prediction.

Since the split is performed randomly, it is a good idea to use the `set.seed()` base function to ensure that if your rerun your code that your split will be the same next time. We can see the number of monitors in our training, testing, and original data by typing in the name of our split object. The result will look like this:
<training data sample number, testing data sample number, original sample number> 

```{r}
set.seed(1234)
pm_split <-rsample::initial_split(data = pm, prop = 2/3)
pm_split

# If stratifying:
# pm_split_strata <-rsample::initial_split(data = pm, prop = 2/3, strata = "state")

```

Importantly the `initial_split` function only determines what rows of our pm dataframe should be assigned for training or testing, it does not actually split the data. 

To extract the testing and training data we can use the `training()` and `testing()` functions also of the `rsample` package.

#### {.scrollable }
```{r}
 train_pm <-rsample::training(pm_split)
 test_pm <-rsample::testing(pm_split)
 
# Scroll through the output!
count(train_pm, state)
count(test_pm, state)
```
####

We will use the `rsample` package again we look at implementing what are called cross validation techniques.

Cross validation splits our training data into multiple training data sets to allow for an assessment of the accuracy of the model as we are training it. 

Note: People typically ignore spatial dependence with cross validation of air pollution monitoring data in the air pollution field, so we will do the same.  However, it might make sense to leave out blocks of monitors rather than  random individual monitors to help account for some spatial dependence.




### Recipes

SHOULD I INCLUDE THIS???
[Here](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/) is a great guide to the recipes package.

A recipe is a standardized format for a sequence of steps for processing the data.

This can be very useful because it makes testing out different preprocessing steps or different algorithms with the same preprocessing very easy and reproducible.

**Creating a recipe specifies how a dataframe of predictors should be created  - it specifies what  variables to be used  and the  preprocessing steps  but it does not execute these steps or create the dataframe of predictors.**

#### List the ingredients / specify the variables  with the `recipe()` function

The first thing to do to create a recipe is to specify which variables we will be using as our outcome and predictors using the `recipe()` function. In terms of the metaphore of baking, we can think of this as listing our ingredients. The naming convention for recipe object names is `*_rec` or `rec`. 

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","recipes1.png"))
```


In our case recall that our `value` variable, which is the average annual gravimetric monitor PM~2.5~ concentration in ug/m^3^. Our predictors are all the other variables except the monitor ID, which is the `id` variable.

The reason not to include this variable is because this variable includes the county number and a number designating which particular monitor the values came from of the monitors there are in that county. Since this number is arbitrary and the county information is also given in the data, and the fact that each monitor only has one value in the `value` variable, nothing is gained by including this variable and it may instead introduce noise. However, it is useful to keep this data to take a look at what is happening later. We will show you want to do in this case in just a bit.

The simplest recipe with no preprocessing steps, would be to simply list the outcome and predictor variables.

We can do so in two ways. 

1) Using formula notation
2) Assigning roles to each variable

Let's look at the first way using formula notation, which looks like this:

outcome(s) ~ predictor(s)  

If in the case of multiple predictors or a multivariate situation with two outcomes, use a plus sign

outcome1 + outcome2 ~ predictor1 + predictor2

If we want to include all predictors we can use a period like so:

outcome_variable_name ~ .

Now with our data we will start by making a recipe for our training data. In the simplest case we might use all predictors like this:

```{r}

simple_rec <-train_pm %>%
  recipes::recipe(value ~ .)

simple_rec
```


However, to deal with the id variable we could use the `update_role()` function of the `recipes` package:

```{r}

simple_rec <-train_pm %>%
  recipes::recipe(value ~ .) %>%
  recipes::update_role(id, new_role = "id variable")

simple_rec
```
We could also specify the outcome and predictors in the same way as the id variable. Please see [here](https://tidymodels.github.io/recipes/reference/recipe.html) for examples of other roles for variables. The role can be however any value. The order is important here, as we first make all variables predictors and then override this role for the outcome and id variable. We will use the `everything()` function of the `dplyr` package to start with all of the variables in `train_pm`.

```{r}

simple_rec <-recipe(train_pm) %>%
    update_role(everything(), new_role = "predictor")%>%
    update_role(value, new_role = "outcome")%>%
    update_role(id, new_role = "id variable")

simple_rec

```

If we want to take a look at our formula from our recipe we can do use the `formula()` function of the `stats` package.

```{r}
formula(simple_rec)
```

We can also view our recipe in more detail using the base `summary()` function.

```{r}
summary(simple_rec)
```

#### List the preprocessing steps using the step functions of the `recipe` package

The other thing the recipes package allows for is specifying preprocessing steps.

```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","recipes2.png"))
```


**This [link](https://tidymodels.github.io/recipes/reference/index.html){target="_blank"} and this [link](https://cran.r-project.org/web/packages/recipes/recipes.pdf){target="_blank"} show the many options for recipe step functions.**

<u>There are step functions for a variety of purposes:</u>

1) [**Imputation**](https://en.wikipedia.org/wiki/Imputation_(statistics)){target="_blank"}  -- which means filling in missing values based on the existing data 
2) [**Transformation**](https://en.wikipedia.org/wiki/Data_transformation_(statistics)){target="_blank"}  -- which means changing all values of a variable in the same way, typically to make it more normal or easier to interpret)  
3) [**Discretization**](https://en.wikipedia.org/wiki/Discretization_of_continuous_features) -- which means converting continous values into discrete or nominal values - binning for example to reduce the number of possible levels)  (However this is gernally not advisable!)
4) [**Encoding / Creating Dummy Variables**](https://en.wikipedia.org/wiki/Dummy_variable_(statistics)) -- which means creating a numeric code for categorical variables
[**More on Dummy Variables and one hot encoding**](https://medium.com/p/b5840be3c41a/responses/show)
5) [**Data type conversions**](https://cran.r-project.org/web/packages/hablar/vignettes/convert.html) -- which means changing from integer to factor or numeric to date etc.
6) [**Interaction**](https://statisticsbyjim.com/regression/interaction-effects/) term addition to the model -- which means that we would be modeling for predictors that would inflence the capacity of each other to predict the outcome
7) [**Normalization**](https://en.wikipedia.org/wiki/Normalization_(statistics)) -- which means centering and scaling the data to a similar range of values
8) [**Dimensionality Reduction/ Signal Extraction**](https://en.wikipedia.org/wiki/Dimensionality_reduction) -- which means mathematically obtaining a new smaller set of variables that capture the variation or signal in the original variables (ex. Principal Component Analysis and Independent Component Analysis)
9) **Filtering** -- Filtering options for removing variables (ex. remove variabels that are highly correlated to others or remove varaibles with very little varaince and therefore likely little predictive capacity)
10) [**Row operations**](https://tartarus.org/gareth/maths/Linear_Algebra/row_operations.pdf) -- which means performing functions on the values within the rows  (ex. rearranging, filtering, imputing)
11) **Checking functions** -- Sanity checks to look for missing values, to look at the varible classes etc.

All of the step functions look like `step_*` except for the check functions which look like `check_*`.

There are several ways to select what variables to apply steps to:  
1) tidyselect methods: `contains()`, `matches()`, `starts_with()`, `ends_with()`, `everything()`, `num_range()`  
2) based on the type: `all_nominal()`, `all_numeric()` , `has_type()` 
3) based on the role: `all_predictors()`, `all_outcomes()`, `has_role()`
4) name - use the actual name of the variable/variables of interest  


Let's try adding some steps to our recipe.

We might consider log transforming our population and area variables (that aren't densities)- let's take a look at the range of these variables.
```{r}
pm %>%
  select(matches("_pop|_area")) %>%
  map(range)
```
We can see that the range for each of these variables is quite large, we can log transform this data using the `step_log()` function of the `recipes` package.


We would also want to potentially one hot encode some of our categorical variables so that they can be used with certain algorithms. We can do this with the `step_dummy()` function and the `one_hot = TRUE` argument. Our fips variable includes a numeric code for state and county - and therefore is essentially a proxy for county.  So if we one hot encode county, we would want to include this rather than the fips id. This would be preferable because the fips is relatively speaking arbitary- the numbers although meaningful dont really have an order. Just because Alabama comes first in the alphabet, does not mean that monitors in this state have a rank order different from monitors in Wyoming.

We can remove the `fips` variable from the predictors using `update_role()` to make sure that the role is no longer `"predictor"`. We can make the role anything we want actually, so we will keep it something identifiable.

We might also want to remove variables that appear to be redudant and are highly correlated with others, as we know from our exploratory data analysis that many of our variables are correlated with one another. We can do this using the `step_corr()` function.

It is important to add the steps to the recipe in an order that makes sense just like with a cooking recipe.


For example, we 


check_new_values: This check will break the bake function if any of the checked columns does contain values it did
not contain when prep was called on the recipe. If the check passes, nothing is changed to the data

check_range creates a specification of a recipe check that will check if the range of a numeric
variable changed in the new data.


As steps are estimated by prep, these operations are applied to the training set. Rather than running
bake to duplicate this processing, this function will return variables from the processed training set.
Usage
juice(object, ..., composition = "tibble")
Arguments
object A recipe object that has been prepared with the option retain = TRUE.
... One or more selector functions to choose which variables will be returned by
the function. See selections() for more details. If no selectors are given, the
default is to use everything().
composition Either "tibble", "matrix", "data.frame", or "dgCMatrix" for the format of the
processed data set. Note that all computations during the baking process are
done in a non-sparse format. Also, note that this argument should be called
after any selectors and the selectors should only resolve to numeric columns
(otherwise an error is thrown).


Details
Given a data set, this function estimates the required quantities and statistics required by any operations.
prep() returns an updated recipe with the estimates.
Note that missing data handling is handled in the steps; there is no global na.rm option at the
recipe-level or in prep().
Also, if a recipe has been trained using prep() and then steps are added, prep() will only update
the new operations. If fresh = TRUE, all of the operations will be (re)estimated.
As the steps are executed, the training set is updated. For example, if the first step is to center the
data and the second is to scale the data, the step for scaling is given the centered data.
Value
A recipe whose step objects have been updated with the required quantities (e.g. parameter estimates, model objects, etc). Also, the term_info object is likely to be modified as the operations
are executed

[parameter vs hyperparameter](https://www.datacamp.com/community/tutorials/parameter-optimization-machine-learning-models){target="_blank"}

```{r}
simple_rec %<>%
  step_log(matches("_pop|_area")) %>%
  update_role("fips", new_role = "county id") %>%
  step_dummy(state, county, city, zcta, one_hot = TRUE,) %>%
  step_corr(all_predictors(), - CMAQ, - aod) %>%
  check_new_values(all_predictors()) %>%
  check_range(all_predictors())

  
simple_rec
```


< skip option in step functions: A logical. Should the check be skipped when the recipe is baked by bake.recipe()?
While all operations are baked when prep.recipe() is run, some operations
may not be able to be conducted on new data (e.g. processing the outcome
variable(s)). Care should be taken when using skip = TRUE as it may affect the
computations for subsequent operations.

#### Preparing the data for the recipe

The next major function of the `recipes` package is `prep()`.

This function updates the recipe object based on the training data. It estimates parameters for preprocessing, however it doesn't actually executethe preprocessing itself (unless you specify one of the arguments to do so). This allows the recipe to be ready to use on other datasets. 

According to the RDocumentation:

>Given a data set, this function estimates the required quantities and statistics required by any operations.

>prep() returns an updated recipe with the estimates.

There are some important arguments to know about:
1) training - you must supply a training data set to estimate parameters for preprocessing operations(recipe steps) - this may already be included in your recipe - as is the case for us
2) fresh - if TRUE - will retrain and estimate parameters for any previous steps that were already prepped
3) verbose - if `TRUE` shows the progress as the steps are evaluated and the size of the preprocessed training set
4) retain - if `TRUE` then the preprocessed training set will be saved within the receipe as a template. This is good if you are likely to add more steps and dont want to rerun the `prep()` on the previous steps. However this can make the recipe size large. (running skipped steps what???????????)


```{r}
prepped_rec <- prep(simple_rec, verbose = TRUE, retain = TRUE )
names(prepped_rec)
```

Since we retained our preprocessed training data, we can take a look at it like this:

```{r}
glimpse(prepped_rec$template)
# prepped_rec$var_info
# prepped_rec$term_info
# prepped_rec$steps
# prepped_rec$template
# prepped_rec$levels
# prepped_rec$tr_info
# prepped_rec$orig_lvls
#print(prepped_rec$last_term_info, print(n = 1e3)
```

There are also lots of other useful things to checkout in the output.
you can see the `steps` that were run, the variable info (`var_info`), the model `term_info`, the new `levels` of the variables, the original levels of the variables `orig_lvls`, info about the training data set size and completeness (`tr_info`).

Note:  You may see the `prep.recipe()` function in material that you read about the `recipes` package. This is refering to the `prep()` function of the `recipes` package.

#### Running the preprocessing

The `juice()` function of the `recipes` package will actually extract the preprocessed data giving you the final training design matrix to pass onto the model.

**Note**: if you used the `retain = TRUE` argument of the `prep()` function, then you can use the data stored in `template` in the output. This is a good option if you are adding more steps to the recipe over time and dont want to run the previous steps again. This is also a good idea if you have steps within the recipe that have the option `skip = FALSE`.

< "juice will return the results of a recipes where all steps have been applied to the data, irrespective
of the value of the step’s skip argument"



```{r, echo=FALSE, out.width="400px"}
knitr::include_graphics(here::here("img","recipes3.png"))
```

```{r}
juiced_rec <- recipes::juice(prepped_rec)

identical(juiced_rec, prepped_rec$template)

prepped_not_retained_rec <- prep(simple_rec, verbose = TRUE, retain = FALSE )
#juiced_rec2 <- recipes::juice(prepped_not_retained_rec)# this is not possible
baked_rec <- recipes::bake(prepped_not_retained_rec, new_data = train_pm)#but this is
identical(juiced_rec, baked_rec)

# options are prep(retain = FALSE) and bake() or
# prep(retain = TRUE) and juice()  or not necessary but can also do
# prep(retain = TRUE) and bake()

# this does not work: baked_rec <- recipes::bake(juiced_rec, new_data = train_pm)

```

```{r}
baked_rec <- recipes::bake(prepped_rec, new_data = train_pm)


identical(juiced_rec, baked_rec)
identical(prepped_rec$template, baked_rec)
```






> bake() takes a trained recipe and applies the operations to a data set to create a design matrix.
 for example:  it applies the centering to new data sets using these means used to create the recipe

what are these????
prep.recipe()
bake.recipe()

prepper for cross validation?


From Broom:

allows for an easy/tidy way to look at the fitted model
tidy() grabs the coefficients from the model
glance() summarizes the model fit
augment() gives 150 row observation level summary



great blog about cross validation etc https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6

## Data Visualization

## Summary

We have evaluated average consumption estimates of 15 dietary factors with probably non-communicable disease (NCD) risk from 195 different countries around the world. To do so we imported data from a PDF using the `pdftools` package, as well as data from two CSV files using readr. We used `tidyverse` packages such as `dplyr`, `stringr`, and `tidy` to clean and join the data from the PDF with the CSV files. 

We learned that Regression is a powerful and flexible statistical tool that simplifies or estimates the relationships between variables using a mathematical model. We learned about the utility of the regression to compare groups, look for associations between variables, and to predict outcomes based on multiple predictor or explanatory variables. We then compared this to other popular tests like the $t$-test and the ANOVA. We learned that these tests are actually equivalent to specialized types of regressions.

Our statistical analysis focused on evaluating differences in the consumption of red meat around the world between females and males and across different age groups. First we looked at the assumptions of [$t$-tests](https://stattrek.com/statistics/dictionary.aspx?definition=two-sample%20$t$-test){target="_blank"} and regressions, and determined that the relative percentage data of red meat consumption to the optimal guideline suggested amount was right skewed. We learned that we could transform the data by taking the log of these values to achieve more normally distributed data. To compare males and females we used a $t$-test and learned that a $t$-test is a specialized form of a linear regression. To compare the 15 different age groups we used an ANOVA and learned that the ANOVA is also a specialized form of linear regression. We examined how we obtained the same results using either statistical test. This was also the case if we looked at the effect of gender and controlled for the paired country structure in the data by either including `location_name` in the model as another term or by using a mixed effects model to control for this structure as a random effect but not specifically test for the influence of `location_name` on red meat consumption estimates. We learned that fixed effects are those that we wish to evaluate, while random effects are those that may influence the relationships of our variables of interest but that we do not wish to actively evaluate. Using these tests and models, we determined that males consume more red meat than women on average around the world. 

Our ANOVA analysis of age determined that indeed there at least one age group consumed a significantly different amount of red meat compared to the other age groups, and this was still the case when we controlled `location_name`. However, we learned that we could that the ANOVA does not provide information about which age groups are different. We learned how the regression however could provide some comparisons of different age groups relative to the reference age group. Furthermore, our data visualizations allowed us to determine that in general red met consumption appears to be higher in the younger age groups relative to the older age groups. 

Finally, we also looked at differences in red meat consumption between the different countries and saw in our ANOVA analysis and our regression analysis that there were significant differences. We were able to use a regression that included `sex`, `age_group_name`, and `location_name` to evaluate the influence of each of the three demographic factors on consumption while controlling or accounting for the other two. Our results demonstrated that all three influenced or were associated with red meat consumption.

In preforming our statistical analyses we learned about the assumptions of the $t$-test, the regression, and the ANOVA. We also learned about important methods to tests these assumptions.

#### $t$-test assumptions:

1) Normality of the data for both groups (this is not as much of an issue if the number of observations is relatively large total n>30 - can evaluate by plotting the distribution and by creating [Q-Q plots]([Q-Q Plots](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html){target="_blank"}))
2) Equal variance between the two groups (make sure you do the correct test if the data is not normal)
3) Balanced sample sizes of the two groups
4) Independent observations (or independent paired observations)

We can evaluate if our data is [normally distributed](https://www.physiology.org/doi/full/10.1152/advan.00064.2017){target="_blank"} by plotting the distribution and by creating [Q-Q plots]([Q-Q Plots](http://onlinestatbook.com/2/advanced_graphs/q-q_plots.html){target="_blank"} to compare the distribution of our data to the theoretical [normal distribution](https://www.physiology.org/doi/full/10.1152/advan.00064.2017){target="_blank"}.

<u>If our data is not normally distributed, we can consider these options:</u>

1) We can still perform a $t$-test if our n is large
2) We can transform the data before performing a $t$-test
3) We can use a nonparametric test (Wilcoxon signed rank test, the Wilcoxon rank sum test, and the Two-sample Kolmogorov-Smirnov (KS) test)
4) We can perform a $t$-test with resampling methods (which should be especially considered when the groups are imbalanced)

See this [case study](https://opencasestudies.github.io/ocs-bp-rural-and-urban-obesity){target="_blank"} for more information on $t$-test assumptions.

We learned that we can test if 2 groups have equal variance using:  
1) the F test with `var.test()`  
2) Mood's test using [`mood.test()`](https://files.eric.ed.gov/fulltext/ED065559.pdf) (use if the data is not normally distributed)  

#### Linear regression assumptions:

L (linear) - There is a linear relationship between the variables.  
I (independent) - The samples are independent from one another.  
N (normal) - The residuals are normally distributed.  
E (equal variances) - The variance of the residuals of the groups is similar.   
It is also important that predictor variables are not correlated with one another.  

#### ANOVA assumptions:

1) Normality of the data for all tested groups (less of an issue if the number of observations is relatively large total n>30) 
2) Equal variance between the groups - aka Homogeneity of variance (make sure you do the correct test if the data is not normal) 
3) Balanced sample sizes of the groups 
4) Independent observations (or independent paired observations) 

We learned about three tests for examining the equality of variance of 3 or more groups:
1) [Bartlett's test](https://www.itl.nist.gov/div898/handbook/eda/section3/eda357.htm){target="_blank"} (works well if the data appears to be normally distributed) 
2) [Fligner-Killeen](http://wiki.stat.ucla.edu/socr/index.php/AP_Statistics_Curriculum_2007_NonParam_VarIndep){target="_blank"} test is nonparametric and does not assume normality 
3) [Levene's test](https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm){target="_blank"}, is more robust to violations of normality than the Bartlett's test, but not as robust as the Fligner- Killeen test 


Using the `ggplot2` package we were able to visualize trends in the data and to compare consumption of these dietary factors in the US with that of the other countries.
We see that the populations in many countries are over-consuming foods that are associated with health risk when over-consumed. In particular processed meat and sugar-sweetened beverages appear to be the most over consumed. Importantly both of these appear to be consumed at higher quantities by males and younger adults. People in the US  appear to consume less sugar-sweetened beverages than other countries, however, people are still over-consuming. Processed meat however appears to be especially bad in the US. In terms of food that need to be consumed in adequate amounts to overcome health risk, nearly all countries for all factors are not reaching guideline levels. However, there are some countries consuming more than adequate amounts of legumes, vegetables, fruits and fiber. People in the US appear to eat more milk products and consume more omega-3 fatty acid and calcium rich foods than other countries. All countries including the US consume very low levels of polyunsaturated fatty acids. These [polyunsaturated fatty acids](https://en.wikipedia.org/wiki/Polyunsaturated_fat) are abundant in seeds, nuts, avocados, as well as fish. Likely the low level of consumption of nuts and seeds contributes to these low polyunsaturated fatty acid estimates. The supplementary table included in the article suggests that poor consumption of polyunsaturated fatty acids is associated with ischemic heart disease. The article takes this data further to evaluate the association of consumption levels of these foods with mortality.

Analyses like the one in our case study are important for defining which groups could benefit the most from interventions, education, and policy changes when attempting to mitigate public health challenges. You can see in the [article](https://www.thelancet.com/action/showPdf?pii=S0140-6736%2819%2930041-8){target="_blank"} however that many additional considerations would be involved to perform a more thorough analysis to adequately understand the data enough to recommend policy changes.


### Suggested Homework

Students can evaluate consumption estimates of another dietary factor besides red meat.

### Helpful Links

review of [tidymodels](https://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/){target="_blank"} 

guide for [preprocessing with recipes](http://www.rebeccabarter.com/blog/2019-06-06_pre_processing/)

[guide](https://briatte.github.io/ggcorr/) for using GGally to create correlation plots
[guide](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/) for using parnsip to try different algorithms or engines
[recipe functions](https://tidymodels.github.io/recipes/reference/index.html)

<u>Terms and concepts covered:</u>  

[Tidyverse](https://www.tidyverse.org/){target="_blank"}  
[RStudio cheatsheets](https://rstudio.com/resources/cheatsheets/){target="_blank"}  
[Inference](https://www.britannica.com/science/inference-statistics){target="_blank"}  
[Regression](https://lindeloev.github.io/tests-as-linear/){target="_blank"}  
[Different types of regression](https://www.analyticsvidhya.com/blog/2015/08/comprehensive-guide-regression/){target="_blank"}  
[Ordinary least squares method](http://setosa.io/ev/ordinary-least-squares-regression/){target="_blank"}  
[Residual](https://www.statisticshowto.datasciencecentral.com/residual/){target="_blank"}  

<u>Packages used in this case study: </u>

 Package   | Use                                                                         
---------- |-------------
[here](https://github.com/jennybc/here_here){target="_blank"}       | to easily load and save data  
[readr](https://readr.tidyverse.org/){target="_blank"}      | to import the CSV file data  
[dplyr](https://dplyr.tidyverse.org/){target="_blank"}      | to arrange/filter/select/compare specific subsets of the data  
[skimr](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data   
[summarytools](https://cran.r-project.org/web/packages/skimr/index.html){target="_blank"}      | to get an overview of data in a different style   
[pdftools](https://cran.r-project.org/web/packages/pdftools/pdftools.pdf){target="_blank"}   | to read a PDF into R   
[magrittr](https://magrittr.tidyverse.org/articles/magrittr.html){target="_blank"}   | to use the `%<>%` pipping operator  
[purrr](https://purrr.tidyverse.org/){target="_blank"}      | to perform functions on all columns of a tibble   
[tibble](https://tibble.tidyverse.org/){target="_blank"}     | to create data objects that we can manipulate with  dplyr/stringr/tidyr/purrr  
[tidyr](https://tidyr.tidyverse.org/){target="_blank"}      | to separate data within a column into multiple columns 
[ggplot2](https://ggplot2.tidyverse.org/){target="_blank"}    | to make visualizations with multiple layers  


Do I need to talk about machine learning in general more... classification vs prediction?